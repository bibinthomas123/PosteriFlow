# Distribution Separation Metrics Guide

## Overview
Added comprehensive distribution separation, entropy, and sharpness metrics to evaluate PriorityNet model performance. These metrics measure how well the model distinguishes between high and low priority signals and how decisive/sharp its predictions are.

## Metrics Implemented

### 1. **AUC (Area Under ROC Curve)**
- **Purpose**: Measures how well model separates high vs low priority signals
- **Range**: [0.5, 1.0]
  - 1.0 = Perfect separation
  - 0.5 = Random/no separation
- **Implementation**: Binary classification (high SNR=1, low SNR=0)
- **Threshold**: AUC > 0.65 (good separation)

```python
auc = DistributionMetrics.compute_separation_auc(high_snr_preds, low_snr_preds)
```

### 2. **Entropy (Sharpness Indicator)**
- **Purpose**: Measures concentration of probability distribution
- **Formula**: Shannon entropy = -Î£(p * log(p))
- **Interpretation**:
  - Low entropy (< 1.5) = Sharp/concentrated distribution
  - High entropy (> 2.5) = Blurry/spread-out distribution
- **Comparison**: High SNR entropy should be < Low SNR entropy

```python
entropy = DistributionMetrics.compute_entropy(probabilities)
```

### 3. **Sharpness**
- **Purpose**: Direct measure of decisiveness in predictions
- **Formula**: 1.0 / (1.0 + smoothness)
  - Smoothness = variance of moving average
- **Range**: [0, 1]
  - 1.0 = Maximum sharpness (decisive)
  - 0.0 = Minimum sharpness (uncertain)
- **Interpretation**: Higher values indicate more confident/decisive predictions

```python
sharpness = DistributionMetrics.compute_sharpness(scores, window_size=5)
```

### 4. **Wasserstein Distance**
- **Purpose**: Measures "earth mover's distance" between two distributions
- **Range**: [0, 1]
  - Large values = Well-separated distributions
  - Small values = Similar distributions
- **Threshold**: Wasserstein > 0.1 (good separation)
- **Interpretation**: Geometrically intuitive measure of distribution difference

```python
distance = DistributionMetrics.compute_wasserstein_distance(scores1, scores2)
```

### 5. **KL Divergence (Kullback-Leibler)**
- **Purpose**: Measures how much one distribution diverges from another
- **Range**: [0, âˆž]
  - 0 = Identical distributions
  - > 0.1 = Significant divergence
- **Formula**: KL(p||q) = Î£(p * log(p/q))
- **Threshold**: KL > 0.1 (distinct distributions)
- **Interpretation**: Information-theoretic measure of distribution distance

```python
kl_div = DistributionMetrics.compute_kl_divergence(hist_high, hist_low)
```

## Integration Points

### 1. **Unit Tests** (`tests/test_priority_net.py`)
12 comprehensive test functions:
- `test_distribution_separation_auc` - Perfect separation test
- `test_distribution_separation_auc_overlapping` - Realistic overlapping test
- `test_distribution_separation_auc_poor` - Poor separation test
- `test_entropy_sharp_distribution` - Sharp distribution entropy
- `test_entropy_uniform_distribution` - Uniform distribution entropy
- `test_sharpness_metric` - Sharpness comparison
- `test_kl_divergence_identical_distributions` - Zero divergence test
- `test_kl_divergence_different_distributions` - Non-zero divergence test
- `test_wasserstein_distance` - Large distance test
- `test_wasserstein_distance_similar` - Small distance test
- `test_priority_net_separation_quality` - Full model separation test
- `test_distribution_metrics_batch_consistency` - Consistency across batches
- `test_entropy_sharpness_correlation` - Inverse correlation test

Run tests:
```bash
pytest tests/test_priority_net.py -k "distribution or entropy or sharpness" -v
```

### 2. **Stress Test Script** (`experiments/test_priority_net.py`)
New function: `distribution_separation_analysis()`

Integrated into full validation pipeline:
```
1. Synthetic tests
2. Dense overlaps
3. Monotonicity & sensitivity
4. Calibration & spread
5. Uncertainty quality
5.1. DISTRIBUTION SEPARATION & SHARPNESS â† NEW
6. Edge conditioning
7. SNR & N-wise breakdown
8. Cross-device determinism
9. Throughput & memory
10. OOD extremes
11. Real events (GWTC-3)
```

Calls in sequence:
```python
distribution_separation_analysis(model, val_dataset)
```

## Typical Output

```
================================================================================
5ï¸âƒ£DistSep DISTRIBUTION SEPARATION & SHARPNESS
================================================================================
ðŸŽ¯ AUC (High vs Low SNR separation): 0.8234
ðŸ“Š Entropy (Lower = Sharper):
   High SNR: 0.7823
   Low SNR:  1.2456
   All:      0.9832
âš¡ Sharpness (Higher = More Decisive):
   High SNR: 0.8945
   Low SNR:  0.6234
   All:      0.7582
ðŸ“ Wasserstein Distance (High vs Low): 0.2456
ðŸ”€ KL Divergence (High vs Low): 2.3456

ðŸ“ˆ Statistical Summary:
   High SNR mean=0.782 std=0.087
   Low SNR  mean=0.345 std=0.156
   All      mean=0.564 std=0.213
   Range: [0.001, 0.998]
```

## Performance Interpretation

### Good Model Performance
- AUC > 0.75 (excellent separation)
- High SNR entropy < Low SNR entropy (clear distinction)
- High SNR sharpness > 0.85 (decisive on strong signals)
- Wasserstein distance > 0.15 (well-separated distributions)
- KL divergence > 0.5 (distinct distributions)

### Warning Signs
- AUC < 0.65 (poor separation ability)
- High SNR entropy â‰ˆ Low SNR entropy (model doesn't distinguish)
- Sharpness < 0.5 (uncertain/blurry predictions)
- Wasserstein < 0.1 (nearly identical distributions)
- KL divergence < 0.1 (too similar distributions)

## Key Differences from Existing Metrics

| Metric | Use Case | Sensitivity |
|--------|----------|-------------|
| **Correlation** | Overall ranking quality | Ordinal (rank-based) |
| **AUC** | Binary separation (high vs low) | Threshold-based |
| **Entropy** | Distribution sharpness | Probabilistic |
| **Sharpness** | Decision decisiveness | Variance-based |
| **Wasserstein** | Distribution distance | Geometric |
| **KL Divergence** | Information difference | Information-theoretic |

## Technical Notes

### Data Flow
1. Collect predictions for 300 samples (configurable)
2. Split by SNR threshold: High (>18.0), Low (<10.0), Middle (skipped)
3. Compute all 5 metrics on splits
4. Report statistical summary

### Robustness Features
- Clip probabilities to [1e-10, 1.0] to avoid log(0)
- Handle empty distributions gracefully
- Use numpy histogram binning for KL divergence
- Edge case handling for small sample sizes

### Computational Cost
- **Time complexity**: O(n log n) for AUC and Wasserstein
- **Space complexity**: O(n) for histogram storage
- **Total overhead**: <1% of validation time

## Usage Example

```python
from experiments.test_priority_net import DistributionMetrics
import numpy as np

# Get model predictions on test data
high_snr_scores = model(high_snr_detections)  # [0.85, 0.90, 0.88, ...]
low_snr_scores = model(low_snr_detections)    # [0.15, 0.22, 0.18, ...]

# Compute metrics
auc = DistributionMetrics.compute_separation_auc(high_snr_scores, low_snr_scores)
entropy_high = DistributionMetrics.compute_entropy(high_snr_scores)
entropy_low = DistributionMetrics.compute_entropy(low_snr_scores)
sharpness = DistributionMetrics.compute_sharpness(high_snr_scores)
wasserstein = DistributionMetrics.compute_wasserstein_distance(high_snr_scores, low_snr_scores)

print(f"AUC: {auc:.4f}")
print(f"Entropy high: {entropy_high:.4f}, low: {entropy_low:.4f}")
print(f"Sharpness: {sharpness:.4f}")
print(f"Wasserstein: {wasserstein:.4f}")
```

## Files Modified

1. **tests/test_priority_net.py**
   - Added `DistributionMetrics` class (87 lines)
   - Added 12 new test functions (190 lines)
   - Total: 277 lines added

2. **experiments/test_priority_net.py**
   - Added `DistributionMetrics` class (48 lines)
   - Added `distribution_separation_analysis()` function (74 lines)
   - Integrated into main validation pipeline
   - Total: 122 lines added

## References

- **AUC**: Receiver Operating Characteristic (Fawcett, 2006)
- **Entropy**: Shannon Information Theory (Shannon, 1948)
- **Wasserstein Distance**: Optimal Transport (Monge, 1781; Kantorovich, 1942)
- **KL Divergence**: Information Theory (Kullback & Leibler, 1951)

## Future Enhancements

1. **Distribution Fitting**: Fit parameterized distributions (Gaussian, Beta) to scores
2. **Confidence Intervals**: Bootstrap confidence intervals for metrics
3. **Visualization**: ROC curves, entropy plots, distribution histograms
4. **Comparison**: Compare against baseline models using these metrics
5. **Per-EventType**: Separate metrics for BBH/BNS/NSBH
6. **Sliding Window**: Track metrics as function of training epoch

---

**Last Updated**: November 19, 2025
**Author**: Amp
**Status**: Production Ready âœ…
