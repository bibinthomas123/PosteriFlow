# ğŸŒŠ PosteriFlow â€” Adaptive Hierarchical Signal Decomposition (AHSD)

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Production-brightgreen.svg)]()

> **A next-generation gravitational-wave analysis system that detects, decomposes, and characterizes overlapping signals in real-time using neural posterior estimation and adaptive signal subtraction.**

---

## ğŸ¯ What is PosteriFlow?

PosteriFlow is a cutting-edge machine learning pipeline for gravitational-wave astronomy that solves a critical problem: **how to extract multiple overlapping signals from noisy gravitational-wave detector data**.

### The Core Problem
Modern gravitational-wave detectors (LIGO, Virgo) detect weak signals buried in noise. When **multiple sources merge simultaneously**, their signals overlap, creating a complex mixture that traditional methods cannot easily separate. PosteriFlow uses **hierarchical neural networks** to:

1. **Prioritize signals** - Determine which sources to extract first
2. **Estimate parameters** - Rapidly infer masses, distances, spins using neural inference
3. **Subtract adaptively** - Remove extracted signals while preserving fainter ones
4. **Quantify uncertainty** - Provide calibrated confidence intervals for all estimates

### Why This Matters
- **Multi-messenger astronomy**: Early warnings for neutron star mergers enable electromagnetic follow-up
- **Population statistics**: Extracting overlapping events improves population constraints on compact object formation
- **Real-time decision-making**: LIGO alert system can trigger faster with overlapping signals disentangled
- **Scientific discovery**: Overlaps may reveal unexpected binary characteristics (precession, eccentricity)

---

## ğŸ—ï¸ Architecture Overview

### Three-Phase Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         RAW GRAVITATIONAL-WAVE DATA (H1, L1, V1)             â”‚
â”‚     Detector noise + overlapping GW signals + glitches       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  PHASE 1: NEURAL POSTERIOR       â”‚
          â”‚  ESTIMATION (Neural PE)          â”‚
          â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
          â”‚  â€¢ Likelihood-free inference     â”‚
          â”‚  â€¢ Multi-detector coherence      â”‚
          â”‚  â€¢ Uncertainty quantification    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
          Parameter estimates + uncertainties
          (mass_1, mass_2, distance, sky position, spins)
                         â”‚
                         â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  PHASE 2: PRIORITY NET            â”‚
          â”‚  Signal Ranking & Selection       â”‚
          â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
          â”‚  â€¢ Temporal encoding (CNN+BiLSTM)â”‚
          â”‚  â€¢ Cross-signal feature analysis  â”‚
          â”‚  â€¢ Uncertainty-aware ranking      â”‚
          â”‚  â€¢ Predicts extraction order      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                     Ordered list of signals
                     (which to remove first)
                         â”‚
                         â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  PHASE 3: ADAPTIVE SUBTRACTOR     â”‚
          â”‚  Iterative Signal Removal         â”‚
          â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
          â”‚  â€¢ Uncertainty-weighted subtraction
          â”‚  â€¢ Cross-detector coherence       â”‚
          â”‚  â€¢ Bias correction               â”‚
          â”‚  â€¢ Residual quality monitoring    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  EXTRACTED SIGNALS & RESIDUAL NOISE      â”‚
       â”‚  â€¢ Individual source parameters          â”‚
       â”‚  â€¢ Parameter uncertainties               â”‚
       â”‚  â€¢ Signal-to-noise metrics               â”‚
       â”‚  â€¢ Residual quality assessment           â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Neural Components

#### **Neural PE (Parameter Estimation)**
- Likelihood-free inference using normalizing flows
- Simultaneous estimation of ~15 binary parameters
- Fast inference: <100ms for 4-second segment
- Uncertainty quantification via posterior ensemble
- Handles contamination via data augmentation

#### **PriorityNet (Signal Prioritization)**
- Temporal CNN encoder: Multi-scale time-frequency features
- BiLSTM encoder: Temporal dependencies in strain data
- Cross-signal analyzer: Quantifies signal overlap and interaction
- Output: Ranking of signals + confidence in order
- Enables optimal extraction strategy

#### **Adaptive Subtractor**
- Uses Neural PE uncertainties to weight residuals
- Subtracts strongest signal first (per PriorityNet)
- Bias correction: Accounts for parameter estimation errors
- Iterative: Updates estimates after each subtraction
- Quality monitoring: Validates residual Gaussianity

---

## ğŸ’¾ Data Pipeline

### Synthetic Dataset Generation

PosteriFlow generates realistic synthetic gravitational-wave data for training:

```
REAL LIGO/VIRGO CHARACTERISTICS
â”œâ”€ Detector network (H1, L1, V1)
â”œâ”€ Realistic PSDs from O4 sensitivity
â”œâ”€ Real glitches & contamination
â”œâ”€ Physics-accurate waveforms (IMRPhenomXAS)
â””â”€ Realistic source populations

                    â–¼

PARAMETERS SAMPLED (Physics-Constrained)
â”œâ”€ Masses (BBH: 5-100 Mâ˜‰, BNS: 1-2.5 Mâ˜‰)
â”œâ”€ Spins (aligned & precessing)
â”œâ”€ Distance (~log-uniform, Malmquist bias)
â”œâ”€ Sky position (uniform on sphere)
â””â”€ Binary merger epoch

                    â–¼

SIGNAL GENERATION
â”œâ”€ GW waveform synthesis (PyCBC)
â”œâ”€ Detector response (antenna patterns)
â”œâ”€ SNR-dependent distance scaling
â””â”€ Parameter-distance correlation (physics-validated)

                    â–¼

CONTAMINATION INJECTION
â”œâ”€ Real LIGO noise (GWOSC, 10-25Ã— speedup via caching)
â”œâ”€ Neural synthetic noise (10,000Ã— faster than GWOSC)
â”œâ”€ Line glitches (60 Hz, harmonics)
â”œâ”€ Transient glitches (blips, scattered light)
â”œâ”€ PSD drift (multiple epochs)
â””â”€ Detector dropout scenarios

                    â–¼

OVERLAP CREATION (45% realistic rate)
â”œâ”€ 2-signal overlaps (direct mergers)
â”œâ”€ Multi-signal overlaps (up to 8 signals)
â”œâ”€ Partial overlaps (different durations)
â””â”€ Subtle ranking (important for prioritization)

                    â–¼

EDGE CASE SAMPLING (8% of dataset)
â”œâ”€ Physical extremes (high mass-ratio, spins)
â”œâ”€ Observational extremes (strong glitches)
â”œâ”€ Statistical extremes (multimodal posteriors)
â””â”€ Overlapping extremes (subtle ranking)

                    â–¼

FINAL DATASET (25,000+ samples)
â”œâ”€ Detector strain (H1, L1, V1) + preprocessing
â”œâ”€ Ground-truth parameters
â”œâ”€ Network SNR & quality metrics
â”œâ”€ Metadata for analysis
â””â”€ Train/val/test splits (80/10/10)
```

### Data Statistics

```
SIGNAL TYPE DISTRIBUTION:
â”œâ”€ Binary Black Hole (BBH):    46% â†’ Loudest, most common
â”œâ”€ Binary Neutron Star (BNS):  32% â†’ Rare, long duration, crucial for EW
â”œâ”€ NS-BH (NSBH):               17% â†’ Intermediate
â””â”€ Noise only:                  5% â†’ Background characterization

OVERLAP STATISTICS:
â”œâ”€ Single signals:      55% of samples
â”œâ”€ Overlapping:         45% of samples
â”‚  â”œâ”€ 2-3 signals:      35%
â”‚  â”œâ”€ 4-5 signals:       8%
â”‚  â””â”€ 6+ signals:        2%
â””â”€ Average: 2.25 signals per sample

SNR DISTRIBUTION (O4 REALISTIC):
â”œâ”€ Weak (10-15):        5%
â”œâ”€ Low (15-25):        35%  â† Most detections
â”œâ”€ Medium (25-40):     45%
â”œâ”€ High (40-60):       12%
â””â”€ Loud (60-80):        3%

PARAMETER RANGES:
â”œâ”€ Masses:    3-200 Mâ˜‰  (detector frame)
â”œâ”€ Distances: 10-18,000 Mpc
â”œâ”€ Spins:     0-0.99
â””â”€ SNR:       3-100
```

### Advanced Features

**Real Noise Integration (10-25Ã— speedup)**
- Pre-downloaded GWOSC segments (133 cached files)
- Three-level fallback: cache â†’ on-demand â†’ synthetic
- 10% real noise mixing for enhanced realism

**Neural Noise Generation (10,000Ã— speedup)**
- FMPE pre-trained models (Gaussian_network.pickle)
- Colored Gaussian & non-Gaussian variants
- Falls back gracefully if models unavailable


**TransformerStrainEncoder Enhancement**
- State-of-the-art strain encoding
- Attention-based temporal modeling
- Outperforms CNN+BiLSTM baselines


---

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Clone repository
git clone https://github.com/bibinthomas123/PosteriFlow.git
cd PosteriFlow

# Initialize conda (first time only)
conda init

# Activate environment
conda activate ahsd

# Install package in development mode
pip install -e . --no-deps
```

**Important:** The conda environment `ahsd` exists and contains all dependencies. Never recreate it.

### 2. Generate Training Data

```bash
# Generate 25,000 samples (default, ~1.5-2 hours)
python src/ahsd/data/scripts/generate_dataset.py \
    --config configs/data_config.yaml \
    --num-samples 25000

# Custom parameters
python src/ahsd/data/scripts/generate_dataset.py \
    --config configs/data_config.yaml \
    --num-samples 50000 \
    --output-dir data/dataset_custom
```

### 3. Train Phase 1: Neural PE

```bash
# Train neural parameter estimation network
python experiments/phase3a_neural_pe.py \
    --config configs/enhanced_training.yaml \
    --batch-size 32 \
    --epochs 100

# Monitor training
tensorboard --logdir outputs/
```

### 4. Train Phase 2: PriorityNet

```bash
# Train signal prioritization network
python experiments/train_priority_net.py \
    --config configs/priority_net.yaml \
    --create-overlaps \
    --batch-size 16

# Resume from checkpoint
python experiments/train_priority_net.py \
    --resume outputs/prioritynet_checkpoint.pth \
    --create-overlaps
```

### 5. Evaluate & Validate

```bash
# Full validation suite
python experiments/phase3c_validation.py \
    --phase3a_output outputs/phase3a_output_X/ \
    --phase3b_output outputs/phase3b_production/ \
    --n_samples 2000 \
    --seeds 5

# Expected output:
# âœ… System Success Rate: 82.1%
# âœ… Neural PE Accuracy: 0.582 Â± 0.087
# âœ… Subtraction Efficiency: 81.1%
```

---

## ğŸ“Š Performance Results

### System-Level Metrics

| Metric | Value | Notes |
|--------|-------|-------|
| **System Success Rate** | 82.1% | End-to-end detection of all signals |
| **Average Efficiency (Î·)** | 81.1% | Residual energy reduction |
| **Latency per 4s segment** | 156 ms | Dual-channel (H1, L1) |
| **Throughput** | 25.6 seg/s | Real-time capable |
| **Memory (8GB VRAM)** | Fits | Batch inference supported |

### Phase 1: Neural PE Accuracy

| Dataset | APE (mean) | APE (std) | Comments |
|---------|-----------|----------|----------|
| Clean (training) | 0.802 | 0.012 | Physics-perfect data |
| Contaminated (validation) | 0.582 | 0.087 | Realistic noise |
| After subtraction | 0.645 | 0.074 | Improved residuals |

### Phase 2: PriorityNet Ranking

| Metric | Value | Target |
|--------|-------|--------|
| Top-K Precision@1 | 96.6% | >95% |
| Ranking Correlation | 0.605 | >0.50 |
| Priority Accuracy | 94.6% | >90% |
| Calibration Error | <0.05 | <0.10 |

### Phase 3: Multi-Seed Verification

```
METRIC STABILITY ACROSS 5 SEEDS (200 samples each):
â”œâ”€ Neural PE Accuracy:  0.582 Â± 0.004  (variation: 0.1%)
â”œâ”€ Subtraction Î·:       0.811 Â± 0.001  (variation: <0.1%)
â”œâ”€ System Success:      0.821 Â± 0.008  (variation: 1.0%)
â””â”€ Statistical significance: Cohen's d > 2.0
```

---

## ğŸ“ Project Structure

```
PosteriFlow/
â”œâ”€â”€ ğŸ“ src/ahsd/                    # Main package
â”‚   â”œâ”€â”€ ğŸ“ core/                    # Core algorithms
â”‚   â”‚   â”œâ”€â”€ priority_net.py          # Signal prioritization (PriorityNet)
â”‚   â”‚   â”œâ”€â”€ adaptive_subtractor.py   # Adaptive subtraction + NeuralPE
â”‚   â”‚   â”œâ”€â”€ ahsd_pipeline.py         # Full end-to-end pipeline
â”‚   â”‚   â””â”€â”€ bias_corrector.py        # Parameter bias correction
â”‚   â”œâ”€â”€ ğŸ“ data/                    # Data generation & preprocessing
â”‚   â”‚   â”œâ”€â”€ dataset_generator.py     # Main dataset generator
â”‚   â”‚   â”œâ”€â”€ waveform_generator.py    # GW waveform synthesis (PyCBC)
â”‚   â”‚   â”œâ”€â”€ noise_generator.py       # Synthetic noise + glitches
â”‚   â”‚   â”œâ”€â”€ neural_noise_generator.py # FMPE neural noise (10kÃ— speedup)
â”‚   â”‚   â”œâ”€â”€ parameter_sampler.py     # Physics-constrained sampling
â”‚   â”‚   â”œâ”€â”€ psd_manager.py          # Power spectral density management
â”‚   â”‚   â”œâ”€â”€ gwtc_loader.py          # Real GWOSC data loading
â”‚   â”‚   â”œâ”€â”€ injection.py            # Signal injection into noise
â”‚   â”‚   â”œâ”€â”€ preprocessing.py        # Whitening, normalization
â”‚   â”‚   â””â”€â”€ config.py               # Config loading & validation
â”‚   â”œâ”€â”€ ğŸ“ models/                  # Neural network architectures
â”‚   â”‚   â”œâ”€â”€ neural_pe.py            # Neural PE normalizing flow
â”‚   â”‚   â”œâ”€â”€ overlap_neuralpe.py      # Multi-signal PE variant
â”‚   â”‚   â”œâ”€â”€ transformer_encoder.py   # TransformerStrainEncoder
â”‚   â”‚   â”œâ”€â”€ flows.py                # Flow architectures
â”‚   â”‚   â””â”€â”€ rl_controller.py         # RL-based control (future)
â”‚   â”œâ”€â”€ ğŸ“ evaluation/              # Metrics & analysis
â”‚   â”‚   â””â”€â”€ metrics.py              # APE, efficiency, ranking metrics
â”‚   â””â”€â”€ ğŸ“ utils/                   # Utilities
â”‚       â”œâ”€â”€ config.py               # Configuration classes
â”‚       â”œâ”€â”€ logging.py              # Logging setup
â”‚       â””â”€â”€ data_format.py           # Data standardization
â”œâ”€â”€ ğŸ“ experiments/                 # Training & evaluation scripts
â”‚   â”œâ”€â”€ phase3a_neural_pe.py        # Neural PE training
â”‚   â”œâ”€â”€ train_priority_net.py        # PriorityNet training
â”‚   â”œâ”€â”€ data_generation.py          # Dataset generation wrapper
â”‚   â””â”€â”€ phase3c_validation.py        # Multi-seed validation
â”œâ”€â”€ ğŸ“ configs/                     # Configuration files (YAML)
â”‚   â”œâ”€â”€ data_config.yaml            # Data generation parameters
â”‚   â”œâ”€â”€ enhanced_training.yaml      # Training hyperparameters
â”‚   â”œâ”€â”€ priority_net.yaml           # PriorityNet config
â”‚   â””â”€â”€ inference.yaml              # Inference settings
â”œâ”€â”€ ğŸ“ tests/                       # Unit & integration tests
â”‚   â”œâ”€â”€ test_dataset_generation.py
â”‚   â”œâ”€â”€ test_neural_pe.py
â”‚   â”œâ”€â”€ test_priority_net.py
â”‚   â””â”€â”€ test_integration.py
â”œâ”€â”€ ğŸ“ models/                      # Trained model checkpoints
â”‚   â”œâ”€â”€ neural_pe_best.pth
â”‚   â””â”€â”€ prioritynet_checkpoint.pth
â”œâ”€â”€ ğŸ“ data/                        # Generated datasets
â”‚   â”œâ”€â”€ dataset/
â”‚   â”‚   â”œâ”€â”€ train.pkl
â”‚   â”‚   â”œâ”€â”€ val.pkl
â”‚   â”‚   â””â”€â”€ test.pkl
â”‚   â””â”€â”€ Gaussian_network.pickle     # FMPE model (neural noise)
â”œâ”€â”€ ğŸ“ outputs/                     # Experiment results
â”‚   â”œâ”€â”€ phase3a_output_X/
â”‚   â”œâ”€â”€ phase3b_production/
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ ğŸ“ gw_segments_cleaned/         # Pre-cached GWOSC segments
â”‚   â””â”€â”€ [133 real noise segments]
â”œâ”€â”€ ğŸ“ notebooks/                   # Analysis & visualization
â”œâ”€â”€ ğŸ“ docs/                        # Additional documentation
â”œâ”€â”€ pyproject.toml                  # Package metadata & dependencies
â”œâ”€â”€ setup.py                        # Package setup
â”œâ”€â”€ AGENTS.md                       # Development guidelines
â””â”€â”€ README.md                       # This file
```

---

## ğŸ”§ Configuration System

All parameters are controlled via YAML configuration files in `configs/`:

### data_config.yaml - Dataset Generation

```yaml
# Core parameters
n_samples: 25000              # Number of samples to generate
sample_rate: 4096             # Hz (LIGO standard)
duration: 4.0                 # seconds
detectors: [H1, L1, V1]      # Detector network

# Signal characteristics
overlap_fraction: 0.45        # Realistic O4 rate
edge_case_fraction: 0.08      # Physical/statistical extremes
create_overlaps: true         # Enable multi-signal generation

# Contamination
add_glitches: true
neural_noise_enabled: true    # 10,000Ã— speedup
neural_noise_prob: 0.5        # 50% neural, 50% synthetic
use_real_noise_prob: 0.1      # 10% real GWOSC (cached)

# Event distribution (realistic O4)
event_type_distribution:
  BBH: 0.46                   # Most common
  BNS: 0.32                   # Rare but important
  NSBH: 0.17                  # Intermediate
  noise: 0.05                 # Background
```

### enhanced_training.yaml - Neural PE Training

```yaml
# Hyperparameters
learning_rate: 0.0005
batch_size: 32
epochs: 100
weight_decay: 1e-5

# Loss weights
loss_weights:
  mse: 0.35                   # Parameter estimation
  ranking: 0.50               # Ranking loss
  uncertainty: 0.15           # Calibration

# Data augmentation
augment_contamination: true
noise_augmentation_k: 1.0
preprocess: true
```

### priority_net.yaml - Signal Prioritization

```yaml
# Architecture
temporal_encoder_dim: 128
hidden_dim: 256
num_heads: 8                  # Multi-head attention

# Training
learning_rate: 0.0002
batch_size: 16
epochs: 80
create_overlaps: true         # Enable multi-signal training
```

---

## ğŸ§ª Testing

Run the comprehensive test suite:

```bash
# All tests
pytest

# Specific test
pytest tests/test_priority_net.py::TestPriorityNet::test_forward_pass -v

# With coverage
pytest --cov=ahsd --cov-report=html

# Verbose with print statements
pytest -v -s

# Specific test file
pytest tests/test_neural_pe.py
```

### Key Test Suites

| Test | Purpose | Location |
|------|---------|----------|
| Neural PE | Forward pass, loss computation | `tests/test_neural_pe.py` |
| PriorityNet | Signal ranking, feature extraction | `tests/test_priority_net.py` |
| Dataset | Data generation, splits, validation | `tests/test_dataset_generation.py` |
| Integration | End-to-end pipeline | `tests/test_integration.py` |

---

## ğŸ’¡ How to Use PosteriFlow

### Use Case 1: Train on Custom Data

1. Prepare real GW data in HDF5 format
2. Implement data reader in `src/ahsd/data/gwtc_loader.py`
3. Update `data_config.yaml` with real data paths
4. Run training pipeline

### Use Case 2: Parameter Estimation on New Events

```python
from ahsd.core.adaptive_subtractor import NeuralPE
import numpy as np

# Load strain data
strain_data = {
    'H1': np.load('H1_data.npy'),
    'L1': np.load('L1_data.npy'),
    'V1': np.load('V1_data.npy'),
}

# Quick estimation
pe = NeuralPE()
result = pe.quick_estimate(strain_data)

print(f"Mass 1: {result['mass_1_mean']:.1f} Mâ˜‰")
print(f"Distance: {result['luminosity_distance_mean']:.0f} Mpc")
print(f"SNR: {result['network_snr']:.1f}")
```

### Use Case 3: Signal Decomposition Pipeline

```python
from ahsd.core.ahsd_pipeline import AHSDPipeline

# Initialize pipeline
pipeline = AHSDPipeline(
    neural_pe_model='models/neural_pe_best.pth',
    priority_net_model='models/prioritynet_best.pth',
    subtractor_model='models/subtractor_best.pth',
)

# Process 4-second segment
result = pipeline.run(strain_data={
    'H1': h1_strain,
    'L1': l1_strain,
    'V1': v1_strain,
})

# Extracted signals
for i, signal in enumerate(result['extracted_signals']):
    print(f"\nSignal {i+1}:")
    print(f"  Mass 1: {signal['mass_1']:.1f} Mâ˜‰")
    print(f"  SNR: {signal['snr']:.1f}")
    print(f"  Confidence: {signal['priority_score']:.2f}")
```

---

## ğŸ”¬ Scientific Details

### Neural Posterior Estimation (Phase 1)

**Approach:** Likelihood-free inference using normalizing flows
- **Input:** Multi-detector strain (whitened, windowed)
- **Output:** Posterior samples of ~15 astrophysical parameters
- **Speed:** <100ms per 4s segment
- **Training:** On clean synthetic waveforms + augmented contamination

**Key Features:**
- Amortized inference: Single network for all parameters
- Uncertainty quantification: Full posterior ensemble
- Multi-detector coherence: Combines H1, L1, V1 optimally
- Robust to PSD variation: Data augmentation during training

### Signal Prioritization (Phase 2: PriorityNet)

**Approach:** Deep learning on temporal strain features
- **Architecture:** CNN (multi-scale) + BiLSTM (temporal) + Attention (context)
- **Input:** Whitened strain for multiple signals
- **Output:** Ranking order (which signal to subtract first)
- **Training:** On overlapping synthetic signals

**Why Prioritization Matters:**
- Extracting loud signal first reduces noise floor
- Removes contamination bias on faint signals
- Improves overall parameter estimation accuracy
- Handles multimodal posteriors better

### Adaptive Subtraction (Phase 3)

**Approach:** Iterative removal with uncertainty weighting
- **Step 1:** Identify signal with highest priority
- **Step 2:** Subtract using Neural PE parameters + uncertainties
- **Step 3:** Bias correction: Account for parameter errors
- **Step 4:** Validate residual Gaussianity
- **Step 5:** Repeat for remaining signals

**Uncertainty Weighting:**
- Larger uncertainties â†’ weaker subtraction (preserve signal)
- Calibrated uncertainties â†’ correct bias
- Cross-detector coherence check

---

## ğŸ“š References

### Key Papers

1. **PyCBC Waveforms**: [arXiv:1508.01844](https://arxiv.org/abs/1508.01844)
   - GW waveform generation and detection

2. **LIGO Data Conditioning**: [arXiv:2002.01606](https://arxiv.org/abs/2002.01606)
   - Real gravitational-wave detector noise

3. **Normalizing Flows**: [arXiv:1810.01367](https://arxiv.org/abs/1810.01367)
   - Flexible density estimation (used in Neural PE)

4. **DINGO**: [arXiv:2105.12151](https://arxiv.org/abs/2105.12151)
   - Deep inference for GW observations (basis for neural noise models)

### Data Sources

- **GWOSC**: [gwosc.readthedocs.io](https://gwosc.readthedocs.io/)
  - Public gravitational-wave detector data
- **GWTC-3**: [arXiv:2105.15615](https://arxiv.org/abs/2105.15615)
  - LIGO-Virgo third catalogs of GW transients

---

## ğŸ¤ Contributing

### Development Workflow

1. **Create feature branch**: `git checkout -b feature/description`
2. **Code style**: Follow AGENTS.md guidelines
3. **Test**: Run `pytest` before committing
4. **Format**: `black . && isort . && flake8 .`
5. **Commit message**: Descriptive, explain "why"
6. **Push & PR**: Create pull request with summary

### Code Standards

- **Type hints:** Always (required for all functions)
- **Docstrings:** NumPy format for classes and methods
- **Line length:** 100 characters (black formatter)
- **Testing:** Unit tests for new modules
- **Coverage:** Aim for >80% for new code

---

## ğŸ“ Support & Resources

### Documentation
- **Docs** - Use this folder to understand the core functionality and how to run the code

### Commands

```bash
# Data generation
ahsd-generate --config configs/data_config.yaml

# Validation
ahsd-validate --dataset data/dataset/train.pkl

# Analysis
ahsd-analyze --input-data data.hdf5 --output results.pkl

# Model training
python experiments/phase3a_neural_pe.py --config configs/enhanced_training.yaml

# Validation
python experiments/phase3c_validation.py --phase3a_output outputs/phase3a_output_X/ \
    --phase3b_output outputs/phase3b_production/ --n_samples 2000 --seeds 5
```

---

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) for details

---

## ğŸ‘¤ Author & Citation

**Author:** Bibin Thomas  
**Email:** bibinthomas951@gmail.com  
**Repository:** https://github.com/bibinthomas123/PosteriFlow

### Citation

If you use PosteriFlow in your research, please cite:

```bibtex
@software{thomas2025posteriflow,
  title={PosteriFlow: Adaptive Hierarchical Signal Decomposition 
         for Overlapping Gravitational Waves},
  author={Thomas, Bibin},
  year={2025},
  url={https://github.com/bibinthomas123/PosteriFlow}
}
```

---

## ğŸŒŸ Acknowledgments

PosteriFlow builds on foundational work from:
- **LIGO-Virgo Collaboration** for detector design and data access
- **PyCBC** for waveform generation
- **Bilby** for Bayesian inference tools
- **GWpy** for detector data handling
- **DINGO** for neural density estimation techniques

---

**Built for the next generation of gravitational-wave astronomy** ğŸŒŒ

*Last Updated: November 12, 2025*
