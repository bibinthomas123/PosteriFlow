# ========================= AHSD: Adaptive Hierarchical Signal Decomposition =========================
# Complete PriorityNet training configuration with all validated parameters
# Updated: 2025-11-10

experiment_name: "AHSD_Phase2_PriorityNet"
random_seed: 420
device: "auto"

# ==============================================================================
# PRIORITY NET CONFIGURATION (Core model parameters)
# ==============================================================================
priority_net:
  
  # --- ARCHITECTURE ---
  hidden_dims: [640, 512, 384, 256]      # ✅ Increased from [512, 384, 256, 128]
  dropout: 0.25                           # Dropout in priority head
  
  # --- MODEL FLAGS ---
  use_strain: true                       # Enable temporal strain encoder
  use_edge_conditioning: true            # Enable edge-type embedding
  n_edge_types: 19                        # Number of edge case types
  use_transformer_encoder: true          # ✅ Use Transformer encoder (Whisper+adapter, 1.5x faster, better accuracy)
  
  # --- OPTIMIZER ---
  optimizer: "AdamW"
  learning_rate: 8.0e-5
  weight_decay: 1.5e-5
  
  # --- TRAINING SCHEDULE ---
  batch_size: 48
  epochs: 200
  patience: 15                            
  
  # --- WARMUP ---
  warmup_epochs: 6
  warmup_start_factor: 0.02
  
  # --- LEARNING RATE SCHEDULER ---
  scheduler: "ReduceLROnPlateau"
  scheduler_patience: 7
  scheduler_factor: 0.5
  scheduler_threshold: 5e-3
  min_lr: 1.0e-6
  
  # --- LOSS FUNCTION ---
  ranking_weight: 0.70                    # Ranking loss weight (reduced from 0.60)
  mse_weight: 0.20                        # MSE loss weight (increased from 0.30)
  uncertainty_weight: 0.10                # ⬆️ Uncertainty loss weight (3.5x from 0.10) - Nov 13 fix
  use_snr_weighting: true                 # Weight losses by SNR
  loss_scale_factor: 0.002                # Scale factor for loss terms (from Nov 12 fix)
  
  # --- UNCERTAINTY CALIBRATION (NEW - Nov 13 fix) ---
  uncertainty_lower_bound: 0.01           # Prevent uncertainty collapse
  uncertainty_upper_bound: 0.50           # Prevent uncertainty explosion
  uncertainty_bounds_weight: 0.05         # Penalty weight for bounds violations
  
  # --- PREDICTION SATURATION FIX (NEW - Nov 13, UPDATED Nov 13 09:55) ---
  output_bias_init: 0.5                   # Initialize output bias to dataset mean (0.5 not 0.2)
  output_weight_std: 0.05                 # Weight std (5x larger for steeper initial gradients)
  calib_mean_weight: 0.20                 # Mean alignment penalty weight (↑ from 0.15)
  calib_max_weight: 0.75                  # Max range expansion penalty weight (↑ from 0.50) — dominates more
  calib_range_weight: 0.45                # Range regularization penalty weight (↑ from 0.30) — dynamic push
  affine_gain_min: 0.7                    # Affine gain lower bound (prevent collapse)
  affine_gain_max: 1.5                    # Affine gain upper bound (prevent explosion)
  affine_bias_min: -0.1                   # Affine bias lower bound
  affine_bias_max: 0.1                    # Affine bias upper bound
  
  # --- GRADIENT MANAGEMENT ---
  gradient_clip_norm: 2.0                 # Gradient clipping threshold
  gradient_log_threshold: 0.5             # Threshold for gradient norm logging
  
  # --- LABEL SMOOTHING ---
  label_smoothing: 0.0
  
  # --- ATTENTION/MODAL FUSION ---
  use_modal_fusion: true                 # Enable multi-modal attention fusion
  attention_num_heads: 8
  attention_dropout: 0.15
  
  # --- OVERLAP HANDLING ---
  overlap_use_attention: true             # Use attention for overlap encoding
  overlap_importance_hidden: 32           # Hidden dim for importance network



# ==============================================================================
# NEURAL POSTERIOR ESTIMATION (Flow-based parameter inference)
# ==============================================================================
neural_posterior:
  
  # --- FLOW TYPE (UNIFIED CONTROL) ---
  flow_type: "flowmatching"               # ✅ NEW: 'flowmatching' (OT-CFM) or 'realnvp'
  #                                        # Automatically adjusts architecture & training
  
  # --- PARAMETERS (9-dimensional parameter space) ---
  param_names:
    - mass_1                              # Primary mass
    - mass_2                              # Secondary mass
    - luminosity_distance                 # Distance to source
    - ra                                  # Right ascension
    - dec                                 # Declination
    - theta_jn                            # Inclination angle
    - psi                                 # Polarization angle
    - phase                               # Coalescence phase
    - geocent_time                        # Time at geocenter
  
  # --- ARCHITECTURE (auto-adjusted by flow_type) ---
  context_dim: 768                        # ⬆️ Increased from 512 (50% more capacity for overlapping signals)
  max_iterations: 5                       # Max adaptive refinement iterations
  
  # --- NORMALIZING FLOW (FlowMatching vs RealNVP) ---
  flow_config:
    # FlowMatching (OT-CFM): velocity-based, fewer layers, more expressive
    # RealNVP: coupling blocks, more layers, traditional approach
    
    # Auto-selected based on flow_type above, but can override:
    hidden_features: 256                  # Hidden features per flow block
    num_layers: 4                         # Layers: 4 for FlowMatching, 10 for RealNVP (⬆️ increased from 8)
    solver_steps: 10                      # ODE solver steps (FlowMatching only)
    num_blocks_per_layer: 2               # Coupling blocks per layer (RealNVP only)
    dropout: 0.10                         # Dropout for stability
  
  # --- EVENT-TYPE-SPECIFIC CONFIGURATION (Nov 13 enhancement) ---
  event_type: "BBH"                       # Options: BBH, BNS, NSBH (used for physics-informed priors)
  enable_event_specific_priors: true      # Use physics-informed priors per event type
  
  # --- TRAINING ---
  learning_rate: 3.0e-4
  batch_size: 64                          # Flow training batch size
  epochs: 40                              # Training epochs
  patience: 10
  weight_decay: 1.0e-5
  gradient_clip: 5.0
  optimizer: "AdamW"
  
  # --- LEARNING RATE SCHEDULER ---
  scheduler: "ReduceLROnPlateau"
  scheduler_patience: 8
  scheduler_factor: 0.6
  min_lr: 1.0e-6
  
  # --- PHYSICS LOSS (Nov 13 FIX: prevent out-of-range predictions) ---
  physics_loss_weight: 1.0                # ✅ Increased from 0.2 to 1.0 (hard constraint)
  bounds_penalty_weight: 0.01             # ✅ REDUCED from 1.0 → 0.01 (Nov 13 FIX: prevent Epoch 1 spike)
  
  # --- JACOBIAN REGULARIZATION ---
  jacobian_reg_weight: 0.001              # Flow weight regularization (10x default)
  
  # --- DATA AUGMENTATION ---
  data_augmentation:
    enabled: true
    noise_scaling: [0.98, 1.02]          # Noise level variation
    time_shifts: [-0.002, 0.002]         # Time shift range (seconds)
    apply_probability: 0.3                # Probability of augmentation


# ==============================================================================
# ADAPTIVE SUBTRACTION (Signal removal for overlaps)
# ==============================================================================
adaptive_subtractor:
  complexity_level: "medium"              # low, medium, high
  waveform_approximant: "IMRPhenomPv2"   # Waveform model
  post_newtonian_order: "3.5PN"          # PN order for waveforms
  
  # --- PHYSICS ---
  spin_effects: true                      # Include spin effects
  tidal_effects: true                     # Include tidal effects
  
  # --- SUBTRACTION STRENGTH ---
  base_strength: 0.1                      # Base subtraction strength
  max_strength: 0.8                       # Maximum strength
  uncertainty_threshold: 0.3              # Uncertainty threshold for adaptation
  match_threshold: 0.7                    # Match threshold for acceptance


# ==============================================================================
# REINFORCEMENT LEARNING CONTROLLER (Adaptive complexity)
# ==============================================================================
rl_controller:
  enabled: true
  
  # --- STATE ---
  state_features:
    - remaining_signals
    - residual_power
    - current_snr
    - extraction_success_rate
  
  # --- COMPLEXITY LEVELS ---
  complexity_levels: ["low", "medium", "high"]
  complexity_configs:
    low:
      flow_layers: 4
      inference_samples: 500
    medium:
      flow_layers: 8
      inference_samples: 1000
    high:
      flow_layers: 14
      inference_samples: 2000
  
  # --- LEARNING ---
  learning_rate: 1.0e-3
  epsilon: 0.1                            # Exploration rate
  epsilon_decay: 0.995
  memory_size: 10000
  batch_size: 32


# ==============================================================================
# BIAS CORRECTION NETWORK (Systematic error correction)
# ==============================================================================
bias_corrector:
  enabled: true
  
  # --- ARCHITECTURE ---
  hidden_dims: [256, 128, 64]
  context_dim: 16
  dropout: 0.10
  
  # --- TRAINING ---
  learning_rate: 1.0e-4
  batch_size: 64
  epochs: 15
  patience: 8


# ==============================================================================
# DATASET CONFIGURATION
# ==============================================================================
data:
  # --- SAMPLING ---
  sample_rate: 4096                       # Hz
  segment_duration: 4.0                   # seconds
  
  # --- FREQUENCY RANGE ---
  f_low: 20.0                             # Hz (low frequency cutoff)
  f_high: 1024.0                          # Hz (high frequency cutoff)
  
  # --- SPLITS ---
  validation_split: 0.15                  # Validation fraction
  test_split: 0.05                        # Test fraction


# ==============================================================================
# MONITORING & OUTPUT
# ==============================================================================
monitoring:
  save_frequency: 5                       # Save checkpoint every N epochs
  log_frequency: 1                        # Log metrics every N epochs
  early_stopping: true
  
  # --- METRICS TO TRACK ---
  metrics:
    - nll_loss
    - parameter_accuracy
    - extraction_efficiency
    - train_val_gap
    - gradient_norm
  
  # --- RL MONITORING ---
  rl_monitoring:
    track_complexity_changes: true
    log_rl_rewards: true
    complexity_distribution: true
  
  # --- BIAS CORRECTION MONITORING ---
  bias_monitoring:
    track_correction_effectiveness: true
    log_physics_violations: true

output:
  save_best_only: true                    # Save only best checkpoint
  save_intermediate: true                 # Save intermediate checkpoints
  generate_plots: true                    # Generate training plots


# ==============================================================================
# VALIDATION FLAGS (True/False Summary)
# ==============================================================================
# These flags are logged during training startup for debugging:
#
# PRIORITY NET:
#  ✓ use_strain: true (temporal strain encoder enabled)
#  ✓ use_edge_conditioning: true (edge-type embedding enabled)
#  ✓ use_transformer_encoder: true (legacy flag, not used in current code)
#  ✓ use_snr_weighting: true (SNR-weighted loss enabled)
#  ✓ use_modal_fusion: true (multi-modal attention fusion enabled)
#  ✓ attention_num_heads: 8 (number of attention heads in modal fusion)
#  ✓ attention_dropout: 0.15 (dropout in attention layers)
#  ✓ overlap_use_attention: true (attention-weighted pairwise overlap analysis)
#  ✓ overlap_importance_hidden: 32 (hidden dim for overlap importance network)
#  ✓ early_stopping: true
#  ✓ label_smoothing: 0.02
#
# NEURAL POSTERIOR:
#  ✓ data_augmentation.enabled: true
#
# ADAPTIVE SUBTRACTION:
#  ✓ spin_effects: true
#  ✓ tidal_effects: true
#
# RL CONTROLLER:
#  ✓ enabled: true
#  ✓ track_complexity_changes: true
#  ✓ log_rl_rewards: true
#
# BIAS CORRECTOR:
#  ✓ enabled: true
#  ✓ track_correction_effectiveness: true
#  ✓ log_physics_violations: true
#
# OUTPUT:
#  ✓ save_best_only: true
#  ✓ save_intermediate: true
#  ✓ generate_plots: true
