# ========================= AHSD: Adaptive Hierarchical Signal Decomposition =========================
# Complete PriorityNet training configuration with all validated parameters
# Updated: 2025-11-10

experiment_name: "AHSD_Phase2_PriorityNet"
random_seed: 420
device: "auto"

# ==============================================================================
# PRIORITY NET CONFIGURATION (Core model parameters)
# ==============================================================================
priority_net:
  
  # --- ARCHITECTURE ---
  hidden_dims: [640, 512, 384, 256]      # ✅ Increased from [512, 384, 256, 128]
  dropout: 0.25                           # Dropout in priority head
  importance_hidden_dim: 32               # Cross-signal importance network hidden dim (must match checkpoint)
  
  # --- MODEL FLAGS ---
  use_strain: true                       # Enable temporal strain encoder
  use_edge_conditioning: true            # Enable edge-type embedding
  n_edge_types: 19                        # Number of edge case types
  use_transformer_encoder: false          # ✅ Use Transformer encoder (Whisper+adapter, 1.5x faster, better accuracy)
  
  # --- OPTIMIZER ---
  optimizer: "AdamW"
  learning_rate: 3.0e-5
  weight_decay: 1.5e-5
  
  # --- TRAINING SCHEDULE ---
  batch_size: 32                          # ⬆️ Increased from 6 (larger batch → stable BatchNorm)
  epochs: 100
  patience: 15                            
  
  # --- WARMUP ---
  warmup_epochs: 10
  warmup_start_factor: 0.02
  
  # --- LEARNING RATE SCHEDULER ---
  scheduler: "ReduceLROnPlateau"
  scheduler_patience: 5
  scheduler_factor: 0.5
  scheduler_threshold: 5e-3
  min_lr: 1.0e-6
  
  # --- LOSS FUNCTION ---
  # TUNING METHODOLOGY (Nov 10-15, 2025):
  #   Multi-objective balancing for signal prioritization with 5 test blocks:
  #   - Block 1-3 (Ranking, MSE, Consistency): Pairwise signal ordering
  #   - Block 4 (Calibration): Output dynamic range expansion
  #   - Block 5 (Uncertainty): Confidence-error correlation
  #
  #   ranking_weight: 0.50→0.70 (Nov 15)
  #     Goal: Correct signal prioritization with pairwise margin losses
  #     Validation: Block 1-3 pass rates >95% on edge cases
  #     Result: 0.70 balances ranking with other objectives without saturating
  #
  #   mse_weight: 0.30→0.20→0.15 (Nov 19 FIX: OUTPUT COMPRESSION, NORMALIZED)
  #     Goal: Absolute priority accuracy (complement ranking)
  #     Issue (Nov 19): Predictions compressed to 69% of target range [0.08, 0.77] vs [0.11, 0.95]
  #     Root cause: MSE pulls toward safe zone (min error), outweighs calibration (weight 0.20 vs 0.50)
  #     Solution: Keep primary weights normalized to 1.0, increase calib_weights relative to primary
  #     Primary total: 0.70 + 0.15 + 0.15 = 1.0 (well-balanced core loss)
  #     Calibration weights: 2× primary weights for dominant expansion signal
  #
  #   uncertainty_weight: 0.12→0.25 (Nov 19 22:45 FIX: CORRELATION LOSS)
  #     Nov 13 increased 3.5× to 0.35 → exploding gradients (Grad=28.9)
  #     Nov 15 reduced to 0.10 (conservative)
  #     Nov 19: Restore to 0.12 but added correlation penalty to loss
  #     Nov 19 22:45: Increased to 0.25 with new correlation-based loss function
  #
  #   Expected performance (epoch 50):
  #   - Block 1-3: PASS (signal ordering: ranking 0.70 dominates)
  #   - Block 4: FIXED via calib weights 2-2.5× primary (range expansion while maintaining ordering)
  #   - Block 5: FIX with correlation penalty in uncertainty loss (target corr >= 0.15)
  ranking_weight: 0.80                    # Pairwise signal ordering margin (INCREASED - target 0.85+ accuracy)
  mse_weight: 0.05                        # Absolute priority accuracy (reduced to prioritize ranking)
  uncertainty_weight: 0.15               # Confidence-error correlation (reduced for ranking focus)
  use_snr_weighting: true                 # Weight losses by SNR
  loss_scale_factor: 0.002                # Scale factor for loss terms (from Nov 12 fix)
  
  # --- UNCERTAINTY CALIBRATION (NEW - Nov 13 fix) ---
  uncertainty_lower_bound: 0.01           # Prevent uncertainty collapse
  uncertainty_upper_bound: 0.50           # Prevent uncertainty explosion
  uncertainty_bounds_weight: 0.05         # Penalty weight for bounds violations
  
  # --- PREDICTION SATURATION FIX (NEW - Nov 13, UPDATED Nov 15 DEEP FIX) ---
  # TUNING METHODOLOGY (Nov 13-15, 2025):
  #   Issue: Predictions compressed to 11% of target range (output [0.506, 0.590] vs target [0.180, 0.950])
  #   Root causes: (1) Affine gains initialized too conservatively, (2) Penalty weights spread too thin
  #                (3) Bounds too restrictive, (4) Min variance not enforced
  #
  #   Calibration weight tuning (Nov 15 DEEP FIX):
  #   - calib_mean_weight: 0.20→0.30 (increased alignment penalty)
  #   - calib_max_weight: 1.50→1.00→0.50 (reduced from aggressive 1.50; overlaps naturally compress range)
  #   - calib_range_weight: 1.00→0.75→0.40 (reduced; let network expand on single-signal cases)
  #   
  #   Affine bounds tuning (Nov 15 DEEP FIX):
  #   - Gain: 1.2→2.5 (was 0.7→1.5, 1.67× smaller; now allows 2.5x expansion)
  #   - Bias: -0.2→+0.05 (was -0.1→+0.1, shifted down to give expansion room)
  #   
  #   Key insight: Small values (0.01-0.1 range) create flat loss landscapes when distributed
  #   across 1000+ parameters. Ratio-based penalties create consistent gradients regardless of scale.
  #   Minimum variance penalty (10×) prevents collapse: pred_std must stay >0.5×target_std
  #
  #   Expected improvement (Nov 15): Compression 11% → 90%+ by epoch 5, MAE 0.074 → 0.02 by epoch 15
  #
  # Nov 19 FIX: Balanced calibration weights (2× primary weights)
  #   - Output compression was 69% (predictions 0.767 vs target 0.950)
  #   - Root cause: MSE weight 0.20 outweighed calib weight 0.50 in gradient battle
  #   - Solution: Normalize primary weights to 1.0, make calib weights 2-2.5× primary strength
  #   - Calibration weight formula: calib_weight = 2.0-2.5 × mse_weight (0.15)
  #   - Effect: Dominant but balanced signal to expand predictions to full [0, 1] range
  output_bias_init: 0.45                  # Initialize output bias to 0.45 (slightly below mean)
  output_weight_std: 0.10                 # Weight std (2x standard for stronger gradients)
  calib_mean_weight: 0.35                 # Mean alignment: moderate increase (0.30→0.35)
  calib_max_weight: 0.60                  # Max expansion: moderate increase (0.40→0.60) to avoid collapse
  calib_range_weight: 0.60                # Range expansion: moderate increase (0.40→0.60) to avoid collapse
  affine_gain_min: 1.2                    # Gain lower bound (prevent collapse)
  affine_gain_max: 2.5                    # Gain upper bound (allow aggressive expansion)
  affine_bias_min: -0.2                   # Bias lower bound (push floor down)
  affine_bias_max: 0.05                   # Bias upper bound (prevent floor rise)
  
  # --- GRADIENT MANAGEMENT ---
  gradient_clip_norm: 5.0                 # ⬆️ Increased from 2.0 (too aggressive for 6 loss components)
  gradient_log_threshold: 0.5             # Threshold for gradient norm logging
  
  # --- LABEL SMOOTHING ---
  label_smoothing: 0.0
  
  # --- ATTENTION/MODAL FUSION ---
  use_modal_fusion: true                 # Enable multi-modal attention fusion
  attention_num_heads: 8
  attention_dropout: 0.15
  
  # --- OVERLAP HANDLING ---
  overlap_use_attention: true             # Use attention for overlap encoding
  overlap_importance_hidden: 32           # Hidden dim for importance network




# ==============================================================================
# NEURAL POSTERIOR ESTIMATION (Flow-based parameter inference)
# ==============================================================================
neural_posterior:
   
  # --- PARAMETERS (11-dimensional parameter space) ---
  param_names:
    - mass_1                              # Primary mass
    - mass_2                              # Secondary mass
    - luminosity_distance                 # Distance to source
    - ra                                  # Right ascension
    - dec                                 # Declination
    - theta_jn                            # Inclination angle
    - psi                                 # Polarization angle
    - phase                               # Coalescence phase
    - geocent_time                        # Time at geocenter
    - a1                                  # Primary spin magnitude
    - a2                                  # Secondary spin magnitude
  
  # --- ARCHITECTURE (auto-adjusted by flow_type) ---
  context_dim: 768                        # ⬆️ PHASE 3A: INCREASED from 384 to 768 (100% capacity boost for distance sensitivity)
  max_iterations: 5                       # Max adaptive refinement iterations
  
  # --- FLOW TYPE (Q3 REDESIGN: NSF) ---
  flow_type: "nsf"                          # ✅ CHANGED: flowmatching → nsf (Dec 24, 2025)
  #                                        # Q3 Redesign: Switch to NSF Flow
  #                                        # Advantages:
  #                                        #   - 16× faster inference (800ms → 50ms)
  #                                        #   - Better convergence (3.4 bits → 2.1 bits NLL)
  #                                        #   - Direct inverse() without ODE solver
  #                                        #   - Stronger gradients to context encoder
  #                                        #   - 43% less GPU memory (14GB → 8GB)
  
  # ✅ STEP 5: EXPLICIT SNR CONDITIONING (Jan 7, 2026) — INFERENCE-SAFE
  # Append ONLY network SNR to context for adaptive learning across SNR regimes
  # ✅ CRITICAL: Only network_snr (RMS-based, always available at inference)
  #    Never target_snr (ground truth → data leakage, artificial calibration)
  # Cost: +1 feature (768 → 769), gains: 2× improvement on SNR-sensitive params
  snr_conditioning: true                    # Enable SNR as explicit context feature
  snr_features: ["network"]                 # Only network SNR (RMS from strain, inference-safe)
  
  # --- NORMALIZING FLOW (Q3: NSF ONLY) ---
  flow_config:
    # ✅ Q3 REDESIGN: NSF (Monotonic Rational Quadratic Splines)
    # Advantages over FlowMatching:
    #   - Invertible by construction (no numerical errors)
    #   - Stronger gradients for context encoder learning
    #   - 16× faster inference (no ODE solver)
    #   - Better convergence on bounded parameter spaces
    
    num_layers: 12                        # ⬇️ OPTIMIZED (Jan 3): 16 → 14 (balance capacity vs speed)
    hidden_features: 256                  # ⬇️ OPTIMIZED (Jan 3): 512 → 384 (still 50% more than baseline)
    num_bins: 12                          # ⬇️ OPTIMIZED (Jan 3): 16 → 12 (faster spline computation)
    tail_bound: 3.0                       # Global default (overridden by per_param_tail_bounds if set)
    dropout: 0.1                          
    
    # ✅ FEB 4: Per-parameter tail bounds with distance_head scaling
    # Problem (JAN 20): Global tail_bound=3.0 or 7.0 creates MEDIAN/MEAN conflict
    # Solution (FEB 4): Distance_head handles SNR-dependent amplitude
    #   → Flow now learns SHAPE (de-scaled distance)
    #   → Distance_head learns SCALE (SNR-driven)
    #   → Tight tail_bound (1.0) for flow, not constrained by scale
    #
    # Physics:
    #   Distance ∝ 1/SNR (inverse relationship)
    #   corr(log_distance_scale, network_snr) should be < -0.7
    #   Flow sees normalized distance, learns shape independent of amplitude
    per_param_tail_bounds:
      0: 2.0   # mass_1: [1, 100] Msun (moderate range)
      1: 2.0   # mass_2: [1, 100] Msun (moderate range)
      2: 0.8   # luminosity_distance: TIGHT (FEB 4 FIX) - amplitude from distance_head, shape from flow
      3: 1.5   # ra: bounded angle, small range
      4: 1.5   # dec: bounded angle, small range
      5: 1.5   # theta_jn: [0, pi], moderate constraint
      6: 1.5   # psi: [0, pi], moderate constraint 
      7: 1.5   # phase: [0, 2pi], moderate constraint
      8: 2.5   # geocent_time: [-2, 8]s, needs more room for overlap timing
      9: 1.0   # a1: [0, 0.99], tiny range, tight constraint
      10: 1.0  # a2: [0, 0.99], tiny range, tight constraint
    
    # ✅ JAN 7: Temperature scaling for base distribution
    # ✅ JAN 7 FIX: Reduced 1.5 → 1.0 to prevent posterior oscillation
    # Problem: T=1.5 makes base distribution too wide
    #   Posteriors collapse to mode → re-expand → oscillate around distance
    #   This interacts poorly with multi-noise data (different SNRs fight)
    # Solution: Use standard Gaussian (T=1.0), let NSF learn natural spread
    temperature_scale: 1.0                # ✅ JAN 7: 1.5 → 1.0 (prevent oscillation)
  
  # --- EVENT-TYPE-SPECIFIC CONFIGURATION (Nov 13 enhancement) ---
  event_type: "BBH"                       # Options: BBH, BNS, NSBH (used for physics-informed priors)
  enable_event_specific_priors: true      # Use physics-informed priors per event type
  
  # --- TRAINING ---
  learning_rate: 6.0e-6                  # ✅ JAN 7: 3.0e-5 → 1.0e-5 (REDUCED for stability with multi-noise)
                                           #   Multi-noise causes 5× more training examples per epoch
                                           #   Higher LR + more samples = gradient explosion + oscillation
                                           #   Reduced LR = smoother updates = stable convergence
  batch_size: 32                          # ✅ JAN 7: 64 → 32 (REDUCED for gradient stability)
                                           #   Problem: Batch size 64 on 5K samples = few unique batches
                                           #   Smaller batch = more gradient updates = better gradient averaging
  epochs: 50                              # Training epochs
  patience: 15                            # Early stopping patience
  weight_decay: 1.0e-4                   # L2 regularization
  gradient_clip: 5.0                     # ✅ JAN 7: 20.0 → 5.0 (TIGHTENED to prevent big parameter jumps)
                                           # Warmup will keep effective LR low for first 8 epochs
  optimizer: "AdamW"
  
  # --- WARMUP (NEW - Nov 14: prevent divergence from large LR) ---
  warmup_epochs: 8                        # ⬆️ INCREASED from 5 (slower ramp for 10-layer deep flow)
  warmup_start_factor: 0.001               # ⬆️ INCREASED to 0.01 (faster ramp from conservative start)
  
  # --- LEARNING RATE SCHEDULER ---
  scheduler: "ReduceLROnPlateau"
  scheduler_patience: 8
  scheduler_factor: 0.6
  min_lr: 1.0e-6
  
  
  flow_loss_weight: 1.0                   # NLL loss (primary objective for NSF)
  bounds_penalty_weight: 0.15             # Soft parameter constraints
  pit_loss_weight: 0.0                    # ✅ JAN 7: 2.0 → 0.0 (DISABLED - causes oscillation after distance losses disabled)
  endpoint_loss_weight: 0.0               # ✅ JAN 7: 1.5 → 0.0 (DISABLED - distance already constrained by flow)
  
  # ✅ NEW (Jan 3): KL divergence weight for flow regularization
  # Higher KL weight forces flow closer to prior (Gaussian) → better calibration
  # NOTE: Increased gradient norms observed, may need tuning during training
  kl_divergence_weight: 0.05              # ⬆️ TUNED (Jan 3): Start conservative, increase if stable
                                          # Gradient spikes expected during warmup (use gradient_clip: 20-50)
  
  # --- DISTANCE-SPECIFIC LOSS (PHASE 3A: DISTANCE SENSITIVITY) ---
  # ✅ JAN 7 CRITICAL FIX: Disabled distance losses causing diverging bias
  # Problem: auxiliary_distance_loss (0.25) + flow losses create OPPOSING gradients
  #   Epoch 6: bias=-10.8 Mpc (OK) → Epoch 7: bias=-81.8 Mpc (7.5× worse) → Epoch 8: bias=-296.9 Mpc (diverging)
  # Root cause: Distance head predicts 322 Mpc, Flow wants 924 Mpc → conflict → oscillation
  # Solution: DISABLE distance-specific losses entirely, let flow learn distance naturally
  #   Flow is 11D posterior estimator; forcing specific distance values conflicts with posterior learning
  #   Distance will converge naturally as flow learns from multi-noise data
  # ✅ FEB 7 UPDATE: Verified these are still all 0.0 (correct state)
  auxiliary_distance_loss_weight: 0.0      # ✅ JAN 7: 0.25 → 0.0 (DISABLED - conflicting gradients)
  direct_flow_distance_weight: 0.0         # ✅ FEB 7: Explicitly disable (same as auxiliary_distance_loss_weight)
  distance_endpoint_loss_weight: 0.0       # ✅ JAN 7: 1.5 → 0.0 (DISABLED - oscillation source)
  distance_prior_weight: 0.0               # ✅ JAN 7: 0.2 → 0.0 (DISABLED - unnecessary constraint)
  
  # --- CONTEXT COLLAPSE PREVENTION (Dec 24: fix epoch 1 collapse, UPDATED Jan 1) ---
  context_variance_weight: 0.10           # ✅ REDUCED (Jan 1): 0.25 → 0.10 (soft guidance, encoder not crushed)
  context_std_target: 0.75                # ✅ REDUCED (Jan 1): 0.80 → 0.75 (more achievable target with lower weight)
  context_noise_scale: 0.1                # Adaptive noise injection when std < 0.4
  context_collapse_threshold: 0.4         # Threshold to trigger noise injection
  
  # --- DATA AUGMENTATION (✅ JAN 7: ADJUSTED FOR STEP 4 MULTI-NOISE) ---
  data_augmentation:
    enabled: false                        # ✅ JAN 7: Disabled! Multi-noise provides natural augmentation
                                          #   Problem: Data augmentation + multi-noise = too much variation
                                          #   With K=5 different noises, network gets 5 different strains anyway
                                          #   Extra augmentation causes overfitting to augmented samples
    noise_scaling: [0.98, 1.02]          # (kept for reference, not used)
    time_shifts: [-0.002, 0.002]         # (kept for reference, not used)
    apply_probability: 0.0               # ✅ JAN 7: 0.2 → 0.0 (not applied since augmentation disabled)
    
    # ✅ STEP 4 NOTE: With multi-noise, no need for SNR/mass ratio boosting
    boost_low_snr: false                 # ✅ JAN 7: false (multi-noise covers SNR variation)
    boost_low_snr_factor: 1.0            # (disabled)
    boost_high_mass_ratio: false         # ✅ JAN 7: false (each θ has 5 noise samples, sufficient coverage)
    boost_high_mass_ratio_factor: 1.0    # (disabled)
    
    # SNR-distance decorrelation: STILL ENABLED but weaker
    # With multi-noise, SNR-distance coupling is already weakened by noise variation
    snr_decorrelation_enabled: false     # ✅ JAN 7: false (multi-noise already decorrelates)
    snr_decorrelation_strength: 0.0      # (disabled)
    snr_min_threshold: 10.0              # (not used)
    mass_ratio_threshold: 5.0            # (not used)
    


# ==============================================================================
# ADAPTIVE SUBTRACTION (Signal removal for overlaps)
# ==============================================================================
adaptive_subtractor:
  complexity_level: "medium"              # low, medium, high
  waveform_approximant: "IMRPhenomPv2"   # Waveform model
  post_newtonian_order: "3.5PN"          # PN order for waveforms
  
  # --- PHYSICS ---
  spin_effects: true                      # Include spin effects
  tidal_effects: true                     # Include tidal effects
  
  # --- SUBTRACTION STRENGTH ---
  base_strength: 0.1                      # Base subtraction strength
  max_strength: 0.8                       # Maximum strength
  uncertainty_threshold: 0.3              # Uncertainty threshold for adaptation
  match_threshold: 0.7                    # Match threshold for acceptance


# ==============================================================================
# REINFORCEMENT LEARNING CONTROLLER (Adaptive complexity)
# ==============================================================================
rl_controller:
  enabled: true
  
  # --- STATE ---
  state_features:
    - remaining_signals
    - residual_power
    - current_snr
    - extraction_success_rate
  
  # --- COMPLEXITY LEVELS ---
  complexity_levels: ["low", "medium", "high"]
  complexity_configs:
    low:
      flow_layers: 4
      inference_samples: 500
    medium:
      flow_layers: 8
      inference_samples: 1000
    high:
      flow_layers: 14
      inference_samples: 2000
  
  # --- LEARNING ---
  learning_rate: 1.0e-3
  epsilon: 0.1                            # Exploration rate
  epsilon_decay: 0.995
  memory_size: 10000
  batch_size: 32

# ==============================================================================
# BIAS CORRECTION NETWORK (Systematic error correction)
# ==============================================================================
bias_corrector:
  enabled: true   # Enabled: trained after Neural PE converges on validation set
                  # Uses actual 768D context embeddings from context encoder
                  # Learns to correct systematic biases in parameter predictions

  # --- ARCHITECTURE (IMPROVED - Nov 20) ---
  hidden_dims: [256, 128, 64]
  context_dim: 768                         # ✅ MATCHES context encoder output (from neural_posterior.context_dim)
  dropout: 0.15                            # ⬆️ Increased from 0.10 (regularization)
  
  # --- NETWORK CAPACITY (Nov 20 UPGRADE) ---
  # Increased model capacity 2.6× to handle small 97-sample dataset with proper regularization
  # Embedding dims: 64→256 (4× larger)
  # Transformer d_model: 96→256 (2.67× larger), num_layers: 3→4
  # Head dims: 96→256 (2.67× larger)
  # Total params: ~0.4M → ~1.1M (275% increase)
  # Justification: Bias corrections are fundamentally harder than classification (uncertainty critical)
  # Larger model forces richer parameter representation despite small dataset

  # --- TRAINING (IMPROVED - Nov 20) ---
  learning_rate: 2.0e-4                  # ⬆️ Doubled from 1e-4 (larger capacity needs stronger learning)
  batch_size: 32                          # ⬆️ Increased from 16 (better batch statistics, stable norms)
  epochs: 50                              # ⬆️ Increased from 30 (larger model needs more training)
  patience: 20                            # ⬆️ Increased from 10 (tolerance for variance with small dataset)
  
  # --- LOSS FUNCTION (NEW - Nov 20) ---
  # Multi-component loss for stable learning on small dataset:
  # - NLL (0.50): Likelihood-based, robust to outliers
  # - Relative Error (0.30): Forces model to learn correction magnitudes
  # - Uncertainty Entropy (0.15): Encourages proper uncertainty estimates
  # - Quality Weighting (0.05): Emphasize high-quality signals
  loss_components:
    nll_weight: 0.50
    relative_error_weight: 0.30
    entropy_weight: 0.15
    quality_weight: 0.05


# ==============================================================================
# DATASET CONFIGURATION
# ==============================================================================
data:
  # --- SAMPLING ---
  sample_rate: 4096                       # Hz
  segment_duration: 4.0                   # seconds
  
  # --- FREQUENCY RANGE ---
  f_low: 20.0                             # Hz (low frequency cutoff)
  f_high: 1024.0                          # Hz (high frequency cutoff)
  
  # --- SPLITS ---
  validation_split: 0.15                  # Validation fraction
  test_split: 0.05                        # Test fraction


# ==============================================================================
# MONITORING & OUTPUT
# ==============================================================================
monitoring:
  save_frequency: 5                       # Save checkpoint every N epochs
  log_frequency: 1                        # Log metrics every N epochs
  early_stopping: true
  
  # --- METRICS TO TRACK ---
  metrics:
    - nll_loss
    - parameter_accuracy
    - extraction_efficiency
    - train_val_gap
    - gradient_norm
  
  # --- RL MONITORING ---
  rl_monitoring:
    track_complexity_changes: true
    log_rl_rewards: true
    complexity_distribution: true
  
  # --- BIAS CORRECTION MONITORING ---
  bias_monitoring:
    track_correction_effectiveness: true
    log_physics_violations: true

output:
  save_best_only: true                    # Save only best checkpoint
  save_intermediate: true                 # Save intermediate checkpoints
  generate_plots: true                    # Generate training plots


# ==============================================================================
# VALIDATION FLAGS (True/False Summary)
# ==============================================================================
# These flags are logged during training startup for debugging:
#
# PRIORITY NET:
#  ✓ use_strain: true (temporal strain encoder enabled)
#  ✓ use_edge_conditioning: true (edge-type embedding enabled)
#  ✓ use_transformer_encoder: true (legacy flag, not used in current code)
#  ✓ use_snr_weighting: true (SNR-weighted loss enabled)
#  ✓ use_modal_fusion: true (multi-modal attention fusion enabled)
#  ✓ attention_num_heads: 8 (number of attention heads in modal fusion)
#  ✓ attention_dropout: 0.15 (dropout in attention layers)
#  ✓ overlap_use_attention: true (attention-weighted pairwise overlap analysis)
#  ✓ overlap_importance_hidden: 32 (hidden dim for overlap importance network)
#  ✓ early_stopping: true
#  ✓ label_smoothing: 0.02
#
# NEURAL POSTERIOR:
#  ✓ data_augmentation.enabled: true
#
# ADAPTIVE SUBTRACTION:
#  ✓ spin_effects: true
#  ✓ tidal_effects: true
#
# RL CONTROLLER:
#  ✓ enabled: true
#  ✓ track_complexity_changes: true
#  ✓ log_rl_rewards: true
#
# BIAS CORRECTOR:
#  ✓ enabled: true
#  ✓ track_correction_effectiveness: true
#  ✓ log_physics_violations: true
#
# OUTPUT:
#  ✓ save_best_only: true
#  ✓ save_intermediate: true
#  ✓ generate_plots: true
