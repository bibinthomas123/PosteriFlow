# ========================= AHSD: Adaptive Hierarchical Signal Decomposition =========================
# Complete PriorityNet training configuration with all validated parameters
# Updated: 2025-11-10

experiment_name: "AHSD_Phase2_PriorityNet"
random_seed: 420
device: "auto"

# ==============================================================================
# TRAINING STAGE & COMPONENT CONFIGURATION (Nov 14, 2025)
# ==============================================================================
stage: "1A"                                  # Stage 1A: Neural PE training (PriorityNet disabled)
                                             # Stage 1B: RL controller training (PriorityNet frozen)
                                             # Stage 1C: Joint fine-tuning (all components trained)

components:
  priority_net: false                        # ⊘ DISABLED in Stage 1A (enables in 1B+)
  context_encoder: true                      # ✅ TRAINING: Context encoding for flow
  normalizing_flow: true                     # ✅ TRAINING: Flow-based posterior estimation
  bias_corrector: true                       # ✅ TRAINING: Systematic error correction
  uncertainty_estimator: true                # ✅ TRAINING: Confidence calibration
  rl_controller: false                       # ⊘ INACTIVE in Stage 1A (activates in 1B)
  adaptive_subtractor: false                 # ⊘ UNUSED: Signal subtraction (future)

# ==============================================================================
# PRIORITY NET CONFIGURATION (Core model parameters)
# ==============================================================================
priority_net:
  
  # --- ARCHITECTURE ---
  hidden_dims: [640, 512, 384, 256]      # ✅ Increased from [512, 384, 256, 128]
  dropout: 0.25                           # Dropout in priority head
  importance_hidden_dim: 32               # Cross-signal importance network hidden dim (must match checkpoint)
  
  # --- MODEL FLAGS ---
  use_strain: true                       # Enable temporal strain encoder
  use_edge_conditioning: true            # Enable edge-type embedding
  n_edge_types: 19                        # Number of edge case types
  use_transformer_encoder: true          # ✅ Use Transformer encoder (Whisper+adapter, 1.5x faster, better accuracy)
  
  # --- OPTIMIZER ---
  optimizer: "AdamW"
  learning_rate: 3.0e-5
  weight_decay: 1.5e-5
  
  # --- TRAINING SCHEDULE ---
  batch_size: 24                          # ⬆️ Increased from 6 (larger batch → stable BatchNorm)
  epochs: 200
  patience: 15                            
  
  # --- WARMUP ---
  warmup_epochs: 6
  warmup_start_factor: 0.02
  
  # --- LEARNING RATE SCHEDULER ---
  scheduler: "ReduceLROnPlateau"
  scheduler_patience: 7
  scheduler_factor: 0.5
  scheduler_threshold: 5e-3
  min_lr: 1.0e-6
  
  # --- LOSS FUNCTION ---
  ranking_weight: 0.70                    # Ranking loss weight (reduced from 0.60)
  mse_weight: 0.20                        # MSE loss weight (increased from 0.30)
  uncertainty_weight: 0.10                # ⬇️ Reduced from 0.35 (3.5× increase was too aggressive) - Nov 15 fix
  use_snr_weighting: true                 # Weight losses by SNR
  loss_scale_factor: 0.002                # Scale factor for loss terms (from Nov 12 fix)
  
  # --- UNCERTAINTY CALIBRATION (NEW - Nov 13 fix) ---
  uncertainty_lower_bound: 0.01           # Prevent uncertainty collapse
  uncertainty_upper_bound: 0.50           # Prevent uncertainty explosion
  uncertainty_bounds_weight: 0.05         # Penalty weight for bounds violations
  
  # --- PREDICTION SATURATION FIX (NEW - Nov 13, UPDATED Nov 15 DEEP FIX) ---
  output_bias_init: 0.45                  # Initialize output bias to 0.45 (slightly below mean to help range)
  output_weight_std: 0.10                 # Weight std (10x larger for even steeper initial gradients)
  calib_mean_weight: 0.30                 # Mean alignment penalty weight (↑ from 0.20)
  calib_max_weight: 0.50                  # ⬇️ Reduced from 1.00 (overlaps lower SNR naturally; don't suppress gradients)
  calib_range_weight: 0.40                # ⬇️ Reduced from 0.75 (same reason; let network expand on single-signal cases)
  affine_gain_min: 1.2                    # Affine gain lower bound (↑ from 0.7) — prevent collapse below 1.2x
  affine_gain_max: 2.5                    # Affine gain upper bound (↑ from 1.5) — allow aggressive expansion
  affine_bias_min: -0.2                   # Affine bias lower bound (↑ from -0.1) — push floor down
  affine_bias_max: 0.05                   # Affine bias upper bound (↓ from 0.1) — prevent floor rise
  
  # --- GRADIENT MANAGEMENT ---
  gradient_clip_norm: 5.0                 # ⬆️ Increased from 2.0 (too aggressive for 6 loss components)
  gradient_log_threshold: 0.5             # Threshold for gradient norm logging
  
  # --- LABEL SMOOTHING ---
  label_smoothing: 0.0
  
  # --- ATTENTION/MODAL FUSION ---
  use_modal_fusion: true                 # Enable multi-modal attention fusion
  attention_num_heads: 8
  attention_dropout: 0.15
  
  # --- OVERLAP HANDLING ---
  overlap_use_attention: true             # Use attention for overlap encoding
  overlap_importance_hidden: 32           # Hidden dim for importance network




# ==============================================================================
# NEURAL POSTERIOR ESTIMATION (Flow-based parameter inference)
# ==============================================================================
neural_posterior:
  
  # --- PARAMETERS (9-dimensional parameter space) ---
  param_names:
    - mass_1                              # Primary mass
    - mass_2                              # Secondary mass
    - luminosity_distance                 # Distance to source
    - ra                                  # Right ascension
    - dec                                 # Declination
    - theta_jn                            # Inclination angle
    - psi                                 # Polarization angle
    - phase                               # Coalescence phase
    - geocent_time                        # Time at geocenter
  
  # --- ARCHITECTURE (auto-adjusted by flow_type) ---
  context_dim: 768                        # ⬆️ Increased from 512 (50% more capacity for overlapping signals)
  max_iterations: 5                       # Max adaptive refinement iterations
  
  # --- FLOW TYPE (UNIFIED CONTROL) ---
  flow_type: "flowmatching"                        # ✅ FlowMatching (OT-CFM based)
  #                                        # Options: 'flowmatching', 'realnvp', 'nsf'
  #                                        # Note: Checkpoint (final_model.pth) was trained with NSF
  #                                        # Will have architecture mismatch - retrain for best performance
  
  # --- NORMALIZING FLOW (NSF vs FlowMatching vs RealNVP) ---
  flow_config:
    # Neural Spline Flow (NSF): RECOMMENDED
    #   - Monotonic rational quadratic splines (invertible by construction)
    #   - No ODE solver approximation errors
    #   - Better for bounded parameter spaces
    #   - Expected NLL: 4-5 bits by epoch 10 (vs FlowMatching 8+ bits)
    # 
    # FlowMatching (OT-CFM): velocity-based, ODE solver approximation
    # RealNVP: coupling blocks, many layers for expressiveness
    
    # ✅ NSF CONFIG (OPTIMAL FOR 9D POSTERIOR)
    num_layers: 12                         # Fewer layers than FlowMatching (splines are more expressive)
    hidden_features: 512                  # Moderate hidden dim (splines naturally model monotonic transforms)
    num_bins: 16                          # Spline resolution - controls piecewise continuity
    tail_bound: 3.0                       # Auto-handles extreme tails (handles [-∞, ∞])
    dropout: 0.1                          # Dropout for stability
    
    # (Ignored for NSF - kept for backward compat with other flow types)
    solver_steps: 50                      # Only for FlowMatching
    num_blocks_per_layer: 2               # Only for RealNVP
  
  # --- EVENT-TYPE-SPECIFIC CONFIGURATION (Nov 13 enhancement) ---
  event_type: "BBH"                       # Options: BBH, BNS, NSBH (used for physics-informed priors)
  enable_event_specific_priors: true      # Use physics-informed priors per event type
  
  # --- TRAINING ---
  # learning_rate: 1.0e-5                  # ⬇️ FURTHER REDUCED from 2.5e-5 (prevent oscillations at convergence) - Nov 14
  learning_rate: 5.0e-5
  batch_size: 64                          # ⬆️ INCREASED to 64 (better gradient estimates for deep flow)
  epochs: 500                               # ⬆️ INCREASED from 30 to allow deep flow to converge
  patience: 15                            # ⬆️ INCREASED from 10 (monitor through epoch 20-30)
  weight_decay: 1.5e-5                   # ⬆️ SLIGHTLY increased (better regularization for large model)
  gradient_clip: 5.0                       # ⬇️ TIGHTER clipping (prevent exploding gradients) - Nov 14: reduced 10→5
  optimizer: "AdamW"
  
  # --- WARMUP (NEW - Nov 14: prevent divergence from large LR) ---
  warmup_epochs: 8                        # ⬆️ INCREASED from 5 (slower ramp for 10-layer deep flow)
  warmup_start_factor: 0.001               # ⬆️ INCREASED to 0.01 (faster ramp from conservative start)
  
  # --- LEARNING RATE SCHEDULER ---
  scheduler: "ReduceLROnPlateau"
  scheduler_patience: 8
  scheduler_factor: 0.6
  min_lr: 1.0e-6
  
  # --- PHYSICS LOSS (Nov 14 FIX: NLL is 24+ bits, flow not learning) ---
  # ⚠️ ROOT CAUSE: Old checkpoint with wrong architecture + physics loss preventing learning
  # Solution: Fresh start with ZERO constraints, let NLL minimize first
  # Epochs 1-20: physics_loss_weight=0.0 (NLL-only training for likelihood)
  # Epochs 20+: Enable bounds_penalty_weight=0.5 to enforce valid ranges
  physics_loss_weight: 0.02                # ⬇️ ZERO: Don't constrain during initial learning
  bounds_penalty_weight: 0.8              # Keep for ground truth penalties
  sample_loss_weight: 0.2                 # ⬇️ ZERO: Let NLL dominate in first 20 epochs
  
  # --- JACOBIAN REGULARIZATION ---
  jacobian_reg_weight: 0.001              # Flow weight regularization (10x default)
  
  # --- DATA AUGMENTATION ---
  data_augmentation:
    enabled: true
    noise_scaling: [0.98, 1.02]          # Noise level variation
    time_shifts: [-0.002, 0.002]         # Time shift range (seconds)
    apply_probability: 0.3               # Probability of augmentation


# ==============================================================================
# ADAPTIVE SUBTRACTION (Signal removal for overlaps)
# ==============================================================================
adaptive_subtractor:
  complexity_level: "medium"              # low, medium, high
  waveform_approximant: "IMRPhenomPv2"   # Waveform model
  post_newtonian_order: "3.5PN"          # PN order for waveforms
  
  # --- PHYSICS ---
  spin_effects: true                      # Include spin effects
  tidal_effects: true                     # Include tidal effects
  
  # --- SUBTRACTION STRENGTH ---
  base_strength: 0.1                      # Base subtraction strength
  max_strength: 0.8                       # Maximum strength
  uncertainty_threshold: 0.3              # Uncertainty threshold for adaptation
  match_threshold: 0.7                    # Match threshold for acceptance


# ==============================================================================
# REINFORCEMENT LEARNING CONTROLLER (Adaptive complexity)
# ==============================================================================
rl_controller:
  enabled: true
  
  # --- STATE ---
  state_features:
    - remaining_signals
    - residual_power
    - current_snr
    - extraction_success_rate
  
  # --- COMPLEXITY LEVELS ---
  complexity_levels: ["low", "medium", "high"]
  complexity_configs:
    low:
      flow_layers: 4
      inference_samples: 500
    medium:
      flow_layers: 8
      inference_samples: 1000
    high:
      flow_layers: 14
      inference_samples: 2000
  
  # --- LEARNING ---
  learning_rate: 1.0e-3
  epsilon: 0.1                            # Exploration rate
  epsilon_decay: 0.995
  memory_size: 10000
  batch_size: 32


# ==============================================================================
# BIAS CORRECTION NETWORK (Systematic error correction)
# ==============================================================================
bias_corrector:
  enabled: false  # Disabled during neural PE training (adds noise without learning)
                  # The untrained uncertainty head outputs constant 0.69 → constant confidence 0.59
                  # Re-enable only after neural PE converges, for final refinement
  
  # --- ARCHITECTURE ---
  hidden_dims: [256, 128, 64]
  context_dim: 16
  dropout: 0.10
  
  # --- TRAINING ---
  learning_rate: 1.0e-4
  batch_size: 64
  epochs: 15
  patience: 8


# ==============================================================================
# DATASET CONFIGURATION
# ==============================================================================
data:
  # --- SAMPLING ---
  sample_rate: 4096                       # Hz
  segment_duration: 4.0                   # seconds
  
  # --- FREQUENCY RANGE ---
  f_low: 20.0                             # Hz (low frequency cutoff)
  f_high: 1024.0                          # Hz (high frequency cutoff)
  
  # --- SPLITS ---
  validation_split: 0.15                  # Validation fraction
  test_split: 0.05                        # Test fraction


# ==============================================================================
# MONITORING & OUTPUT
# ==============================================================================
monitoring:
  save_frequency: 5                       # Save checkpoint every N epochs
  log_frequency: 1                        # Log metrics every N epochs
  early_stopping: true
  
  # --- METRICS TO TRACK ---
  metrics:
    - nll_loss
    - parameter_accuracy
    - extraction_efficiency
    - train_val_gap
    - gradient_norm
  
  # --- RL MONITORING ---
  rl_monitoring:
    track_complexity_changes: true
    log_rl_rewards: true
    complexity_distribution: true
  
  # --- BIAS CORRECTION MONITORING ---
  bias_monitoring:
    track_correction_effectiveness: true
    log_physics_violations: true

output:
  save_best_only: true                    # Save only best checkpoint
  save_intermediate: true                 # Save intermediate checkpoints
  generate_plots: true                    # Generate training plots


# ==============================================================================
# VALIDATION FLAGS (True/False Summary)
# ==============================================================================
# These flags are logged during training startup for debugging:
#
# PRIORITY NET:
#  ✓ use_strain: true (temporal strain encoder enabled)
#  ✓ use_edge_conditioning: true (edge-type embedding enabled)
#  ✓ use_transformer_encoder: true (legacy flag, not used in current code)
#  ✓ use_snr_weighting: true (SNR-weighted loss enabled)
#  ✓ use_modal_fusion: true (multi-modal attention fusion enabled)
#  ✓ attention_num_heads: 8 (number of attention heads in modal fusion)
#  ✓ attention_dropout: 0.15 (dropout in attention layers)
#  ✓ overlap_use_attention: true (attention-weighted pairwise overlap analysis)
#  ✓ overlap_importance_hidden: 32 (hidden dim for overlap importance network)
#  ✓ early_stopping: true
#  ✓ label_smoothing: 0.02
#
# NEURAL POSTERIOR:
#  ✓ data_augmentation.enabled: true
#
# ADAPTIVE SUBTRACTION:
#  ✓ spin_effects: true
#  ✓ tidal_effects: true
#
# RL CONTROLLER:
#  ✓ enabled: true
#  ✓ track_complexity_changes: true
#  ✓ log_rl_rewards: true
#
# BIAS CORRECTOR:
#  ✓ enabled: true
#  ✓ track_correction_effectiveness: true
#  ✓ log_physics_violations: true
#
# OUTPUT:
#  ✓ save_best_only: true
#  ✓ save_intermediate: true
#  ✓ generate_plots: true
