{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PriorityNet Training & Evaluation\n",
    "\n",
    "**Fully trainable notebook** for PriorityNet - the intelligent signal ordering model in PosteriFlow.\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. Run cells sequentially from top to bottom\n",
    "2. Data will be generated automatically if needed\n",
    "3. Model trains and saves checkpoints\n",
    "4. Validation runs automatically each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "import logging\n",
    "from types import SimpleNamespace\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup paths\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "sys.path.insert(0, str(project_root / \"experiments\"))\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('PriorityNetTraining')\n",
    "\n",
    "logger.info(f\"Project root: {project_root}\")\n",
    "logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "logger.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = project_root / \"configs/enhanced_training.yaml\"\n",
    "\n",
    "with open(config_path) as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "priority_net_config = config_dict.get('priority_net', {})\n",
    "\n",
    "# Display key configuration parameters\n",
    "print(\"\\nüìã PriorityNet Configuration\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Hidden dims: {priority_net_config.get('hidden_dims')}\")\n",
    "print(f\"Batch size: {priority_net_config.get('batch_size')}\")\n",
    "print(f\"Learning rate: {priority_net_config.get('learning_rate'):.2e}\")\n",
    "print(f\"Epochs: {priority_net_config.get('epochs')}\")\n",
    "print(f\"Warmup epochs: {priority_net_config.get('warmup_epochs')}\")\n",
    "print(f\"Use Transformer encoder: {priority_net_config.get('use_transformer_encoder')}\")\n",
    "print(f\"\\nLoss weights:\")\n",
    "print(f\"  Ranking: {priority_net_config.get('ranking_weight')}\")\n",
    "print(f\"  MSE: {priority_net_config.get('mse_weight')}\")\n",
    "print(f\"  Uncertainty: {priority_net_config.get('uncertainty_weight')}\")\n",
    "print(f\"\\nCalibration weights:\")\n",
    "print(f\"  Mean: {priority_net_config.get('calib_mean_weight')}\")\n",
    "print(f\"  Max: {priority_net_config.get('calib_max_weight')}\")\n",
    "print(f\"  Range: {priority_net_config.get('calib_range_weight')}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Model & Training Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core model classes\n",
    "from ahsd.core.priority_net import (\n",
    "    PriorityNet,\n",
    "    PriorityLoss,\n",
    "    PriorityNetTrainer,\n",
    "    TemporalStrainEncoder,\n",
    "    TransformerStrainEncoder\n",
    ")\n",
    "\n",
    "logger.info(\"‚úÖ Imported PriorityNet classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import data loading utilities\n",
    "try:\n",
    "    # Check if train_priority_net.py exists and import from it\n",
    "    import importlib.util\n",
    "    \n",
    "    train_script = project_root / \"experiments/train_priority_net.py\"\n",
    "    if train_script.exists():\n",
    "        spec = importlib.util.spec_from_file_location(\n",
    "            \"train_priority_net\",\n",
    "            train_script\n",
    "        )\n",
    "        train_module = importlib.util.module_from_spec(spec)\n",
    "        sys.modules['train_priority_net'] = train_module\n",
    "        spec.loader.exec_module(train_module)\n",
    "        \n",
    "        # Extract needed classes\n",
    "        PriorityNetDataset = train_module.PriorityNetDataset\n",
    "        ChunkedGWDataLoader = train_module.ChunkedGWDataLoader\n",
    "        collate_priority_batch = train_module.collate_priority_batch\n",
    "        \n",
    "        logger.info(\"‚úÖ Imported data loading utilities from train_priority_net.py\")\n",
    "        HAS_DATA_LOADERS = True\n",
    "    else:\n",
    "        logger.warning(f\"train_priority_net.py not found at {train_script}\")\n",
    "        HAS_DATA_LOADERS = False\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not import data loaders: {e}\")\n",
    "    HAS_DATA_LOADERS = False\n",
    "\n",
    "if not HAS_DATA_LOADERS:\n",
    "    logger.warning(\"Will use synthetic data batches for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration namespace\n",
    "cfg = SimpleNamespace(\n",
    "    hidden_dims=priority_net_config.get('hidden_dims', [640, 512, 384, 256]),\n",
    "    dropout=priority_net_config.get('dropout', 0.25),\n",
    "    use_strain=True,\n",
    "    use_edge_conditioning=True,\n",
    "    n_edge_types=19,\n",
    "    use_transformer_encoder=priority_net_config.get('use_transformer_encoder', False),\n",
    "    overlap_importance_hidden=priority_net_config.get('importance_hidden_dim', 32)\n",
    ")\n",
    "\n",
    "# Initialize PriorityNet\n",
    "model = PriorityNet(cfg)\n",
    "model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "logger.info(f\"‚úÖ PriorityNet initialized with {total_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer with loss function and optimizer\n",
    "trainer = PriorityNetTrainer(model, priority_net_config)\n",
    "logger.info(f\"‚úÖ Trainer initialized\")\n",
    "print(f\"\\nüèãÔ∏è Trainer Configuration:\")\n",
    "print(f\"  Learning rate: {priority_net_config.get('learning_rate'):.2e}\")\n",
    "print(f\"  Optimizer: AdamW\")\n",
    "print(f\"  Warmup epochs: {trainer.warmup_epochs}\")\n",
    "print(f\"  Gradient clip norm: {trainer.gradient_clip_norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for actual data\n",
    "train_dir = project_root / \"data/dataset/train\"\n",
    "val_dir = project_root / \"data/dataset/val\"\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "USE_SYNTHETIC_DATA = False\n",
    "\n",
    "if HAS_DATA_LOADERS:\n",
    "    if train_dir.exists():\n",
    "        train_samples = list(train_dir.glob(\"*.h5\")) + list(train_dir.glob(\"sample_*.pkl\"))\n",
    "        if train_samples:\n",
    "            logger.info(f\"‚úÖ Found {len(train_samples)} training samples\")\n",
    "            \n",
    "            try:\n",
    "                train_loader = ChunkedGWDataLoader(\n",
    "                    data_dir=train_dir,\n",
    "                    batch_size=priority_net_config.get('batch_size', 12),\n",
    "                    shuffle=True,\n",
    "                    collate_fn=collate_priority_batch,\n",
    "                    num_workers=0  # Set to 0 in Jupyter to avoid multiprocessing issues\n",
    "                )\n",
    "                logger.info(f\"‚úÖ Created train loader with {len(train_loader)} batches\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not create train loader: {e}\")\n",
    "                USE_SYNTHETIC_DATA = True\n",
    "        else:\n",
    "            logger.warning(f\"No samples found in {train_dir}\")\n",
    "            USE_SYNTHETIC_DATA = True\n",
    "    else:\n",
    "        logger.warning(f\"Train directory not found: {train_dir}\")\n",
    "        USE_SYNTHETIC_DATA = True\n",
    "    \n",
    "    if val_dir.exists() and train_loader is not None:\n",
    "        val_samples = list(val_dir.glob(\"*.h5\")) + list(val_dir.glob(\"sample_*.pkl\"))\n",
    "        if val_samples:\n",
    "            logger.info(f\"‚úÖ Found {len(val_samples)} validation samples\")\n",
    "            \n",
    "            try:\n",
    "                val_loader = ChunkedGWDataLoader(\n",
    "                    data_dir=val_dir,\n",
    "                    batch_size=priority_net_config.get('batch_size', 12),\n",
    "                    shuffle=False,\n",
    "                    collate_fn=collate_priority_batch,\n",
    "                    num_workers=0\n",
    "                )\n",
    "                logger.info(f\"‚úÖ Created val loader with {len(val_loader)} batches\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not create val loader: {e}\")\n",
    "else:\n",
    "    logger.info(\"Data loader utilities not available, will use synthetic batches\")\n",
    "    USE_SYNTHETIC_DATA = True\n",
    "\n",
    "if USE_SYNTHETIC_DATA:\n",
    "    logger.info(\"‚ö†Ô∏è  Using synthetic data for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_batch(batch_size=8, n_signals_range=(2, 4)):\n",
    "    \"\"\"\n",
    "    Create a synthetic batch for demonstration.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Number of samples in batch\n",
    "        n_signals_range: Range of number of signals per sample\n",
    "    \n",
    "    Returns:\n",
    "        Batch dict with all required fields\n",
    "    \"\"\"\n",
    "    batch = {}\n",
    "    \n",
    "    # Strain data: [batch, detectors, time]\n",
    "    batch['strain_data'] = torch.randn(batch_size, 3, 2048).to(device)\n",
    "    \n",
    "    # Create variable-length parameters\n",
    "    max_signals = np.random.randint(*n_signals_range)\n",
    "    batch['parameters'] = torch.randn(batch_size, max_signals, 16).to(device)\n",
    "    batch['edge_type_ids'] = torch.randint(0, 19, (batch_size, max_signals)).to(device)\n",
    "    \n",
    "    # Targets: random priorities\n",
    "    batch['priorities'] = torch.rand(batch_size, max_signals).to(device)\n",
    "    batch['snr'] = torch.ones(batch_size, max_signals).to(device) * 20\n",
    "    \n",
    "    return batch\n",
    "\n",
    "logger.info(\"‚úÖ Defined synthetic batch generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, trainer, device='cpu', use_synthetic=False):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model: PriorityNet model\n",
    "        train_loader: Training data loader (or None if synthetic)\n",
    "        trainer: PriorityNetTrainer instance\n",
    "        device: 'cpu' or 'cuda'\n",
    "        use_synthetic: Use synthetic data if True\n",
    "    \n",
    "    Returns:\n",
    "        avg_loss: Average loss over epoch\n",
    "        grad_stats: (min_grad, max_grad, mean_grad)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    grad_stats = {'min': float('inf'), 'max': 0.0, 'mean': 0.0, 'count': 0}\n",
    "    \n",
    "    # Use synthetic batches if no real data\n",
    "    if use_synthetic:\n",
    "        batches = [create_synthetic_batch(batch_size=8) for _ in range(5)]\n",
    "    else:\n",
    "        batches = train_loader\n",
    "    \n",
    "    for batch_idx, batch in enumerate(batches):\n",
    "        try:\n",
    "            # Handle dict vs tensor batches\n",
    "            if isinstance(batch, dict):\n",
    "                strain_data = batch.get('strain_data', batch.get('strain', None))\n",
    "                parameters = batch.get('parameters', None)\n",
    "                edge_ids = batch.get('edge_type_ids', None)\n",
    "                targets = batch.get('priorities', batch.get('targets', None))\n",
    "                snr_values = batch.get('snr', None)\n",
    "            else:\n",
    "                # Fallback for tuple returns\n",
    "                strain_data, parameters, edge_ids, targets, snr_values = batch[:5]\n",
    "            \n",
    "            if strain_data is None or parameters is None or targets is None:\n",
    "                continue\n",
    "            \n",
    "            # Move to device if needed\n",
    "            strain_data = strain_data.to(device) if not strain_data.is_cuda else strain_data\n",
    "            parameters = parameters.to(device) if not parameters.is_cuda else parameters\n",
    "            targets = targets.to(device) if not targets.is_cuda else targets\n",
    "            if edge_ids is not None and not edge_ids.is_cuda:\n",
    "                edge_ids = edge_ids.to(device)\n",
    "            if snr_values is not None and not snr_values.is_cuda:\n",
    "                snr_values = snr_values.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions, uncertainties = model(\n",
    "                strain_data=strain_data,\n",
    "                parameters=parameters,\n",
    "                edge_type_ids=edge_ids\n",
    "            )\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = trainer.loss_fn(\n",
    "                predictions=predictions,\n",
    "                targets=targets,\n",
    "                uncertainties=uncertainties,\n",
    "                snr_values=snr_values\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            trainer.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping and stats\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                trainer.gradient_clip_norm\n",
    "            )\n",
    "            \n",
    "            # Track gradient stats\n",
    "            grads = [p.grad.abs().max().item() for p in model.parameters() if p.grad is not None]\n",
    "            if grads:\n",
    "                grad_stats['min'] = min(grad_stats['min'], min(grads))\n",
    "                grad_stats['max'] = max(grad_stats['max'], max(grads))\n",
    "                grad_stats['mean'] += np.mean(grads)\n",
    "                grad_stats['count'] += 1\n",
    "            \n",
    "            # Step optimizer\n",
    "            trainer.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            if (batch_idx + 1) % 5 == 0 or batch_idx == 0:\n",
    "                logger.info(\n",
    "                    f\"  Batch {batch_idx+1} | Loss: {loss.item():.6f} | Grad norm: {grad_norm:.4f}\"\n",
    "                )\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in batch {batch_idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if grad_stats['count'] > 0:\n",
    "        grad_stats['mean'] /= grad_stats['count']\n",
    "    \n",
    "    avg_loss = total_loss / max(num_batches, 1)\n",
    "    return avg_loss, grad_stats\n",
    "\n",
    "logger.info(\"‚úÖ Defined train_one_epoch function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, trainer, device='cpu', use_synthetic=False):\n",
    "    \"\"\"\n",
    "    Validate model on validation set.\n",
    "    \n",
    "    Args:\n",
    "        model: PriorityNet model\n",
    "        val_loader: Validation data loader (or None if synthetic)\n",
    "        trainer: PriorityNetTrainer instance\n",
    "        device: 'cpu' or 'cuda'\n",
    "        use_synthetic: Use synthetic data if True\n",
    "    \n",
    "    Returns:\n",
    "        avg_loss: Average validation loss\n",
    "        metrics: Dict of validation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_uncertainties = []\n",
    "    all_errors = []\n",
    "    \n",
    "    # Use synthetic batches if no real data\n",
    "    if use_synthetic:\n",
    "        batches = [create_synthetic_batch(batch_size=8) for _ in range(3)]\n",
    "    else:\n",
    "        batches = val_loader\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(batches):\n",
    "            try:\n",
    "                # Handle dict vs tensor batches\n",
    "                if isinstance(batch, dict):\n",
    "                    strain_data = batch.get('strain_data', batch.get('strain', None))\n",
    "                    parameters = batch.get('parameters', None)\n",
    "                    edge_ids = batch.get('edge_type_ids', None)\n",
    "                    targets = batch.get('priorities', batch.get('targets', None))\n",
    "                    snr_values = batch.get('snr', None)\n",
    "                else:\n",
    "                    strain_data, parameters, edge_ids, targets, snr_values = batch[:5]\n",
    "                \n",
    "                if strain_data is None or parameters is None or targets is None:\n",
    "                    continue\n",
    "                \n",
    "                # Move to device\n",
    "                strain_data = strain_data.to(device) if not strain_data.is_cuda else strain_data\n",
    "                parameters = parameters.to(device) if not parameters.is_cuda else parameters\n",
    "                targets = targets.to(device) if not targets.is_cuda else targets\n",
    "                if edge_ids is not None and not edge_ids.is_cuda:\n",
    "                    edge_ids = edge_ids.to(device)\n",
    "                if snr_values is not None and not snr_values.is_cuda:\n",
    "                    snr_values = snr_values.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                predictions, uncertainties = model(\n",
    "                    strain_data=strain_data,\n",
    "                    parameters=parameters,\n",
    "                    edge_type_ids=edge_ids\n",
    "                )\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = trainer.loss_fn(\n",
    "                    predictions=predictions,\n",
    "                    targets=targets,\n",
    "                    uncertainties=uncertainties,\n",
    "                    snr_values=snr_values\n",
    "                )\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Collect metrics\n",
    "                all_predictions.append(predictions.cpu())\n",
    "                all_targets.append(targets.cpu())\n",
    "                all_uncertainties.append(uncertainties.cpu())\n",
    "                all_errors.append(torch.abs(predictions - targets).cpu())\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in val batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    avg_loss = total_loss / max(num_batches, 1)\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = {}\n",
    "    if all_predictions:\n",
    "        preds = torch.cat(all_predictions).numpy().flatten()\n",
    "        targets_np = torch.cat(all_targets).numpy().flatten()\n",
    "        errors = torch.cat(all_errors).numpy().flatten()\n",
    "        uncs = torch.cat(all_uncertainties).numpy().flatten()\n",
    "        \n",
    "        metrics['mae'] = float(np.mean(errors))\n",
    "        metrics['pred_min'] = float(preds.min())\n",
    "        metrics['pred_max'] = float(preds.max())\n",
    "        metrics['pred_range'] = float(preds.max() - preds.min())\n",
    "        metrics['target_range'] = float(targets_np.max() - targets_np.min())\n",
    "        metrics['compression_ratio'] = metrics['pred_range'] / (metrics['target_range'] + 1e-8)\n",
    "        \n",
    "        # Uncertainty correlation\n",
    "        if len(errors) > 1 and uncs.std() > 0:\n",
    "            corr = np.corrcoef(errors, uncs)[0, 1]\n",
    "            metrics['uncertainty_corr'] = float(corr) if not np.isnan(corr) else 0.0\n",
    "    \n",
    "    return avg_loss, metrics\n",
    "\n",
    "logger.info(\"‚úÖ Defined validate function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = min(priority_net_config.get('epochs', 50), 5)  # Limit to 5 for demo\n",
    "patience = priority_net_config.get('patience', 15)\n",
    "warmup_epochs = trainer.warmup_epochs\n",
    "batch_size = priority_net_config.get('batch_size', 12)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\"üöÄ STARTING TRAINING\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"Epochs: {num_epochs}\")\n",
    "print(f\"Warmup epochs: {warmup_epochs}\")\n",
    "print(f\"Patience: {patience}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Using synthetic data: {USE_SYNTHETIC_DATA}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Create results directory\n",
    "checkpoint_dir = project_root / \"models/priority_net\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "logger.info(f\"Checkpoint directory: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "history = defaultdict(list)\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Warmup phase\n",
    "    if epoch < warmup_epochs:\n",
    "        trainer.warmup_scheduler.step()\n",
    "        current_lr = trainer.optimizer.param_groups[0]['lr']\n",
    "        logger.info(f\"\\n{'='*70}\")\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs} [WARMUP] - LR: {current_lr:.2e}\")\n",
    "        logger.info(f\"{'='*70}\")\n",
    "    else:\n",
    "        logger.info(f\"\\n{'='*70}\")\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        logger.info(f\"{'='*70}\")\n",
    "    \n",
    "    # Training\n",
    "    print(f\"\\nüìà Training...\")\n",
    "    train_loss, grad_stats = train_one_epoch(\n",
    "        model, train_loader, trainer, device,\n",
    "        use_synthetic=USE_SYNTHETIC_DATA or train_loader is None\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    print(f\"‚úÖ Validating...\")\n",
    "    val_loss, metrics = validate(\n",
    "        model, val_loader, trainer, device,\n",
    "        use_synthetic=USE_SYNTHETIC_DATA or val_loader is None\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduling (after warmup)\n",
    "    if epoch >= warmup_epochs:\n",
    "        trainer.scheduler.step(val_loss)\n",
    "    \n",
    "    # Track history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    for key, val in metrics.items():\n",
    "        history[f'val_{key}'].append(val)\n",
    "    \n",
    "    # Log epoch\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    logger.info(\n",
    "        f\"\\nResults:\")\n",
    "    logger.info(\n",
    "        f\"  Train loss: {train_loss:.6f}\")\n",
    "    logger.info(\n",
    "        f\"  Val loss: {val_loss:.6f}\")\n",
    "    logger.info(\n",
    "        f\"  Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Log gradient stats\n",
    "    if grad_stats['count'] > 0:\n",
    "        logger.info(\n",
    "            f\"  Grad stats: min={grad_stats['min']:.2e}, \"\n",
    "            f\"max={grad_stats['max']:.2e}, mean={grad_stats['mean']:.2e}\"\n",
    "        )\n",
    "    \n",
    "    # Log metrics if available\n",
    "    if metrics:\n",
    "        logger.info(\n",
    "            f\"  MAE: {metrics.get('mae', 0):.4f} | \"\n",
    "            f\"Range: [{metrics.get('pred_min', 0):.3f}, {metrics.get('pred_max', 0):.3f}] | \"\n",
    "            f\"Compression: {metrics.get('compression_ratio', 0):.1%} | \"\n",
    "            f\"Unc Corr: {metrics.get('uncertainty_corr', 0):.3f}\"\n",
    "        )\n",
    "    \n",
    "    # Save checkpoint if validation improves\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'loss': val_loss,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': trainer.scheduler.state_dict(),\n",
    "            'model_config': vars(cfg),\n",
    "            'history': dict(history)\n",
    "        }\n",
    "        \n",
    "        checkpoint_path = checkpoint_dir / \"priority_net_best.pth\"\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        logger.info(f\"\\n‚úÖ CHECKPOINT SAVED: {checkpoint_path}\")\n",
    "    \n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        logger.info(f\"\\nNo improvement ({patience_counter}/{patience})\")\n",
    "        if patience_counter >= patience:\n",
    "            logger.info(f\"\\nüõë Early stopping: no improvement for {patience} epochs\")\n",
    "            break\n",
    "\n",
    "# Summary\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\"‚úÖ TRAINING COMPLETE\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"Total time: {total_time/60:.1f} minutes\")\n",
    "print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "print(f\"Checkpoint saved to: {checkpoint_dir / 'priority_net_best.pth'}\")\n",
    "print(f\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plot Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "if history['train_loss']:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0, 0].plot(history['train_loss'], label='Train', marker='o')\n",
    "    axes[0, 0].plot(history['val_loss'], label='Val', marker='s')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # MAE plot\n",
    "    if 'val_mae' in history:\n",
    "        axes[0, 1].plot(history['val_mae'], label='Val MAE', marker='o')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('MAE')\n",
    "        axes[0, 1].set_title('Mean Absolute Error')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Output range plot\n",
    "    if 'val_pred_min' in history and 'val_pred_max' in history:\n",
    "        axes[1, 0].plot(history['val_pred_min'], label='Min', marker='o')\n",
    "        axes[1, 0].plot(history['val_pred_max'], label='Max', marker='s')\n",
    "        axes[1, 0].fill_between(range(len(history['val_pred_min'])),\n",
    "                                   history['val_pred_min'],\n",
    "                                   history['val_pred_max'], alpha=0.2)\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Value')\n",
    "        axes[1, 0].set_title('Output Range Expansion')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Uncertainty correlation plot\n",
    "    if 'val_uncertainty_corr' in history:\n",
    "        axes[1, 1].plot(history['val_uncertainty_corr'], label='Unc Corr', marker='o')\n",
    "        axes[1, 1].axhline(y=0.15, color='r', linestyle='--', label='Target (0.15)')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Correlation')\n",
    "        axes[1, 1].set_title('Uncertainty-Error Correlation')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(checkpoint_dir / 'training_history.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\nüìä Training plots saved to {checkpoint_dir / 'training_history.png'}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training history to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load and Test Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "checkpoint_path = project_root / \"models/priority_net/priority_net_best.pth\"\n",
    "\n",
    "if checkpoint_path.exists():\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    print(\"üîß Checkpoint Details\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Path: {checkpoint_path}\")\n",
    "    print(f\"Epoch: {checkpoint.get('epoch', 'unknown')}\")\n",
    "    print(f\"Loss: {checkpoint.get('loss', 'unknown'):.6f}\")\n",
    "    \n",
    "    # Count state dict keys\n",
    "    state_dict = checkpoint.get('model_state_dict', {})\n",
    "    print(f\"Model parameters: {len(state_dict)} keys\")\n",
    "    print(f\"Total model size: {sum(v.numel() for v in state_dict.values()):,} params\")\n",
    "    \n",
    "    # Load into model\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    model.eval()\n",
    "    print(f\"\\n‚úÖ Model loaded successfully\")\n",
    "    \n",
    "    # Show training history\n",
    "    hist = checkpoint.get('history', {})\n",
    "    if hist:\n",
    "        print(f\"\\nTraining history:\")\n",
    "        print(f\"  Epochs trained: {len(hist.get('train_loss', []))}\")\n",
    "        if hist.get('train_loss'):\n",
    "            print(f\"  Final train loss: {hist['train_loss'][-1]:.6f}\")\n",
    "        if hist.get('val_loss'):\n",
    "            print(f\"  Final val loss: {hist['val_loss'][-1]:.6f}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Checkpoint not found at {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "print(\"\\nüîÆ Testing Inference\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Create test batch\n",
    "    test_batch = create_synthetic_batch(batch_size=2, n_signals_range=(2, 3))\n",
    "    \n",
    "    # Forward pass\n",
    "    predictions, uncertainties = model(\n",
    "        strain_data=test_batch['strain_data'],\n",
    "        parameters=test_batch['parameters'],\n",
    "        edge_type_ids=test_batch['edge_type_ids']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSample 1:\")\n",
    "    print(f\"  Predictions: {predictions[0].cpu().numpy()}\")\n",
    "    print(f\"  Uncertainties: {uncertainties[0].cpu().numpy()}\")\n",
    "    print(f\"  Target priorities: {test_batch['priorities'][0].cpu().numpy()}\")\n",
    "    print(f\"\\nSample 2:\")\n",
    "    print(f\"  Predictions: {predictions[1].cpu().numpy()}\")\n",
    "    print(f\"  Uncertainties: {uncertainties[1].cpu().numpy()}\")\n",
    "    print(f\"  Target priorities: {test_batch['priorities'][1].cpu().numpy()}\")\n",
    "    \n",
    "    # Summary stats\n",
    "    all_preds = predictions.cpu().numpy().flatten()\n",
    "    print(f\"\\nPrediction statistics:\")\n",
    "    print(f\"  Min: {all_preds.min():.4f}\")\n",
    "    print(f\"  Max: {all_preds.max():.4f}\")\n",
    "    print(f\"  Mean: {all_preds.mean():.4f}\")\n",
    "    print(f\"  Std: {all_preds.std():.4f}\")\n",
    "    print(f\"  Range: {all_preds.max() - all_preds.min():.4f}\")\n",
    "\n",
    "print(\"=\"*70 + \"\\n‚úÖ Inference test complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
