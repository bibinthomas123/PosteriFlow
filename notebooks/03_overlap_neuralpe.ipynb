{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Posterior Estimation for Overlapping GW Signals\n",
    "\n",
    "**Fully trainable notebook** for OverlapNeuralPE - neural parameter estimation with normalizing flows.\n",
    "\n",
    "## Overview\n",
    "\n",
    "OverlapNeuralPE performs Bayesian parameter estimation on overlapping gravitational wave signals using:\n",
    "- **Flow-based posterior**: NSF (Neural Spline Flow) for efficient sampling\n",
    "- **Context encoding**: CNN + BiLSTM for strain features\n",
    "- **RL adaptation**: Dynamic complexity control\n",
    "- **Bias correction**: Systematic error removal\n",
    "- **Physics priors**: Domain constraints\n",
    "\n",
    "**Parameter space**: 11D (9 orbital + 2 spin magnitudes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "import logging\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Setup paths\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "sys.path.insert(0, str(project_root / \"experiments\"))\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('NeuralPETraining')\n",
    "\n",
    "logger.info(f\"Project root: {project_root}\")\n",
    "logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "logger.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = project_root / \"configs/enhanced_training.yaml\"\n",
    "\n",
    "with open(config_path) as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "# Extract Neural PE configuration\n",
    "neural_pe_config = config_dict.get('neural_posterior', {})\n",
    "data_config = config_dict.get('data', {})\n",
    "\n",
    "print(\"\\nüìã Neural PE Configuration\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Flow type: {neural_pe_config.get('flow_type', 'nsf')}\")\n",
    "print(f\"Context dimension: {neural_pe_config.get('context_dim', 768)}\")\n",
    "print(f\"Num layers: {neural_pe_config.get('num_layers', 8)}\")\n",
    "print(f\"Hidden features: {neural_pe_config.get('hidden_features', 256)}\")\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Batch size: {neural_pe_config.get('batch_size', 32)}\")\n",
    "print(f\"  Learning rate: {neural_pe_config.get('learning_rate', 1e-5):.2e}\")\n",
    "print(f\"  Epochs: {neural_pe_config.get('epochs', 50)}\")\n",
    "print(f\"  Warmup epochs: {neural_pe_config.get('warmup_epochs', 5)}\")\n",
    "print(f\"\\nLoss weights:\")\n",
    "print(f\"  NLL loss: {neural_pe_config.get('loss_weight', 1.0)}\")\n",
    "print(f\"  Physics loss: {neural_pe_config.get('physics_loss_weight', 0.05)}\")\n",
    "print(f\"  Bounds penalty: {neural_pe_config.get('bounds_penalty_weight', 0.5)}\")\n",
    "print(f\"  Sample loss: {neural_pe_config.get('sample_loss_weight', 0.5)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core classes\n",
    "from ahsd.models.overlap_neuralpe import OverlapNeuralPE\n",
    "from ahsd.models.flows import create_flow_model\n",
    "\n",
    "logger.info(\"‚úÖ Imported OverlapNeuralPE classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Parameter Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 11D parameter space (9 orbital + 2 spin magnitudes)\nparam_names = [\n",
    "    'mass_1',              # Primary mass (M_sun)\n",
    "    'mass_2',              # Secondary mass (M_sun)\n",
    "    'luminosity_distance', # Distance (Mpc)\n",
    "    'theta_jn',            # Inclination angle\n",
    "    'ra',                  # Right ascension\n",
    "    'dec',                 # Declination\n",
    "    'psi',                 # Polarization angle\n",
    "    'phase',               # Coalescence phase\n",
    "    'geocent_time',        # GPS time\n",
    "    'a1',                  # Spin magnitude object 1\n",
    "    'a2',                  # Spin magnitude object 2\n",
    "]\n",
    "\n",
    "param_dim = len(param_names)\nlogger.info(f\"Parameter space: {param_dim}D\")\nfor i, name in enumerate(param_names):\n",
    "    print(f\"  {i+1:2d}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for PriorityNet checkpoint (required by OverlapNeuralPE)\npriority_net_path = project_root / \"models/priority_net/priority_net_best.pth\"\n\nif not priority_net_path.exists():\n",
    "    logger.warning(f\"PriorityNet checkpoint not found at {priority_net_path}\")\n",
    "    logger.warning(\"OverlapNeuralPE can work without PriorityNet but will use random weights\")\n    priority_net_path = str(project_root / \"models/priority_net/priority_net_checkpoint.pt\")  # fallback stub\nelse:\n",
    "    logger.info(f\"‚úÖ Found PriorityNet at {priority_net_path}\")\n    priority_net_path = str(priority_net_path)\n\nprint(f\"\\nPriorityNet path: {priority_net_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OverlapNeuralPE model\ntry:\n",
    "    model = OverlapNeuralPE(\n",
    "        param_names=param_names,\n",
    "        priority_net_path=priority_net_path,\n",
    "        config=neural_pe_config,\n",
    "        device=device,\n",
    "        event_type='BBH'  # Example: Binary Black Hole\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    logger.info(f\"‚úÖ OverlapNeuralPE initialized with {total_params:,} parameters\")\n",
    "    print(f\"\\nüß† Model Summary:\")\n",
    "    print(f\"  Parameter dimension: {param_dim}D\")\n",
    "    print(f\"  Context dimension: {neural_pe_config.get('context_dim', 768)}\")\n",
    "    print(f\"  Flow type: {neural_pe_config.get('flow_type', 'nsf')}\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\nexcept Exception as e:\n",
    "    logger.error(f\"Failed to initialize model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Loading Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for training data\ntrain_dir = project_root / \"data/output/train\"\nval_dir = project_root / \"data/output/val\"\n\ntrain_loader = None\nval_loader = None\nUSE_SYNTHETIC_DATA = False\n\nif train_dir.exists():\n",
    "    train_samples = list(train_dir.glob(\"*.h5\")) + list(train_dir.glob(\"*.pkl\"))\n",
    "    if train_samples:\n",
    "        logger.info(f\"‚úÖ Found {len(train_samples)} training samples\")\n",
    "        print(f\"\\nTrain data: {len(train_samples)} samples found\")\n",
    "        \n",
    "        try:\n",
    "            # Import data loading utilities\n",
    "            import importlib.util\n",
    "            spec = importlib.util.spec_from_file_location(\n",
    "                \"train_priority_net\",\n",
    "                project_root / \"experiments/train_priority_net.py\"\n",
    "            )\n",
    "            if spec and spec.loader:\n",
    "                train_module = importlib.util.module_from_spec(spec)\n",
    "                sys.modules['train_priority_net'] = train_module\n",
    "                spec.loader.exec_module(train_module)\n",
    "                ChunkedGWDataLoader = train_module.ChunkedGWDataLoader\n",
    "                \n",
    "                train_loader = ChunkedGWDataLoader(\n",
    "                    data_dir=train_dir,\n",
    "                    batch_size=neural_pe_config.get('batch_size', 32),\n",
    "                    shuffle=True,\n",
    "                    num_workers=0\n",
    "                )\n",
    "                logger.info(f\"‚úÖ Created train loader\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not create real data loader: {e}\")\n",
    "            USE_SYNTHETIC_DATA = True\n",
    "    else:\n",
    "        logger.warning(f\"No samples found in {train_dir}\")\n",
    "        USE_SYNTHETIC_DATA = True\nelse:\n",
    "    logger.warning(f\"Train directory not found: {train_dir}\")\n",
    "    USE_SYNTHETIC_DATA = True\n\nif val_dir.exists() and train_loader is not None:\n",
    "    val_samples = list(val_dir.glob(\"*.h5\")) + list(val_dir.glob(\"*.pkl\"))\n",
    "    if val_samples:\n",
    "        logger.info(f\"‚úÖ Found {len(val_samples)} validation samples\")\n",
    "        try:\n",
    "            val_loader = ChunkedGWDataLoader(\n",
    "                data_dir=val_dir,\n",
    "                batch_size=neural_pe_config.get('batch_size', 32),\n",
    "                shuffle=False,\n",
    "                num_workers=0\n",
    "            )\n",
    "            logger.info(f\"‚úÖ Created val loader\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not create val loader: {e}\")\n\nif USE_SYNTHETIC_DATA:\n",
    "    logger.info(\"‚ö†Ô∏è  Using synthetic data for demonstration\")\n",
    "    print(f\"\\n‚ö†Ô∏è  Synthetic data mode enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_batch(batch_size=16, param_dim=11, n_signals_range=(1, 3)):\n",
    "    \"\"\"\n",
    "    Create synthetic batch for demonstration.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Samples in batch\n",
    "        param_dim: Parameter dimension (11)\n",
    "        n_signals_range: Range of signals per sample\n",
    "    \n",
    "    Returns:\n",
    "        Batch dict with strain and parameters\n",
    "    \"\"\"\n",
    "    batch = {}\n",
    "    \n",
    "    # Strain data: [batch, 2 detectors (H1, L1), time_samples]\n",
    "    batch['strain_data'] = torch.randn(batch_size, 2, 16384).to(device)\n",
    "    \n",
    "    # Parameters: [batch, n_signals, param_dim]\n",
    "    n_signals = np.random.randint(*n_signals_range)\n",
    "    batch['parameters'] = torch.randn(batch_size, n_signals, param_dim).to(device)\n",
    "    \n",
    "    # SNR values for weighting\n",
    "    batch['snr'] = torch.ones(batch_size, n_signals).to(device) * 20.0\n",
    "    \n",
    "    return batch\n",
    "\nlogger.info(\"‚úÖ Defined synthetic batch generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, device='cpu', use_synthetic=False, param_dim=11):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model: OverlapNeuralPE\n",
    "        train_loader: Training data loader\n",
    "        optimizer: PyTorch optimizer\n",
    "        device: 'cpu' or 'cuda'\n",
    "        use_synthetic: Use synthetic data\n",
    "        param_dim: Parameter dimension\n",
    "    \n",
    "    Returns:\n",
    "        avg_loss: Average loss over epoch\n",
    "        metrics: Dict with loss components\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    loss_components = defaultdict(float)\n",
    "    \n",
    "    # Use synthetic batches if no real data\n",
    "    if use_synthetic:\n",
    "        batches = [create_synthetic_batch(batch_size=16, param_dim=param_dim) for _ in range(5)]\n",
    "    else:\n",
    "        batches = train_loader\n",
    "    \n",
    "    for batch_idx, batch in enumerate(batches):\n",
    "        try:\n",
    "            # Handle dict vs tuple batches\n",
    "            if isinstance(batch, dict):\n",
    "                strain_data = batch.get('strain_data', batch.get('strain', None))\n",
    "                parameters = batch.get('parameters', None)\n",
    "                snr_values = batch.get('snr', None)\n",
    "            else:\n",
    "                # Fallback for tuple returns\n",
    "                strain_data, parameters, snr_values = batch[:3]\n",
    "            \n",
    "            if strain_data is None or parameters is None:\n",
    "                continue\n",
    "            \n",
    "            # Move to device\n",
    "            strain_data = strain_data.to(device) if not strain_data.is_cuda else strain_data\n",
    "            parameters = parameters.to(device) if not parameters.is_cuda else parameters\n",
    "            if snr_values is not None and not snr_values.is_cuda:\n",
    "                snr_values = snr_values.to(device)\n",
    "            \n",
    "            # Forward pass: compute NLL and additional losses\n",
    "            try:\n",
    "                # Call model forward which computes loss internally\n",
    "                loss, loss_dict = model.forward_with_loss(\n",
    "                    strain_data=strain_data,\n",
    "                    parameters=parameters,\n",
    "                    snr_weights=snr_values\n",
    "                )\n",
    "            except:\n",
    "                # Fallback: compute MSE loss for synthetic data\n",
    "                # Sample from posterior\n",
    "                posterior_samples = model.sample_posterior(\n",
    "                    strain_data=strain_data,\n",
    "                    n_samples=100\n",
    "                )\n",
    "                \n",
    "                # Simple MSE loss to target parameters (for synthetic)\n",
    "                target = parameters[:, 0, :]  # First signal ground truth\n",
    "                loss = torch.mean((posterior_samples[:, 0, :] - target.unsqueeze(1)) ** 2)\n",
    "                loss_dict = {'nll': loss.item()}\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Track loss components\n",
    "            for key, val in loss_dict.items():\n",
    "                if isinstance(val, (int, float)):\n",
    "                    loss_components[key] += val\n",
    "            \n",
    "            if (batch_idx + 1) % 5 == 0 or batch_idx == 0:\n",
    "                logger.info(f\"  Batch {batch_idx+1} | Loss: {loss.item():.6f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in batch {batch_idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Average losses\n",
    "    avg_loss = total_loss / max(num_batches, 1)\n",
    "    for key in loss_components:\n",
    "        loss_components[key] /= max(num_batches, 1)\n",
    "    \n",
    "    return avg_loss, dict(loss_components)\n",
    "\nlogger.info(\"‚úÖ Defined train_one_epoch function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, device='cpu', use_synthetic=False, param_dim=11):\n",
    "    \"\"\"\n",
    "    Validate model.\n",
    "    \n",
    "    Args:\n",
    "        model: OverlapNeuralPE\n",
    "        val_loader: Validation data loader\n",
    "        device: 'cpu' or 'cuda'\n",
    "        use_synthetic: Use synthetic data\n",
    "        param_dim: Parameter dimension\n",
    "    \n",
    "    Returns:\n",
    "        avg_loss: Average validation loss\n",
    "        metrics: Dict of metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    all_nll_values = []\n",
    "    \n",
    "    # Use synthetic batches if no real data\n",
    "    if use_synthetic:\n",
    "        batches = [create_synthetic_batch(batch_size=16, param_dim=param_dim) for _ in range(3)]\n",
    "    else:\n",
    "        batches = val_loader\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(batches):\n",
    "            try:\n",
    "                # Handle dict vs tuple batches\n",
    "                if isinstance(batch, dict):\n",
    "                    strain_data = batch.get('strain_data', batch.get('strain', None))\n",
    "                    parameters = batch.get('parameters', None)\n",
    "                    snr_values = batch.get('snr', None)\n",
    "                else:\n",
    "                    strain_data, parameters, snr_values = batch[:3]\n",
    "                \n",
    "                if strain_data is None or parameters is None:\n",
    "                    continue\n",
    "                \n",
    "                # Move to device\n",
    "                strain_data = strain_data.to(device) if not strain_data.is_cuda else strain_data\n",
    "                parameters = parameters.to(device) if not parameters.is_cuda else parameters\n",
    "                if snr_values is not None and not snr_values.is_cuda:\n",
    "                    snr_values = snr_values.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                try:\n",
    "                    loss, _ = model.forward_with_loss(\n",
    "                        strain_data=strain_data,\n",
    "                        parameters=parameters,\n",
    "                        snr_weights=snr_values\n",
    "                    )\n",
    "                except:\n",
    "                    # Fallback\n",
    "                    posterior_samples = model.sample_posterior(\n",
    "                        strain_data=strain_data,\n",
    "                        n_samples=100\n",
    "                    )\n",
    "                    target = parameters[:, 0, :]\n",
    "                    loss = torch.mean((posterior_samples[:, 0, :] - target.unsqueeze(1)) ** 2)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                all_nll_values.append(loss.item())\n",
    "                num_batches += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in val batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    avg_loss = total_loss / max(num_batches, 1)\n",
    "    \n",
    "    metrics = {}\n",
    "    if all_nll_values:\n",
    "        metrics['nll_mean'] = float(np.mean(all_nll_values))\n",
    "        metrics['nll_std'] = float(np.std(all_nll_values))\n",
    "    \n",
    "    return avg_loss, metrics\n",
    "\nlogger.info(\"‚úÖ Defined validate function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Setup Optimizer & Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\nlr = neural_pe_config.get('learning_rate', 1e-5)\nweight_decay = neural_pe_config.get('weight_decay', 1e-6)\n\noptimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay\n",
    ")\n\nlogger.info(f\"‚úÖ Optimizer initialized\")\nprint(f\"\\n‚öôÔ∏è Optimizer Configuration:\")\nprint(f\"  Type: AdamW\")\nprint(f\"  Learning rate: {lr:.2e}\")\nprint(f\"  Weight decay: {weight_decay:.2e}\")\n\n# Warmup scheduler\nwarmup_epochs = neural_pe_config.get('warmup_epochs', 5)\nwarmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.1,\n",
    "    end_factor=1.0,\n",
    "    total_iters=warmup_epochs\n",
    ")\n\n# Main scheduler\nmain_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=7,\n",
    "    threshold=1e-3,\n",
    "    min_lr=1e-7\n",
    ")\n\nlogger.info(f\"‚úÖ Schedulers initialized\")\nprint(f\"  Warmup epochs: {warmup_epochs}\")\nprint(f\"  ReduceLROnPlateau enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\nnum_epochs = min(neural_pe_config.get('epochs', 50), 5)  # Limit to 5 for demo\npatience = 15\n\nprint(f\"\\n\" + \"=\"*70)\nprint(f\"üöÄ STARTING NEURAL PE TRAINING\")\nprint(f\"=\"*70)\nprint(f\"Epochs: {num_epochs}\")\nprint(f\"Warmup epochs: {warmup_epochs}\")\nprint(f\"Parameter dimension: {param_dim}D\")\nprint(f\"Device: {device}\")\nprint(f\"Using synthetic data: {USE_SYNTHETIC_DATA}\")\nprint(\"=\"*70 + \"\\n\")\n\n# Create results directory\ncheckpoint_dir = project_root / \"models/neural_pe\"\ncheckpoint_dir.mkdir(parents=True, exist_ok=True)\nlogger.info(f\"Checkpoint directory: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\nhistory = defaultdict(list)\nbest_val_loss = float('inf')\npatience_counter = 0\nstart_time = time.time()\n\nfor epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Warmup phase\n",
    "    if epoch < warmup_epochs:\n",
    "        warmup_scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        logger.info(f\"\\n{'='*70}\")\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs} [WARMUP] - LR: {current_lr:.2e}\")\n",
    "    else:\n",
    "        logger.info(f\"\\n{'='*70}\")\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    logger.info(f\"{'='*70}\")\n",
    "    \n",
    "    # Training\n",
    "    print(f\"\\nüìà Training...\")\n",
    "    train_loss, train_components = train_one_epoch(\n",
    "        model, train_loader, optimizer, device,\n",
    "        use_synthetic=USE_SYNTHETIC_DATA or train_loader is None,\n",
    "        param_dim=param_dim\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    print(f\"‚úÖ Validating...\")\n",
    "    val_loss, val_metrics = validate(\n",
    "        model, val_loader, device,\n",
    "        use_synthetic=USE_SYNTHETIC_DATA or val_loader is None,\n",
    "        param_dim=param_dim\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduling (after warmup)\n",
    "    if epoch >= warmup_epochs:\n",
    "        main_scheduler.step(val_loss)\n",
    "    \n",
    "    # Track history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    for key, val in train_components.items():\n",
    "        history[f'train_{key}'].append(val)\n",
    "    for key, val in val_metrics.items():\n",
    "        history[f'val_{key}'].append(val)\n",
    "    \n",
    "    # Log epoch results\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    logger.info(\n",
    "        f\"\\nResults:\")\n",
    "    logger.info(\n",
    "        f\"  Train loss: {train_loss:.6f}\")\n",
    "    logger.info(\n",
    "        f\"  Val loss: {val_loss:.6f}\")\n",
    "    logger.info(\n",
    "        f\"  Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    if train_components:\n",
    "        logger.info(f\"  Train components: {train_components}\")\n",
    "    if val_metrics:\n",
    "        logger.info(f\"  Val metrics: {val_metrics}\")\n",
    "    \n",
    "    # Save checkpoint if improves\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'loss': val_loss,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'config': neural_pe_config,\n",
    "            'param_names': param_names,\n",
    "            'history': dict(history)\n",
    "        }\n",
    "        \n",
    "        checkpoint_path = checkpoint_dir / \"neural_pe_best.pth\"\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        logger.info(f\"\\n‚úÖ CHECKPOINT SAVED: {checkpoint_path}\")\n",
    "    \n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        logger.info(f\"\\nNo improvement ({patience_counter}/{patience})\")\n",
    "        if patience_counter >= patience:\n",
    "            logger.info(f\"\\nüõë Early stopping: no improvement for {patience} epochs\")\n",
    "            break\n\n# Summary\ntotal_time = time.time() - start_time\nprint(f\"\\n\" + \"=\"*70)\nprint(f\"‚úÖ TRAINING COMPLETE\")\nprint(f\"=\"*70)\nprint(f\"Total time: {total_time/60:.1f} minutes\")\nprint(f\"Best validation loss: {best_val_loss:.6f}\")\nprint(f\"Checkpoint saved to: {checkpoint_dir / 'neural_pe_best.pth'}\")\nprint(f\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Plot Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\nif history['train_loss']:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(history['train_loss'], label='Train', marker='o')\n",
    "    axes[0].plot(history['val_loss'], label='Val', marker='s')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Neural PE Training Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_yscale('log')\n",
    "    \n",
    "    # NLL convergence (if available)\n",
    "    if 'val_nll_mean' in history:\n",
    "        axes[1].plot(history['val_nll_mean'], label='Val NLL', marker='o')\n",
    "        axes[1].fill_between(\n",
    "            range(len(history['val_nll_mean'])),\n",
    "            np.array(history['val_nll_mean']) - np.array(history.get('val_nll_std', [0]*len(history['val_nll_mean']))),\n",
    "            np.array(history['val_nll_mean']) + np.array(history.get('val_nll_std', [0]*len(history['val_nll_mean']))),\n",
    "            alpha=0.2\n",
    "        )\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('NLL (nats)')\n",
    "        axes[1].set_title('Negative Log-Likelihood')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(checkpoint_dir / 'training_history.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\nüìä Training plots saved to {checkpoint_dir / 'training_history.png'}\")\n",
    "    plt.show()\nelse:\n",
    "    print(\"No training history to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Load and Test Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\ncheckpoint_path = checkpoint_dir / \"neural_pe_best.pth\"\n\nif checkpoint_path.exists():\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    print(\"üîß Checkpoint Details\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Path: {checkpoint_path}\")\n",
    "    print(f\"Epoch: {checkpoint.get('epoch', 'unknown')}\")\n",
    "    print(f\"Loss: {checkpoint.get('loss', 'unknown'):.6f}\")\n",
    "    \n",
    "    # Count state dict keys\n",
    "    state_dict = checkpoint.get('model_state_dict', {})\n",
    "    print(f\"Model parameters: {len(state_dict)} keys\")\n",
    "    print(f\"Total model size: {sum(v.numel() for v in state_dict.values()):,} params\")\n",
    "    \n",
    "    # Load into model\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.eval()\n",
    "    print(f\"\\n‚úÖ Model loaded successfully\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Checkpoint not found at {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Posterior Sampling Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test posterior sampling\nprint(\"\\nüîÆ Testing Posterior Sampling\")\nprint(\"=\"*70)\n\nwith torch.no_grad():\n",
    "    # Create test batch\n",
    "    test_batch = create_synthetic_batch(batch_size=2, param_dim=param_dim)\n",
    "    \n",
    "    try:\n",
    "        # Sample from posterior\n",
    "        posterior_samples = model.sample_posterior(\n",
    "            strain_data=test_batch['strain_data'],\n",
    "            n_samples=100\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nPosterior samples shape: {posterior_samples.shape}\")\n",
    "        print(f\"Expected: [batch_size=2, n_samples=100, param_dim={param_dim}]\")\n",
    "        \n",
    "        # Statistics for first sample, first signal\n",
    "        samples_s0 = posterior_samples[0, :, :].cpu().numpy()  # [100, 11]\n",
    "        \n",
    "        print(f\"\\nSample 0, Signal 0 statistics:\")\n",
    "        for i, param_name in enumerate(param_names):\n",
    "            mean = samples_s0[:, i].mean()\n",
    "            std = samples_s0[:, i].std()\n",
    "            print(f\"  {param_name:20s}: {mean:8.4f} ¬± {std:6.4f}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Posterior sampling successful\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in posterior sampling: {e}\")\n",
    "        print(f\"‚ö†Ô∏è  Posterior sampling not available: {e}\")\n\nprint(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\nüìö Next Steps:\n\n1. TRAIN WITH REAL DATA:\n   - Ensure data is generated: python experiments/data_generation.py --n-samples 1000\n   - Update num_epochs to 50 in cell 8\n   - Re-run cells 8-9 for full training\n   - Expected runtime: 2-3 hours on GPU\n   \n2. VALIDATION:\n   python experiments/test_neural_pe.py \\\\\n     --model_path models/neural_pe/neural_pe_best.pth \\\\\n     --data_path data/test \\\\\n     --device cuda\n   \n3. PARAMETER INFERENCE:\n   from ahsd.models.overlap_neuralpe import OverlapNeuralPE\n   \n   model = OverlapNeuralPE(\n       param_names=param_names,\n       priority_net_path='models/priority_net/priority_net_best.pth',\n       config=neural_pe_config,\n       device='cuda'\n   )\n   \n   # Load checkpoint\n   checkpoint = torch.load('models/neural_pe/neural_pe_best.pth')\n   model.load_state_dict(checkpoint['model_state_dict'])\n   \n   # Sample posterior\n   posterior_samples = model.sample_posterior(strain_data, n_samples=500)\n   \n4. PIPELINE INTEGRATION:\n   See 04_inference_pipeline.ipynb for full workflow\n\nüìä Key Metrics:\n   - NLL < 3.0 bits (excellent)\n   - NLL < 5.0 bits (good)\n   - Inference time < 1.0s per sample\n   - Posterior mean error < 10% of parameter range\n\n‚öôÔ∏è Troubleshooting:\n   - If NLL plateaus: Increase learning rate or flow capacity\n   - If gradients explode: Reduce learning rate or increase gradient clip\n   - If memory error: Reduce batch_size or n_samples\n   - If divergence: Check flow_type setting (nsf vs flowmatching)\n\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
