"""
GW Dataset Generator Module

Complete pipeline for generating synthetic gravitational wave detector data with
overlapping signals, edge cases, and realistic noise characteristics. Supports:

- Single and multiple overlapping signals (2-10 concurrent events)
- Three GW event types: BBH, BNS, NSBH with physics-realistic parameter distributions
- Edge case generation: physical extremes, observational extremes, statistical extremes
- Quota-based sampling for precise control over SNR/event-type distributions
- Real GWOSC noise, synthetic colored noise, or neural noise models
- Pre-merger samples for early warning system training
- Memory-efficient batch processing with resumable checkpoints
- Automatic data preprocessing and normalization

Primary Classes:
    GWDatasetGenerator: Main orchestrator for dataset creation

Key Features:
    - Overlapping signal simulation with realistic time clustering
    - Edge case coverage: extreme parameters, observational conditions, statistical scenarios
    - Joint quota enforcement via Iterative Proportional Fitting (IPF)
    - Stratified train/validation/test splits with exact fraction enforcement
    - Comprehensive logging and progress tracking
    - Checkpoint-based resume capability for interrupted generations
    - Detector PSD management with multiple sources (cached, online, synthetic)

Example:
    >>> generator = GWDatasetGenerator(output_dir='data/output')
    >>> summary = generator.generate(
    ...     n_samples=10000,
    ...     overlap_fraction=0.4,
    ...     edge_case_fraction=0.1,
    ...     premerger_fraction=0.05,
    ...     create_splits=True
    ... )
    >>> print(f"Generated {summary['n_samples']:,} samples")
"""

import numpy as np
import logging
from pathlib import Path
from typing import Dict, List, Optional, Union, Callable, Tuple
from tqdm import tqdm
import time
import gc
import glob
import pickle
from collections import Counter
from functools import wraps

from .config import (
    SAMPLE_RATE,
    DURATION,
    DETECTORS,
    EVENT_TYPE_DISTRIBUTION,
    SNR_DISTRIBUTION,
    SNR_RANGES,
    OVERLAP_FRACTION,
    EDGE_CASE_FRACTION,
)
from .psd_manager import PSDManager
from .parameter_sampler import ParameterSampler
from .waveform_generator import WaveformGenerator
from .noise_generator import NoiseGenerator
from .injection import SignalInjector
from .preprocessing import DataPreprocessor
from .io_utils import DatasetWriter, MetadataManager
from .simulation import OverlappingSignalSimulator, estimate_snr_from_strain
import random

_EDGE_MAP = {}  # Global edge type mapping

import numpy as np


def sample_overlap_size(p_heavy=0.95, base_lambda=3.0, heavy_min=5, heavy_max=10, max_overall = None):
    """
    Sample the number of overlapping GW signals using a mixture distribution.

    Provides realistic multi-source event generation with a heavy-tailed component
    for extreme overlaps. This is essential for training models on realistic scenarios
    where multiple signals can occur simultaneously (rare but critical for detector networks).

    Args:
        p_heavy (float, default=0.95):
            Probability of sampling from heavy-tailed distribution (5-10 signals).
            Set to 0.95 to ensure ~95% of overlaps are heavy overlaps.
        base_lambda (float, default=3.0):
            Mean of Poisson distribution for light component (2-5 signals).
        heavy_min (int, default=5):
            Minimum signals in heavy-tailed distribution.
        heavy_max (int, default=10):
            Maximum signals in heavy-tailed distribution.
        max_overall (int, optional):
            Absolute maximum signals to generate (clips heavy_max if specified).
            Useful for constraining total computational cost.

    Returns:
        int: Number of overlapping signals (typically 2-10).

    Examples:
        >>> # Heavy overlaps (95% heavy, 5% regular)
        >>> size = sample_overlap_size(p_heavy=0.95)
        >>> print(f"{size} signals")  # Usually 5-10
        >>> 
        >>> # Constrain to maximum 6 signals
        >>> size = sample_overlap_size(max_overall=6)
        >>> assert size <= 6

    Notes:
        - Heavy component: uniform(heavy_min, heavy_max) with p=p_heavy
        - Light component: clipped Poisson(base_lambda) to [2, 8] with p=1-p_heavy
        - Distribution ensures training on both 2-3 signal pairs and extreme overlaps
        - Modified (Nov 17, 2025): p_heavy=0.95 for multi-source training focus
    """
    # Use provided max_overall to clip heavy_max
    if max_overall is not None:
        heavy_max = min(heavy_max, max_overall)
    
    if np.random.random() < p_heavy:
        # heavy tail: 5-10 signals (heavy overlaps)
        return int(np.random.randint(heavy_min, heavy_max + 1))
    else:
        # light component: Poisson truncated to [2,5] (was [1,4], increased minimum)
        val = np.random.poisson(base_lambda)
        val = max(2, min(val, 8))  # Changed from (1, 4) to (2, 5)
        return int(val)


def sample_clustered_times(rng, n_signals, duration, overlap_window=0.6):
    """
    Generate clustered time offsets for overlapping GW signals.

    Creates realistic temporal clustering where multiple signals arrive within a
    small time window, simulating simultaneous or near-simultaneous events in the
    detector network. This is essential for training models on overlapping signal
    scenarios which test parameter recovery accuracy under signal interference.

    Args:
        rng (np.random.RandomState):
            Random number generator for reproducible sampling.
        n_signals (int):
            Number of signals to temporally offset.
        duration (float):
            Total observation window duration in seconds.
        overlap_window (float, default=0.6):
            Width of temporal cluster in seconds. Signals are confined within
            ¬±overlap_window from cluster center.

    Returns:
        list[float]: Geocentric time offsets [seconds] for each signal relative to
                    window center. Range: [-duration/2+0.01, duration/2-0.01].

    Examples:
        >>> rng = np.random.RandomState(42)
        >>> times = sample_clustered_times(rng, n_signals=3, duration=4.0, overlap_window=0.5)
        >>> print(f"Times: {times}")
        >>> assert len(times) == 3
        >>> assert all(-2 < t < 2 for t in times)
        >>> # All times within cluster
        >>> assert max(times) - min(times) <= 0.5 * 3  # ¬±3œÉ rule

    Notes:
        - Center sampled uniformly: [-0.5*(duration-window), 0.5*(duration-window)]
        - Individual offsets: N(0, overlap_window/6) ‚Üí ~99.7% within ¬±overlap_window
        - Clipping ensures all times stay within [-duration/2, duration/2]
        - Used in _generate_overlapping_sample() and edge case generators
    """
    center = rng.uniform(-0.5 * (duration - overlap_window), 0.5 * (duration - overlap_window))
    offsets = rng.normal(0, overlap_window / 6.0, size=n_signals)
    times = center + offsets
    return list(np.clip(times, -duration / 2 + 0.01, duration / 2 - 0.01))


def sample_snr_for_overlap(rng, snr_ranges, prefer_low_prob=0.65):
    """
    Sample SNR regime for signals in overlaps with bias toward weak signals.

    In realistic overlapping scenarios, additional signals are often weaker since
    they occur probabilistically. This function biases sampling toward low/weak
    regimes (65% probability) to maintain realistic signal strength distributions
    in multi-source events. The network SNR (combined coherent signal power) remains
    physically realistic while individual signals may be faint.

    Args:
        rng (np.random.RandomState):
            Random number generator for reproducible sampling.
        snr_ranges (dict[str, tuple]):
            SNR range boundaries for each regime:
            - 'weak': [5, 10], 'low': [10, 20], 'medium': [20, 40],
            - 'high': [40, 80], 'loud': [80, ‚àû]
        prefer_low_prob (float, default=0.65):
            Probability of selecting low/weak regimes (vs medium/high/loud).

    Returns:
        float: SNR value sampled uniformly from selected regime bounds.

    Examples:
        >>> rng = np.random.RandomState(42)
        >>> snr_ranges = {
        ...     'weak': (5, 10), 'low': (10, 20), 'medium': (20, 40),
        ...     'high': (40, 80), 'loud': (80, 200)
        ... }
        >>> snr = sample_snr_for_overlap(rng, snr_ranges, prefer_low_prob=0.65)
        >>> print(f"SNR: {snr:.1f}")  # Usually 5-20, rarely >40

    Notes:
        - Low regime (65%): 'weak' (40%) + 'low' (60%), range ~5-20 SNR
        - High regime (35%): 'medium' (70%) + 'high' (20%) + 'loud' (10%), range ~20-200 SNR
        - Used in _generate_overlapping_sample() to balance signal strengths
        - Ensures realistic network SNR while individual source SNRs may vary
    """
    if rng.random() < prefer_low_prob:
        regime = rng.choice(["weak", "low"], p=[0.4, 0.6])
    else:
        regime = rng.choice(["medium", "high", "loud"], p=[0.7, 0.2, 0.1])
    return rng.uniform(*snr_ranges[regime])


def build_overlap_group(pool, n):
    """
    Build a group of n detections by time proximity and mixed types.
    pool: list of candidate detection dicts with 'geocent_time'.
    """
    if not pool or n <= 0:
        return []
    seed = random.choice(pool)
    candidates = sorted(
        pool, key=lambda d: abs(d.get("geocent_time", 0.0) - seed.get("geocent_time", 0.0))
    )
    group = [seed]
    for d in candidates:
        if len(group) >= n:
            break
        if d is seed:
            continue
        group.append(d)
    return group


def encode_edge_type(dets):
    """
    Map overlap size to a stable int ID.
    0: single signal
    3: pairwise overlap
    6: triple overlap
    7+: higher overlaps
    """
    if not dets:
        return 0
    size = len([d for d in dets if d is not None])  # ‚Üê FIX: count only non-None
    if size == 1:
        return 0
    elif size == 2:
        return 3
    elif size == 3:
        return 6
    else:
        return 7  # For size 4+, use 7


def attach_network_snr_safe(params_or_list):
    """
    Attach network_snr to params, handling both single dict and list of dicts.
    """
    from .injection import attach_network_snr  # Import the base function

    if params_or_list is None:
        return

    if isinstance(params_or_list, list):
        for p in params_or_list:
            if p is not None and isinstance(p, dict):
                attach_network_snr(p)  # Call base function
    elif isinstance(params_or_list, dict):
        attach_network_snr(params_or_list)  # Call base function


def rescale_priorities(y_raw):
    """
    Rescale list/array y_raw to [0,1] with mild gamma < 1 to expand headroom.
    IMPORTANT: Hard clip final result to [0, 1] to prevent out-of-range values.
    """
    y_arr = np.asarray(y_raw, dtype=np.float32)
    ymin, ymax = float(np.min(y_arr)), float(np.max(y_arr))
    if ymax - ymin < 1e-6:
        return np.clip(y_arr * 0 + 0.5, 0.0, 1.0).tolist()
    y = (y_arr - ymin) / (ymax - ymin)
    y = y**0.9  # Apply gamma expansion
    # CRITICAL: Hard clip to [0, 1] to prevent any out-of-range values
    y = np.clip(y, 0.0, 1.0)
    return y.tolist()


def maybe_inject_decoy(detections, p=0.30):
    """
    Optionally append a decoy (weaker copy) to force label separation.
    """
    from .injection import attach_network_snr

    if len(detections) == 0 or random.random() >= p:
        return detections
    d0 = detections[0]
    d_decoy = dict(d0)
    if d_decoy.get("network_snr") is not None:
        d_decoy["network_snr"] = max(0.0, float(d_decoy["network_snr"]) * 0.7)
    d_decoy["luminosity_distance"] = float(d_decoy.get("luminosity_distance", 500.0)) * 1.3
    attach_network_snr(d_decoy)
    detections.append(d_decoy)
    return detections


# Add to the GWDatasetGenerator class


class GWDatasetGenerator:
    """
    Main orchestrator for generating gravitational wave datasets with overlapping signals.

    This class manages the complete pipeline for synthetic GW data generation, including:
    - Parameter sampling for BBH, BNS, NSBH events
    - Waveform generation and signal injection into realistic detector noise
    - Overlapping signal simulation with time clustering
    - Edge case generation (physical, observational, statistical extremes)
    - Batch processing with memory-efficient streaming and resumable checkpoints
    - Train/validation/test split creation with stratification

    Attributes:
        output_dir (Path): Output directory for generated samples and metadata.
        sample_rate (int): Sampling frequency in Hz (default: 4096).
        duration (float): Observation window duration in seconds (default: 4.0).
        detectors (list[str]): Active detectors (default: ['H1', 'L1', 'V1']).
        output_format (str): 'pkl' or 'hdf5' format for saving data.
        config (dict): Full configuration including parameters, extremes, and modes.
        parameter_sampler (ParameterSampler): Samples GW source parameters.
        simulation (OverlappingSignalSimulator): Generates detector data from parameters.
        waveform_generator (WaveformGenerator): Creates GW waveforms.
        preprocessor (DataPreprocessor): Normalizes and whitens data.
        psds (dict): Detector power spectral densities.
        logger (logging.Logger): Logging interface for diagnostics.

    Key Parameters (from config):
        - overlap_fraction: Fraction of samples with multiple overlapping signals (default: 0.45)
        - edge_case_fraction: Fraction of edge case samples (default: 0.10)
        - premerger_fraction: Fraction of pre-merger inspiral samples (default: 0.05)
        - max_overlapping_signals: Maximum concurrent signals to simulate (default: 6)
        - quota_mode: Enable quota-based SNR/event-type distribution control (default: False)

    Production Notes:
        - Memory usage: ~300MB for 50K samples with 3 detectors (4096 Hz, 4s duration)
        - Generation rate: ~30-50 samples/second (100K samples ~30 minutes)
        - Resumable: Checkpoints saved every 10K samples enable interrupt recovery
        - Stratified splits: Enforce exact 70/20/10 train/val/test ratios per event type

    Examples:
        >>> # Basic usage: generate 10K samples
        >>> gen = GWDatasetGenerator(output_dir='data/output')
        >>> summary = gen.generate(n_samples=10000)
        >>> print(f"Generated {summary['n_samples']:,} samples")
        >>> 
        >>> # Advanced: quota-based generation with precise distributions
        >>> gen = GWDatasetGenerator(config={'quota_mode': True})
        >>> summary = gen.generate(
        ...     n_samples=50000,
        ...     overlap_fraction=0.45,
        ...     create_splits=True,
        ...     train_frac=0.70, val_frac=0.20, test_frac=0.10
        ... )

    References:
        - LIGO/Virgo detector network: https://gwosc.org
        - Overlapping GW signals: arXiv:2105.14066
        - Parameter inference: arXiv:1909.06296 (Bilby)
    """

    def __init__(
        self,
        output_dir: str = "data/output",
        sample_rate: int = SAMPLE_RATE,
        duration: float = DURATION,
        detectors: List[str] = None,
        output_format: str = "pkl",
        config: Dict = None,
        parameter_sampler=None,
    ):
        """
        Initialize the GW dataset generator.

        Args:
            output_dir (str):
                Directory for all generated files (default: 'data/output').
                Created if doesn't exist.
            sample_rate (int):
                Sampling frequency in Hz (default: 4096).
                Standard LIGO/Virgo rate. Affects computational cost linearly.
            duration (float):
                Observation window duration in seconds (default: 4.0).
                Typical for ~100 kHz waveforms at 4096 Hz ‚Üí 16384 samples.
            detectors (list[str], optional):
                Active detectors. Default: ['H1', 'L1', 'V1'].
                Other options: 'V1' (Virgo), custom detector names.
            output_format (str):
                Storage format: 'pkl' (fast, CPU efficient) or 'hdf5' (shareable).
                Default: 'pkl' for development, 'hdf5' for production sharing.
            config (dict, optional):
                Full configuration dict with keys:
                - 'extreme_cases': Edge case configuration
                - 'max_overlapping_signals': Max concurrent events (default: 6)
                - 'quota_mode': Boolean for quota-based sampling
                - Any other parameters passed to sub-modules.
                If None, uses reasonable defaults.
            parameter_sampler (ParameterSampler, optional):
                Pre-initialized sampler. If None, creates new instance.
                Useful for re-using calibrated samplers across runs.

        Raises:
            RuntimeError: If simulator or key components fail to initialize.

        Notes:
            - Output directory created with parents if needed
            - Logging configured first for dependency debugging
            - Simulator initialization is non-fatal (falls back to legacy mode)
            - Large sample_rate or duration increases memory and CPU requirements
        """

        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.sample_rate = sample_rate
        self.duration = duration
        self.detectors = detectors or DETECTORS
        self.output_format = output_format.lower()

        #  FIX: Initialize extreme config properly
        self.config = config if config is not None else {}
        self.extreme_config = self.config.get("extreme_cases", {})
        self.extreme_enabled = self.extreme_config.get("enabled", False)
        self.extreme_fraction = self.extreme_config.get("fraction", 0.10)
        self.extreme_types_config = self.extreme_config.get("types", {})
        
        # Maximum overlapping signals from config
        self.max_overlapping_signals = self.config.get("max_overlapping_signals", 6)

        # Setup logging FIRST (simulator needs it)
        self.logger = logging.getLogger(__name__)

        #  FIX: Initialize simulator with proper config conversion
        try:
            # Create AHSDConfig object from dict
            from types import SimpleNamespace

            sim_config = SimpleNamespace()

            # Detector configs
            sim_config.detectors = []
            for det_name in self.detectors:
                det_cfg = SimpleNamespace(name=det_name, sampling_rate=sample_rate)
                sim_config.detectors.append(det_cfg)

            # Waveform config
            sim_config.waveform = SimpleNamespace(
                duration=duration,
                approximant=self.config.get("approximant", "IMRPhenomPv2"),
                f_ref=self.config.get("f_ref", 20.0),
            )

            # Initialize simulator
            self.simulation = OverlappingSignalSimulator(sim_config)
            self.logger.info(" Initialized OverlappingSignalSimulator")

        except Exception as e:
            self.logger.warning(f"Simulator initialization failed: {e}")
            self.logger.info("Falling back to legacy generation with SNR estimation")
            self.simulation = None

        self.logger.info(
            f"Extreme cases: {'enabled' if self.extreme_enabled else 'disabled'} "
            f"(fraction={self.extreme_fraction})"
        )

        # Validate format
        if self.output_format not in ["hdf5", "pkl", "pkl_compressed", "both"]:
            raise ValueError(f"Invalid output_format: {self.output_format}")

        # Initialize components
        self.logger.info("Initializing dataset generator components...")
        self.psd_manager = PSDManager(sample_rate, duration)
        # Allow injection of a pre-calibrated ParameterSampler for conditioning
        if parameter_sampler is not None:
            self.parameter_sampler = parameter_sampler
        else:
            self.parameter_sampler = ParameterSampler()
        
        # Initialize SNR reference value for validation (FIX #6)
        self.reference_snr = 20.0
        self.waveform_generator = WaveformGenerator(sample_rate, duration)
        self.noise_generator = NoiseGenerator(sample_rate, duration)
        self.injector = SignalInjector(sample_rate, duration)
        self.preprocessor = DataPreprocessor(sample_rate, duration)
        self.writer = DatasetWriter(output_dir, format=output_format)
        self.metadata_manager = MetadataManager()

        # Initialize real noise generators with pre-downloaded cache
        # Priority: cached segments > RealNoiseGenerator > synthetic noise
        self.use_real_noise_prob = self.config.get("use_real_noise_prob", 0.3)
        self.real_noise_generators = {}
        self.cached_noise_segments = {}  # {detector: [array, array, ...]}
        self.noise_source_stats = {"cached": 0, "real": 0, "synthetic": 0}  # Track usage
        
        # Try to load pre-downloaded segments from gw_segments
        self._load_cached_noise_segments()
        
        # Fall back to RealNoiseGenerator if segments not found
        try:
            from .noise_generator import RealNoiseGenerator

            output_dir_path = Path(output_dir)
            for detector in self.detectors:
                # Skip if we have cached segments
                if detector in self.cached_noise_segments and self.cached_noise_segments[detector]:
                    self.logger.info(f"Using {len(self.cached_noise_segments[detector])} pre-downloaded segments for {detector}")
                    continue
                    
                try:
                    self.real_noise_generators[detector] = RealNoiseGenerator(
                        detector=detector,
                        cache_dir=str(output_dir_path / "noise_cache"),
                        sample_rate=sample_rate,
                        duration=duration,
                    )
                    self.logger.info(f"Real noise generator (on-demand fetching) initialized for {detector}")
                except Exception as e:
                    self.logger.warning(f"Failed to initialize real noise for {detector}: {e}")
                    self.real_noise_generators[detector] = None
        except ImportError:
            self.logger.warning("RealNoiseGenerator not available")

        # Load PSDs
        self.logger.info(f"Loading PSDs for detectors: {self.detectors}")
        self.psds = self.psd_manager.load_detector_psds(self.detectors)

        self.logger.info(f"‚úì Dataset generator initialized (output format: {output_format})")
        # Debug SNR diagnostic counters (can be enabled via config['debug_snr_diagnostic'])
        self._debug_snr_count = 0
        self._debug_snr_limit = int(self.config.get("debug_snr_limit", 50))

    def _load_cached_noise_segments(self) -> None:
        """
        Load pre-downloaded noise segments from gw_segments/ folder.
        
        Expected file format: {detector}_{timestamp}.npy
        e.g., H1_1238166018.npy, L1_1238166018.npy, V1_1238166018.npy
        """
        cache_dir = Path("gw_segments")
        
        if not cache_dir.exists():
            self.logger.info(f"Cache directory {cache_dir} not found. Will use on-demand fetching.")
            return
        
        # Find and load all .npy files
        for detector in self.detectors:
            detector_files = sorted(cache_dir.glob(f"{detector}_*.npy"))
            
            if detector_files:
                segments = []
                for file_path in detector_files:
                    try:
                        segment = np.load(file_path).astype(np.float32)
                        segments.append(segment)
                        self.logger.debug(f"Loaded segment from {file_path.name}")
                    except Exception as e:
                        self.logger.warning(f"Failed to load {file_path.name}: {e}")
                
                if segments:
                    self.cached_noise_segments[detector] = segments
                    self.logger.info(f"‚úì Loaded {len(segments)} pre-downloaded segments for {detector} from {cache_dir}")
            else:
                self.logger.info(f"No cached segments found for {detector} in {cache_dir}")

    def _normalize_noise_power(self, noise: np.ndarray, target_std: float = 1e-21) -> np.ndarray:
        """
        Normalize noise to consistent power level across all samples.
        Prevents non-stationarity issues when mixing real and synthetic noise.
        
        Args:
            noise: Noise array
            target_std: Target standard deviation (default 1e-21 for detector sensitivity)
        
        Returns:
            Normalized noise array
        """
        current_std = np.std(noise)
        # Only normalize if significantly different (avoid unnecessary rescaling)
        if current_std > 0 and np.abs(current_std - target_std) / target_std > 0.5:
            noise = noise * (target_std / current_std)
        return noise

    def _get_noise_for_detector(self, detector_name: str, psd_dict: Dict) -> Tuple[np.ndarray, str]:
        """
        Get noise for a detector with priority order:
        1. Cached pre-downloaded GWOSC segments (if use_real_noise_prob > 0)
        2. Real LIGO/Virgo noise via RealNoiseGenerator (if use_real_noise_prob > 0)
        3. Synthetic colored Gaussian noise (fallback)

        Args:
            detector_name: Detector name ('H1', 'L1', 'V1')
            psd_dict: PSD dictionary for the detector

        Returns:
            Tuple of (noise_data, noise_type) where noise_type is 'cached', 'real', or 'synthetic'
        """
        # Check if real noise should be used (includes cached segments)
        if self.use_real_noise_prob > 0 and np.random.random() < self.use_real_noise_prob:
            
            # Priority 1: Try cached segments first (fastest, 10-25√ó speedup)
            if detector_name in self.cached_noise_segments and self.cached_noise_segments[detector_name]:
                try:
                    # Randomly select a cached segment
                    segment_idx = np.random.randint(0, len(self.cached_noise_segments[detector_name]))
                    noise = self.cached_noise_segments[detector_name][segment_idx]
                    
                    # Ensure correct length
                    expected_len = int(self.duration * self.sample_rate)
                    if len(noise) != expected_len:
                        # Trim or pad as needed
                        if len(noise) > expected_len:
                            start_idx = np.random.randint(0, len(noise) - expected_len + 1)
                            noise = noise[start_idx : start_idx + expected_len]
                        else:
                            # Pad with repetition
                            noise = np.tile(noise, (expected_len // len(noise)) + 1)[:expected_len]
                    
                    # CRITICAL: Check for NaN/Inf (skip corrupted cached segments)
                    if not np.all(np.isfinite(noise)):
                        self.logger.debug(f"Cached segment for {detector_name} contains NaN/Inf, skipping to fallback")
                        raise ValueError("Cached segment has NaN/Inf values")
                    
                    # Normalize to consistent power (prevents non-stationarity issues)
                    noise = self._normalize_noise_power(noise)
                    
                    self.noise_source_stats["cached"] += 1
                    return noise.astype(np.float32), "cached"
                except Exception as e:
                    self.logger.debug(f"Failed to use cached segment for {detector_name}: {e}")
            
            # Priority 2: Try RealNoiseGenerator (on-demand fetching)
            if (
                detector_name in self.real_noise_generators
                and self.real_noise_generators[detector_name] is not None
            ):
                try:
                    noise = self.real_noise_generators[detector_name].get_noise_chunk(
                        duration=self.duration, sample_rate=self.sample_rate
                    )
                    
                    # Normalize to consistent power (prevents non-stationarity issues)
                    noise = self._normalize_noise_power(noise)
                    
                    self.noise_source_stats["real"] += 1
                    return noise, "real"
                except Exception as e:
                    self.logger.debug(
                        f"Failed to fetch real noise for {detector_name}: {e}. Using synthetic."
                    )
        
        # Fallback: generate synthetic colored Gaussian noise (default path)
        noise = self.noise_generator.generate_colored_noise(psd_dict)
        self.noise_source_stats["synthetic"] += 1
        
        # Normalize to consistent power (prevents non-stationarity issues)
        noise = self._normalize_noise_power(noise)
        
        return noise, "synthetic"

    def create_noise_augmentations(self, sample: Dict, k: int) -> List[Dict]:
        """
        Create k augmented versions with different noise realizations

        Args:
            sample: Original sample
            k: Number of augmentations (including original)

        Returns:
            List of k augmented samples
        """

        if k <= 1:
            return [sample]

        augmented_samples = [sample]  # Include original

        # Get signal for this sample (if not noise-only)
        has_signal = sample["type"] != "noise"

        for aug_idx in range(1, k):
            # Create augmented copy
            aug_sample = {
                "sample_id": f"{sample['sample_id']}_aug{aug_idx}",
                "type": sample["type"],
                "is_overlap": sample.get("is_overlap", False),
                "is_edge_case": sample.get("is_edge_case", False),
                "parameters": sample.get("parameters"),
                "detector_data": {},
            }

            # Generate new noise realization for each detector
            for det_name in self.detectors:
                psd_dict = self.psds[det_name]

                # Generate fresh noise
                new_noise, _ = self._get_noise_for_detector(det_name, psd_dict)

                if has_signal and "detector_data" in sample:
                    # Extract signal from original (assuming signal + noise structure)
                    # For simplicity, we regenerate the signal with same params
                    original_data = sample["detector_data"].get(det_name, {})

                    if sample.get("is_overlap", False):
                        # Overlapping signals
                        params_list = sample["parameters"]
                        injected, metadata_list = self.injector.inject_overlapping_signals(
                            new_noise, params_list, det_name, psd_dict
                        )
                        
                        # ‚úÖ PHYSICS CORRECTION: Enforce SNR-distance coupling for augmented overlapping signals
                        for i, (metadata, params) in enumerate(zip(metadata_list, params_list)):
                            if metadata.get("actual_snr", 0) > 0:
                                target_snr = params.get("target_snr", 0)
                                achieved_snr = metadata.get("actual_snr", 0)
                                
                                if target_snr > 0 and achieved_snr > 0:
                                    original_distance = params.get("luminosity_distance", 0)
                                    corrected_distance = original_distance * (target_snr / achieved_snr)
                                    corrected_distance = np.clip(corrected_distance, 10.0, 5000.0)
                                    params["luminosity_distance"] = float(corrected_distance)
                                    metadata["corrected_distance"] = float(corrected_distance)
                                    metadata["distance_correction_factor"] = float(target_snr / achieved_snr)
                    else:
                        # Single signal
                        params = sample["parameters"]
                        if params:
                            injected, metadata = self.injector.inject_signal(
                                new_noise, params, det_name, psd_dict
                            )
                            
                            # ‚úÖ PHYSICS CORRECTION: Enforce SNR-distance coupling for augmented single signal
                            if metadata.get("actual_snr", 0) > 0:
                                target_snr = params.get("target_snr", 0)
                                achieved_snr = metadata.get("actual_snr", 0)
                                
                                if target_snr > 0 and achieved_snr > 0:
                                    original_distance = params.get("luminosity_distance", 0)
                                    corrected_distance = original_distance * (target_snr / achieved_snr)
                                    corrected_distance = np.clip(corrected_distance, 10.0, 5000.0)
                                    params["luminosity_distance"] = float(corrected_distance)
                                    metadata["corrected_distance"] = float(corrected_distance)
                                    metadata["distance_correction_factor"] = float(target_snr / achieved_snr)
                        else:
                            injected = new_noise
                            metadata = {"noise_only": True}
                else:
                    # Noise-only
                    injected = new_noise
                    metadata = {"noise_only": True}

                # Preprocess if needed
                if self.preprocessor:
                    injected = self.preprocessor.preprocess(injected, psd_dict)

                aug_sample["detector_data"][det_name] = {
                    "strain": injected.astype(np.float32),
                    "metadata": metadata if not sample.get("is_overlap") else metadata_list,
                }

            augmented_samples.append(aug_sample)

        return augmented_samples

    def generate_dataset(
        self,
        n_samples: int = 1000,
        overlap_fraction: float = OVERLAP_FRACTION,
        edge_case_fraction: float = EDGE_CASE_FRACTION,
        save_batch_size: int = 100,
        add_glitches: bool = True,
        preprocess: bool = True,
        save_complete: bool = False,
        create_splits: bool = True,
        train_frac: float = 0.8,
        val_frac: float = 0.1,
        test_frac: float = 0.1,
        chunk_size: int = 100,
        noise_augmentation_k: int = 1,
        multi_noise_k: int = 3,
        config: Dict = None,
    ) -> Dict:
        """
        Generate a complete gravitational wave dataset with memory efficiency and reliability.

        Main entry point for synthetic GW data generation. Orchestrates parameter sampling,
        waveform generation, signal injection, and preprocessing. Designed for large-scale
        production with automatic checkpoint/resume, stratified splits, and comprehensive
        logging.

        Args:
            n_samples (int):
                Total samples to generate (default: 1000).
                Typical: 10,000-100,000 for training.
                Memory: ~300MB for 50,000 samples.
            overlap_fraction (float):
                Fraction of samples with overlapping signals (default: 0.45).
                E.g., 0.45 ‚Üí ~450 of 1000 samples have 2-10 simultaneous sources.
            edge_case_fraction (float):
                Fraction of edge case samples (default: 0.10).
                Includes: physical extremes, observational issues, statistical anomalies.
            save_batch_size (int):
                Save batches to disk every N samples (default: 100).
                Trade-off: Small (10) = frequent I/O but better resume granularity;
                Large (1000) = fewer I/O operations but larger in-memory buffers.
            add_glitches (bool):
                Inject detector glitches (default: True).
                Types: 'blip' (Gaussian transient), 'whistle' (chirping), 'scattered_light'.
            preprocess (bool):
                Apply whitening and normalization (default: True).
                Recommended. Disable only for raw physics analysis.
            save_complete (bool):
                Save complete dataset in single file (default: False).
                Use for small datasets (<10K samples) or when convenient.
            create_splits (bool):
                Create train/val/test splits automatically (default: True).
                Uses stratified sampling to match distributions per event type.
            train_frac (float):
                Training set fraction (default: 0.80).
            val_frac (float):
                Validation set fraction (default: 0.10).
            test_frac (float):
                Test set fraction (default: 0.10).
                Must satisfy: train_frac + val_frac + test_frac ‚âà 1.0
            chunk_size (int):
                Split save chunk size (default: 100).
                For memory efficiency, saves splits in multiple files of chunk_size.
            noise_augmentation_k (int):
                Noise augmentation variants per sample (default: 1, meaning no augmentation).
                k=5 generates 5 versions with different noise realizations.
                Use for data augmentation during training.
            multi_noise_k (int):
                STEP 4: Multiple noise realizations per parameter set (default: 1).
                Generates K different noise realizations injected into the SAME signal parameters.
                ‚úÖ Fixes amplitude-distance entanglement by showing network same Œ∏ with different amplitudes
                Usage:
                  - multi_noise_k=1 (default): Current approach, 1 noise per Œ∏
                  - multi_noise_k=3: STEP 4 light, 3 noise per Œ∏ (recommended first try)
                  - multi_noise_k=5: STEP 4 full, 5 noise per Œ∏ (best quality, slowest)
                Impact: Eliminates amplitude-distance shortcuts in neural network
                Cost: K√ó slower generation, K√ó larger dataset (but converges K√ó faster)
                Formula: n_unique_parameters ‚Üí n_unique_parameters √ó K samples
                Example: 50K samples with multi_noise_k=3 ‚Üí 150K samples total
            config (dict, optional):
                Override configuration. Keys:
                - 'premerger_fraction': Pre-merger sample fraction (default: 0.05)
                - 'edge_cases': Dict configuring edge case types and fractions
                - Other parameters passed to sub-modules

        Returns:
            dict: Comprehensive generation summary:
                {
                    'n_samples': int,                          # Total generated
                    'n_batches': int,                          # Number of batch files
                    'generation_time': float,                  # Runtime in seconds
                    'samples_per_second': float,               # Generation rate
                    'output_dir': str,                         # Output directory path
                    'output_format': str,                      # 'pkl' or 'hdf5'
                    'resumed': bool,                           # Whether resumed from checkpoint
                    'statistics': {
                        'event_types': dict,                   # {BBH: 200, BNS: 150, ...}
                        'snr_regimes': dict,                   # {weak: 50, low: 350, ...}
                        'edge_cases': dict,                    # {high_mass_ratio: 10, ...}
                        'noise_sources': dict,                 # {cached: 500, synthetic: 500, ...}
                    },
                    'configuration': {...}                     # Full config used
                }

        Raises:
            ValueError:
                - If fractions don't sum to ~1.0
                - If n_samples < 1
            RuntimeError:
                - If PSD initialization fails
                - If no noise sources available
                - If splitting fails due to data issues

        Examples:
            >>> # Minimal: 100 samples for testing
            >>> gen = GWDatasetGenerator()
            >>> summary = gen.generate_dataset(n_samples=100)
            >>> print(f"Generated {summary['n_samples']:,} samples")
            >>> 
            >>> # Production: 50K samples with all features
            >>> summary = gen.generate_dataset(
            ...     n_samples=50000,
            ...     overlap_fraction=0.45,
            ...     edge_case_fraction=0.10,
            ...     add_glitches=True,
            ...     create_splits=True,
            ...     train_frac=0.70,
            ...     val_frac=0.20,
            ...     test_frac=0.10
            ... )
            >>> print(f"Rate: {summary['samples_per_second']:.1f}/s")
            >>> print(f"Time: {summary['generation_time']/3600:.1f}h")

        Notes:
            - **Resume capability**: Detects existing samples in output_dir and continues
            - **Memory safety**: Batches saved every 100 samples, max RAM ~300MB for 50K
            - **Stratification**: Train/val/test splits maintain event-type distribution
            - **Logging**: Progress logged every 5 minutes, detailed stats every 10K samples
            - **Checkpoints**: Full generation state saved every 10K samples for recovery

        Production Checklist:
            1. Check output_dir has ‚â•100GB free space for 50K samples
            2. Verify detector list matches your analysis setup
            3. Test on small batch (500 samples) before full run
            4. Monitor CPU/GPU utilization during generation
            5. Verify splits sum to n_samples (check for sample loss)
            6. Validate SNR/event-type distributions match config
            7. Run quick sanity check: `python -m ahsd.data.validate_dataset`

        Performance Tuning:
            - **Faster**: Increase batch_size (100‚Üí500), disable preprocess, use synthetic noise
            - **Memory efficient**: Reduce batch_size (100‚Üí50), enable chunk_size splits
            - **Quality**: Enable add_glitches, increase overlap_fraction, enable preprocess

        References:
            - LIGO/Virgo data: https://gwosc.org
            - Overlapping signals: arXiv:2105.14066
            - Noise characteristics: arXiv:1602.03844
        """

        # ‚úÖ STEP 4: Handle multi-noise adjustment
        if multi_noise_k > 1:
            self.logger.info(f"üîÑ STEP 4 ENABLED: Generating {multi_noise_k}√ó noise realizations per parameter")
            self.logger.info(f"   Original target: {n_samples:,} samples")
            self.logger.info(f"   With STEP 4: {n_samples * multi_noise_k:,} total samples ({multi_noise_k}√ó increase)")
            self.logger.info(f"   Generation time: ~{multi_noise_k}√ó slower")
            self.logger.info(f"   Dataset size: ~{multi_noise_k}√ó larger")
            self.logger.info(f"   Benefits: Fixes amplitude-distance entanglement, converges {multi_noise_k}√ó faster")
        
        n_synthetic = int(n_samples * 0.9)
        n_real = n_samples - n_synthetic

        # Initialize tracking dictionaries
        # event_type_counts: counts samples by top-level sample.type (e.g. 'single', 'overlap', 'BBH', etc.)
        # signal_type_counts: counts individual signals inside samples (BBH/BNS/NSBH) so distribution
        # comparisons reflect per-signal expectations in configs.

        event_type_counts = Counter()
        signal_type_counts = Counter()
        snr_regime_counts = Counter()
        edge_case_type_counts = Counter()
        extreme_case_type_counts = Counter()

        def _categorize_snr(snr: float) -> str:
            """Categorize SNR into 5 bins using configured SNR_RANGES for consistency."""
            from .config import SNR_RANGES

            # Check each regime's bounds
            for regime, (min_snr, max_snr) in SNR_RANGES.items():
                if min_snr <= snr < max_snr:
                    return regime

            # Handle out-of-range values
            if snr < 10.0:  # Below weak minimum
                return "weak"
            else:  # Above loud maximum
                return "loud"

        def _track_sample(sample):
            """Track statistics for generated sample
            
            IMPORTANT: For overlapping samples, only count FIRST signal's SNR regime.
            Overlapping signals are pre-sorted by decreasing SNR (brightest first).
            Counting all signals would inflate SNR regime counts: a 3-signal overlap
            would count as +3 instead of +1, distorting the sample-level distribution.
            """
            if sample is None:
                return
            # Track top-level sample type (single vs overlap vs noise)
            event_type = sample.get("type", "unknown")
            event_type_counts[event_type] += 1

            # Track SNR per signal and also maintain per-signal event type counts
            params = sample.get("parameters")
            if params:
                if isinstance(params, list):
                    # Overlapping sample: track each signal for event types
                    # BUT count SNR regime only for the FIRST (brightest) signal
                    for i, p in enumerate(params):
                        if isinstance(p, dict):
                            # Count all signals for event type distribution
                            sig_type = p.get("type") or p.get("event_type") or "unknown"
                            signal_type_counts[sig_type] += 1
                            
                            # Count only FIRST signal's SNR for sample-level histogram
                            # First signal = brightest = best SNR = represents sample's detectability
                            if i == 0 and "target_snr" in p:
                                regime = _categorize_snr(p["target_snr"])
                                snr_regime_counts[regime] += 1
                elif isinstance(params, dict):
                    # Single signal sample - count normally
                    sig_type = params.get("type") or params.get("event_type") or "unknown"
                    signal_type_counts[sig_type] += 1
                    if "target_snr" in params:
                        regime = _categorize_snr(params["target_snr"])
                        snr_regime_counts[regime] += 1

        def _normalize_sample(sample):
            """Normalize parameter dicts inside a sample so downstream analysis finds consistent keys.

            Ensures each per-signal parameter dict has a 'type' key (tries sample-level hints or mass-based inference),
            and normalizes common alternate key names (a_1 -> a1, tilt_1 -> tilt1).
            """
            if sample is None:
                return
            params = sample.get("parameters")

            def _normalize_param_dict(p):
                if not isinstance(p, dict):
                    return p
                # Coerce numpy/string types to native str where appropriate
                for key in ["type", "event_type", "category", "class", "source_type"]:
                    if key in p and p.get(key) is not None:
                        try:
                            val = p.get(key)
                            # numpy strings -> python str
                            if hasattr(val, "tolist"):
                                val = val.tolist()
                            p["type"] = str(val)
                            break
                        except Exception:
                            continue

                # Ensure 'type' exists; attempt multiple inference strategies
                if "type" not in p or p.get("type") in [None, ""]:
                    # 1) event_type key
                    if "event_type" in p and p.get("event_type"):
                        p["type"] = str(p.get("event_type"))
                    else:
                        # 2) name prefix (e.g., 'BBH_30_20')
                        name = p.get("name") or p.get("event_name") or ""
                        if isinstance(name, str) and name.upper().startswith(
                            ("BBH", "BNS", "NSBH")
                        ):
                            if name.upper().startswith("BBH"):
                                p["type"] = "BBH"
                            elif name.upper().startswith("BNS"):
                                p["type"] = "BNS"
                            elif name.upper().startswith("NSBH"):
                                p["type"] = "NSBH"
                        else:
                            # 3) mass-based inference (many variants covered)
                            m1 = p.get("mass_1") or p.get("mass1") or p.get("m1")
                            try:
                                if m1 is None:
                                    # Fallback to sample-level type
                                    p["type"] = sample.get("type", "unknown")
                                else:
                                    m1f = float(m1)
                                    if m1f < 3.0:
                                        p["type"] = "BNS"
                                    elif m1f < 8.0:
                                        p["type"] = "NSBH"
                                    else:
                                        p["type"] = "BBH"
                            except Exception:
                                p["type"] = sample.get("type", "unknown")
                # Normalize alternate key names
                if "a_1" in p and "a1" not in p:
                    p["a1"] = p.pop("a_1")
                if "a_2" in p and "a2" not in p:
                    p["a2"] = p.pop("a_2")
                if "tilt_1" in p and "tilt1" not in p:
                    p["tilt1"] = p.pop("tilt_1")
                if "tilt_2" in p and "tilt2" not in p:
                    p["tilt2"] = p.pop("tilt_2")
                # Also handle alternate mass key names
                if "mass1" in p and "mass_1" not in p:
                    p["mass_1"] = p.pop("mass1")
                if "mass2" in p and "mass_2" not in p:
                    p["mass_2"] = p.pop("mass2")
                if "m1" in p and "mass_1" not in p:
                    p["mass_1"] = p.pop("m1")
                if "m2" in p and "mass_2" not in p:
                    p["mass_2"] = p.pop("m2")

                # Ensure numeric numpy scalars are converted to native Python types for JSON/pickle friendliness
                for k, v in list(p.items()):
                    try:
                        if hasattr(v, "item"):
                            p[k] = v.item()
                    except Exception:
                        pass

                # Add detectors for edge type encoding
                if "detectors" not in p:
                    p["detectors"] = sample.get("detectors", [])

                return p

            if isinstance(params, list):
                sample["parameters"] = [_normalize_param_dict(p) for p in params]
            elif isinstance(params, dict):
                sample["parameters"] = _normalize_param_dict(params)

            # CRITICAL: Validate and fix priorities to ensure [0, 1] range
            priorities = sample.get("priorities", [])
            if priorities:
                # Validate each priority is in valid range
                fixed_priorities = []
                for p in priorities:
                    try:
                        p_float = float(p)
                        # Hard clip to [0, 1]
                        if not (0.0 <= p_float <= 1.0):
                            logger = logging.getLogger(__name__)
                            logger.warning(
                                f"Sample {sample.get('sample_id', 'unknown')}: "
                                f"Priority {p_float} out of range [0, 1], clipping"
                            )
                            p_float = float(np.clip(p_float, 0.0, 1.0))
                        fixed_priorities.append(p_float)
                    except (ValueError, TypeError):
                        # Invalid priority, use fallback
                        logger = logging.getLogger(__name__)
                        logger.warning(
                            f"Sample {sample.get('sample_id', 'unknown')}: "
                            f"Invalid priority {p}, using default 0.3"
                        )
                        fixed_priorities.append(0.3)
                
                sample["priorities"] = fixed_priorities

            # ‚úÖ FIX: Ensure all samples have edge_type_id (for single samples from legacy paths)
            if "edge_type_id" not in sample or sample.get("edge_type_id") is None:
                parameters = sample.get("parameters", [])
                if not isinstance(parameters, list):
                    parameters = [parameters] if parameters else []
                sample["edge_type_id"] = encode_edge_type(parameters)

            # Track edge/extreme cases
            if sample.get("is_edge_case", False):
                edge_type = sample.get("edge_case_type", "unknown")
                edge_case_type_counts[edge_type] += 1

            if sample.get("is_extreme_case", False):
                extreme_type = sample.get("extreme_case_type", "unknown")
                extreme_case_type_counts[extreme_type] += 1

        def _log_current_stats(checkpoint_num, total_generated):
            """Log comprehensive generation statistics with detailed breakdowns"""
            self.logger.info("")
            self.logger.info(f"{'='*80}")
            self.logger.info(f"CHECKPOINT {checkpoint_num}: Generated {total_generated:,} samples")
            elapsed = time.time() - start_time
            rate = total_generated / elapsed if elapsed > 0 else 0
            remaining = total_to_generate - total_generated
            eta = remaining / rate if rate > 0 else 0
            self.logger.info(
                f"Progress: {total_generated/total_to_generate*100:.1f}% | "
                f"Rate: {rate:.2f}/s | ETA: {eta/60:.1f}m"
            )
            self.logger.info(f"{'='*80}")
            self.logger.info("")

            # ========================================================================
            # EVENT TYPE DISTRIBUTION (signal-level and sample-level)
            # ========================================================================
            self.logger.info("üìä EVENT TYPE DISTRIBUTION:")
            self.logger.info(f"{'‚îÄ'*80}")

            # Signal-level counts (individual BBH/BNS/NSBH signals)
            total_signals = sum(signal_type_counts.values())
            expected = {"BBH": 0.46, "BNS": 0.32, "NSBH": 0.17, "noise": 0.05}

            self.logger.info("Signal-level distribution (individual signals):")
            self.logger.info(
                f"{'Type':<12} {'Count':>8} {'Actual':>8} {'Expected':>8} {'Diff':>8}  {'Status'}"
            )
            self.logger.info(f"{'‚îÄ'*80}")

            for event_type in ["BBH", "BNS", "NSBH", "noise"]:
                count = signal_type_counts.get(event_type, 0)
                actual_pct = (count / total_signals * 100) if total_signals > 0 else 0
                exp_pct = expected.get(event_type, 0.0) * 100
                diff = actual_pct - exp_pct
                status = "‚úì" if abs(diff) < 10 else "‚ö†"
                self.logger.info(
                    f"{event_type:<12} {count:>8,} {actual_pct:>7.1f}% "
                    f"{exp_pct:>7.1f}% {diff:>+7.1f}%  {status}"
                )

            self.logger.info(f"{'‚îÄ'*80}")
            self.logger.info(f"{'TOTAL SIGNALS':<12} {total_signals:>8,}")
            self.logger.info("")

            # Sample-level counts (how many samples are of each top-level type)
            self.logger.info("Sample-level distribution (samples):")
            self.logger.info(f"{'Type':<12} {'Count':>8} {'Actual':>8}")
            self.logger.info(f"{'‚îÄ'*80}")
            total_samples = sum(event_type_counts.values())
            for event_type in ["single", "overlap", "BBH", "BNS", "NSBH", "noise"]:
                count = event_type_counts.get(event_type, 0)
                actual_pct = (count / total_samples * 100) if total_samples > 0 else 0
                self.logger.info(f"{event_type:<12} {count:>8,} {actual_pct:>7.1f}%")

            self.logger.info("")

            # ========================================================================
            # SNR DISTRIBUTION (5 REGIMES)
            # ========================================================================
            self.logger.info("üìà SNR DISTRIBUTION (Astrophysically Realistic):")
            self.logger.info(f"{'‚îÄ'*80}")

            total_snr = sum(snr_regime_counts.values())
            # Use the actual configured SNR distribution
            expected_snr = {
                "weak": float(SNR_DISTRIBUTION.get("weak", 0.05)) * 100,
                "low": float(SNR_DISTRIBUTION.get("low", 0.35)) * 100,
                "medium": float(SNR_DISTRIBUTION.get("medium", 0.45)) * 100,
                "high": float(SNR_DISTRIBUTION.get("high", 0.12)) * 100,
                "loud": float(SNR_DISTRIBUTION.get("loud", 0.03)) * 100,
            }

            self.logger.info(
                f"{'Regime':<12} {'Range':>12} {'Count':>8} {'Actual':>8} {'Expected':>8} {'Diff':>8}  {'Status'}"
            )
            self.logger.info(f"{'‚îÄ'*80}")

            snr_ranges = {
                "weak": "10-15 Hz",
                "low": "15-25 Hz",
                "medium": "15-25 Hz",
                "high": "25-40 Hz",
                "loud": "40+ Hz",
            }

            for regime in ["weak", "low", "medium", "high", "loud"]:
                count = snr_regime_counts.get(regime, 0)
                actual_pct = (count / total_snr * 100) if total_snr > 0 else 0
                exp_pct = expected_snr[regime]
                diff = actual_pct - exp_pct
                status = "‚úì" if abs(diff) < 5 else "‚ö†"

                self.logger.info(
                    f"{regime.capitalize():<12} {snr_ranges[regime]:>12} {count:>8,} "
                    f"{actual_pct:>7.1f}% {exp_pct:>7.1f}% {diff:>+7.1f}%  {status}"
                )

            self.logger.info(f"{'‚îÄ'*80}")
            self.logger.info(f"{'TOTAL SIGNALS':<12} {'':<12} {total_snr:>8,}")

            if total_snr > 0:
                snr_mean = (
                    sum(
                        count
                        * {"weak": 9, "low": 12.5, "medium": 20, "high": 32.5, "loud": 45}[regime]
                        for regime, count in snr_regime_counts.items()
                    )
                    / total_snr
                )
                self.logger.info(f"Mean SNR (approx): {snr_mean:.1f}")

            self.logger.info("")

            # ====================================================================
            # Joint quotas diagnostic (if available)
            # ====================================================================
            try:
                if quota_mode and (
                    bool(self.config.get("quota_verbose", False))
                    or bool(self.config.get("quota_debug", False))
                ):
                    try:
                        jq = joint_quotas
                        per_snr = {}
                        per_event = {}
                        for (r, e), v in jq.items():
                            per_snr[r] = per_snr.get(r, 0) + v
                            per_event[e] = per_event.get(e, 0) + v

                        self.logger.info("Joint quota remaining (per-snr): " + str(per_snr))
                        self.logger.info("Joint quota remaining (per-event): " + str(per_event))
                        try:
                            self.logger.info("\n" + _format_joint_table(jq))
                        except Exception:
                            self.logger.debug(f"Joint quotas raw: {jq}")
                    except NameError:
                        # joint_quotas not defined in scope yet
                        pass
            except Exception:
                pass

            # ========================================================================
            # EDGE CASES BREAKDOWN
            # ========================================================================
            if edge_case_type_counts:
                self.logger.info("üîß EDGE CASE TYPES:")
                self.logger.info(f"{'‚îÄ'*80}")

                edge_categories = {
                    "Physical Extremes": [
                        "high_mass_ratio",
                        "extreme_spins",
                        "eccentric_mergers",
                        "precessing_systems",
                        "short_duration_high_mass",
                        "low_snr_threshold",
                    ],
                    "Observational": [
                        "strong_glitches",
                        "psd_drift",
                        "detector_dropout",
                        "sky_position_extremes",
                    ],
                    "Statistical": [
                        "multimodal_posteriors",
                        "heavy_tailed_regions",
                        "uninformative_priors",
                    ],
                    "Overlapping": ["subtle_ranking", "heavy_overlaps", "partial_overlaps"],
                }

                for category, edge_types in edge_categories.items():
                    category_total = sum(edge_case_type_counts.get(et, 0) for et in edge_types)
                    if category_total > 0:
                        self.logger.info(f"\n{category}: {category_total:,} samples")
                        for edge_type in edge_types:
                            count = edge_case_type_counts.get(edge_type, 0)
                            if count > 0:
                                pct = count / category_total * 100
                                self.logger.info(f"  {edge_type:<35s}: {count:>6,} ({pct:>5.1f}%)")

                self.logger.info(f"{'‚îÄ'*80}")
                self.logger.info(f"Total edge cases: {sum(edge_case_type_counts.values()):,}")
                self.logger.info("")

            # ========================================================================
            # EXTREME CASES BREAKDOWN
            # ========================================================================
            if extreme_case_type_counts:
                self.logger.info("‚ö° EXTREME CASE TYPES (Publication Quality):")
                self.logger.info(f"{'‚îÄ'*80}")

                # ‚úì FIX: Calculate expected percentages from config, not hardcoded values
                # Each extreme type fraction is relative to total extreme_cases fraction
                extreme_expected = {}
                for extreme_type, type_config in self.extreme_types_config.items():
                    if isinstance(type_config, dict) and type_config.get("enabled", True):
                        # Get the fraction of this type relative to all extreme cases
                        type_fraction = type_config.get("fraction", 0.0)
                        # Convert to percentage of total samples
                        # expected_pct = type_fraction * extreme_fraction * 100
                        expected_pct = type_fraction * self.extreme_fraction * 100
                        extreme_expected[extreme_type] = expected_pct

                self.logger.info(
                    f"{'Type':<35} {'Count':>8} {'%':>7} " f"{'Expected':>8}  {'Status'}"
                )
                self.logger.info(f"{'‚îÄ'*80}")

                total_extreme = sum(extreme_case_type_counts.values())

                for extreme_type in sorted(extreme_case_type_counts.keys()):
                    count = extreme_case_type_counts[extreme_type]
                    actual_pct = (count / total_generated * 100) if total_generated > 0 else 0
                    exp_pct = extreme_expected.get(extreme_type, 0.5)

                    # ‚úì FIX: Use z-scores for statistical significance instead of binary thresholds
                    # Calculate expected count and standard error for multinomial distribution
                    type_fraction = self.extreme_types_config.get(extreme_type, {}).get(
                        "fraction", 0.0
                    )
                    expected_count = total_extreme * type_fraction / sum(
                        self.extreme_types_config.get(t, {}).get("fraction", 0.0)
                        for t in extreme_case_type_counts.keys()
                    )
                    
                    # Standard error for binomial: sqrt(n*p*(1-p))
                    # Using proportion of extreme samples for this type
                    if total_extreme > 0 and type_fraction > 0:
                        # Relative fraction within extreme cases
                        relative_fraction = (
                            type_fraction
                            / sum(
                                self.extreme_types_config.get(t, {}).get("fraction", 0.0)
                                for t in extreme_case_type_counts.keys()
                            )
                        )
                        std_error = (
                            np.sqrt(
                                total_extreme
                                * relative_fraction
                                * (1 - relative_fraction)
                            )
                            if total_extreme > 1
                            else 1.0
                        )
                        z_score = (
                            (count - expected_count) / std_error
                            if std_error > 0
                            else 0
                        )
                    else:
                        z_score = 0

                    # Status based on statistical significance (œÉ = standard deviations)
                    # ‚úì‚úì: Within 1œÉ (68%), ‚úì: Within 2œÉ (95%), ‚ö†: Outside 2œÉ (rare)
                    if abs(z_score) <= 1.0:
                        status = "‚úì‚úì"  # Within 1œÉ
                    elif abs(z_score) <= 2.0:
                        status = "‚úì"  # Within 2œÉ
                    else:
                        status = "‚ö†"  # Outside 2œÉ

                    self.logger.info(
                        f"{extreme_type:<35} {count:>8,} {actual_pct:>6.2f}% "
                        f"{exp_pct:>7.2f}%  {status}"
                    )

                self.logger.info(f"{'‚îÄ'*80}")
                self.logger.info(
                    f"{'Total extreme cases':<35} {total_extreme:>8,} "
                    f"{total_extreme/total_generated*100:>6.1f}%"
                )
                self.logger.info("")

            # ========================================================================
            # SAMPLE COMPOSITION SUMMARY
            # ========================================================================
            self.logger.info("üì¶ SAMPLE COMPOSITION:")
            self.logger.info(f"{'‚îÄ'*80}")

            n_single = (
                event_type_counts.get("BBH", 0)
                + event_type_counts.get("BNS", 0)
                + event_type_counts.get("NSBH", 0)
                + event_type_counts.get("noise", 0)
            )
            n_overlap_actual = event_type_counts.get("overlap", 0)
            n_edge = sum(edge_case_type_counts.values())
            n_extreme = sum(extreme_case_type_counts.values())

            # Guard against division by zero at start of generation
            if total_generated == 0:
                self.logger.info("No samples generated yet...")
                self.logger.info(f"{'‚îÄ'*80}")
                return

            self.logger.info(
                f"Single events:     {n_single:>8,} ({n_single/total_generated*100:>5.1f}%)"
            )
            self.logger.info(
                f"Overlap events:    {n_overlap_actual:>8,} ({n_overlap_actual/total_generated*100:>5.1f}%)"
            )
            self.logger.info(
                f"Edge cases:        {n_edge:>8,} ({n_edge/total_generated*100:>5.1f}%)"
            )
            self.logger.info(
                f"Extreme cases:     {n_extreme:>8,} ({n_extreme/total_generated*100:>5.1f}%)"
            )
            self.logger.info(f"{'‚îÄ'*80}")
            self.logger.info(f"Total analyzed:    {total_generated:>8,} (100.0%)")

            self.logger.info(f"{'='*80}")
            self.logger.info("")

        # Update config if provided
        if config is not None:
            for key, value in config.items():
                if key not in self.config:
                    self.config[key] = value

        # Reload extreme config
        self.extreme_config = self.config.get("extreme_cases", {})
        self.extreme_enabled = self.extreme_config.get("enabled", False)
        self.extreme_fraction = self.extreme_config.get("fraction", 0.10)
        self.extreme_types_config = self.extreme_config.get("types", {})

        # Check for existing batches
        batch_dir = self.output_dir / "batches"
        batch_dir.mkdir(parents=True, exist_ok=True)

        existing_batch_files = sorted(glob.glob(str(batch_dir / "batch*.pkl")))
        existing_sample_count = 0

        if existing_batch_files:
            self.logger.info("=" * 80)
            self.logger.info("CHECKING EXISTING BATCHES")
            self.logger.info("=" * 80)
            self.logger.info(f"Found {len(existing_batch_files)} existing batch files")

            for batch_file in existing_batch_files:
                try:
                    with open(batch_file, "rb") as f:
                        batch_data = pickle.load(f)

                        if isinstance(batch_data, list):
                            batch_count = len(batch_data)
                        elif isinstance(batch_data, dict):
                            if "samples" in batch_data:
                                batch_count = len(batch_data["samples"])
                            elif "data" in batch_data:
                                batch_count = len(batch_data["data"])
                            else:
                                batch_count = 1
                        else:
                            batch_count = 1

                        existing_sample_count += batch_count

                except Exception as e:
                    self.logger.warning(f"Failed to load {Path(batch_file).name}: {e}")

            self.logger.info(f"‚úì Total existing samples: {existing_sample_count:,}")
            self.logger.info("=" * 80)
            self.logger.info("")

        if existing_sample_count >= n_samples:
            self.logger.info("=" * 80)
            self.logger.info("‚úì TARGET SAMPLES ALREADY GENERATED!")
            self.logger.info("=" * 80)
            self.logger.info(f"Existing: {existing_sample_count:,} / Target: {n_samples:,}")

            if create_splits:
                train_dir = self.output_dir / "train"
                if not train_dir.exists():
                    self.logger.info("")
                    self.logger.info("Creating splits from existing batches...")
                    self._create_splits_from_batches(
                        train_frac, val_frac, test_frac, chunk_size, noise_augmentation_k
                    )
                else:
                    self.logger.info("Splits already exist - skipping")

            self.logger.info("=" * 80)

            return {
                "n_samples": existing_sample_count,
                "generation_time": 0,
                "samples_per_second": 0,
                "output_dir": str(self.output_dir),
                "output_format": self.output_format,
                "resumed": True,
                "already_complete": True,
            }

        # Calculate remaining samples
        remaining = n_samples - existing_sample_count

        self.logger.info("=" * 80)
        if existing_sample_count > 0:
            self.logger.info("RESUMING DATASET GENERATION")
        else:
            self.logger.info("STARTING DATASET GENERATION")
        self.logger.info(f"Target: {n_samples:,} samples")
        self.logger.info(f"Remaining: {remaining:,} samples")
        self.logger.info("=" * 80)
        self.logger.info("")

        start_time = time.time()

        # ========================================================================
        # ‚úÖ CRITICAL: EMPIRICAL SNR CALIBRATION 
        # ========================================================================
        # Before any sampling, calibrate conditional SNR distributions by event type
        # This ensures _sample_target_snr() uses correct event-type-specific regime probabilities
        self.logger.info("üéØ Performing empirical SNR calibration by event type...")
        self.logger.info("   (This ensures proper regime distribution for each event type)")
        
        conditional_snr = self.parameter_sampler.empirical_calibrate(
            n_samples=5000,
            random_seed=42
        )
        
        self.logger.info("‚úÖ Conditional SNR distribution calibrated:")
        for event_type in ['BBH', 'BNS', 'NSBH']:
            if event_type in conditional_snr:
                self.logger.info(f"   {event_type}:")
                for regime, fraction in sorted(conditional_snr[event_type].items()):
                    self.logger.info(f"      {regime:>8}: {fraction:>6.1%}")
        self.logger.info("")

        # ========================================================================
        # CALCULATE SAMPLE DISTRIBUTION
        # ========================================================================

        n_overlap = int(n_samples * overlap_fraction)
        n_regular = n_samples - n_overlap

        # Pre-merger samples
        premerger_fraction = self.config.get("premerger_fraction", 0.15)
        n_premerger = int(n_samples * premerger_fraction)

        # Edge case breakdown
        edge_case_counts = {}
        edge_config = self.config.get("edge_cases", {})

        self.logger.info("Calculating sample distribution...")
        self.logger.info("")

        n_edge_total = int(n_samples * edge_case_fraction)
        # Physical extremes
        phys_config = edge_config.get("physical_extremes", {})
        if phys_config.get("enabled", False):
            n_phys = int(n_edge_total * phys_config.get("fraction", 0.25))
            self.logger.info(
                f"Physical extremes: {n_phys:,} samples ({phys_config.get('fraction', 0.15):.1%})"
            )
            for edge_type, type_config in phys_config.get("types", {}).items():
                n_type = int(n_phys * type_config.get("fraction", 0.0))
                if n_type > 0:
                    edge_case_counts[edge_type] = n_type
                    self.logger.info(f"  - {edge_type}: {n_type:,}")

        # Observational extremes
        obs_config = edge_config.get("observational_extremes", {})
        if obs_config.get("enabled", False):
            n_obs = int(n_edge_total * obs_config.get("fraction", 0.25))
            self.logger.info(
                f"Observational extremes: {n_obs:,} samples ({obs_config.get('fraction', 0.10):.1%})"
            )
            for edge_type, type_config in obs_config.get("types", {}).items():
                n_type = int(n_obs * type_config.get("fraction", 0.0))
                if n_type > 0:
                    edge_case_counts[edge_type] = n_type
                    self.logger.info(f"  - {edge_type}: {n_type:,}")

        # Statistical extremes
        stat_config = edge_config.get("statistical_extremes", {})
        if stat_config.get("enabled", False):
            n_stat = int(n_edge_total * stat_config.get("fraction", 0.10))
            self.logger.info(
                f"Statistical extremes: {n_stat:,} samples ({stat_config.get('fraction', 0.10):.1%})"
            )
            for edge_type, type_config in stat_config.get("types", {}).items():
                n_type = int(n_stat * type_config.get("fraction", 0.0))
                if n_type > 0:
                    edge_case_counts[edge_type] = n_type
                    self.logger.info(f"  - {edge_type}: {n_type:,}")

        # Overlapping extremes
        overlap_config = edge_config.get("overlapping_extremes", {})
        if overlap_config.get("enabled", False):
            n_overlap_edge = int(n_edge_total * overlap_config.get("fraction", 0.25))
            self.logger.info(
                f"Overlapping extremes: {n_overlap_edge:,} samples ({overlap_config.get('fraction', 0.10):.1%})"
            )
            for edge_type, type_config in overlap_config.items():
                if edge_type in ["enabled", "fraction"]:
                    continue
                n_type = int(n_overlap_edge * type_config.get("fraction", 0.0))
                if n_type > 0:
                    edge_case_counts[edge_type] = n_type
                    self.logger.info(f"  - {edge_type}: {n_type:,}")

        # Adjust regular samples
        total_edge = sum(edge_case_counts.values())
        n_regular_single = max(0, n_regular - total_edge - n_premerger)
        n_regular_overlap = n_overlap

        # ========================================================================
        # FINAL BREAKDOWN SUMMARY
        # ========================================================================
        self.logger.info("")
        self.logger.info("=" * 80)
        self.logger.info("SAMPLE BREAKDOWN SUMMARY")
        self.logger.info("=" * 80)
        self.logger.info(
            f"Regular single events:    {n_regular_single:>6,} ({n_regular_single/n_samples*100:>5.1f}%)"
        )
        self.logger.info(
            f"Pre-merger samples:       {n_premerger:>6,} ({n_premerger/n_samples*100:>5.1f}%)"
        )
        self.logger.info(
            f"Regular overlaps:         {n_regular_overlap:>6,} ({n_regular_overlap/n_samples*100:>5.1f}%)"
        )
        self.logger.info(
            f"Edge cases (total):       {total_edge:>6,} ({total_edge/n_samples*100:>5.1f}%)"
        )

        if self.extreme_enabled:
            expected_extreme = int(n_samples * self.extreme_fraction)
            self.logger.info(
                f"Extreme cases (expected): {expected_extreme:>6,} ({self.extreme_fraction*100:>5.1f}%)"
            )

        self.logger.info(f"{'‚îÄ' * 40}")
        self.logger.info(f"Total:                    {n_samples:>6,} (100.0%)")
        self.logger.info("=" * 80)
        self.logger.info("")

        #  ADD THIS SECTION HERE:
        # ========================================================================
        # GENERATION PLAN (Accounting for Probabilistic Extremes)
        # ========================================================================
        if self.extreme_enabled:
            self.logger.info("=" * 80)
            self.logger.info("GENERATION PLAN (with probabilistic extremes)")
            self.logger.info("=" * 80)

            # Calculate expected extremes within regular samples
            n_expected_extremes = int(n_regular_single * self.extreme_fraction)
            n_regular_actual = n_regular_single - n_expected_extremes

            self.logger.info(f"Regular single samples:        {n_regular_single:>6,}")
            self.logger.info(
                f"  ‚Üí Pure regular (expected):   {n_regular_actual:>6,} (~{(n_regular_actual/n_samples)*100:.1f}%)"
            )
            self.logger.info(
                f"  ‚Üí Probabilistic extremes:    ~{n_expected_extremes:>5,} (~{(n_expected_extremes/n_samples)*100:.1f}%)"
            )
            self.logger.info("")
            self.logger.info(
                f"Pre-merger samples:            {n_premerger:>6,} ({(n_premerger/n_samples)*100:.1f}%)"
            )
            self.logger.info(
                f"Regular overlap samples:       {n_regular_overlap:>6,} ({(n_regular_overlap/n_samples)*100:.1f}%)"
            )
            self.logger.info(
                f"Explicit edge cases:           {total_edge:>6,} ({(total_edge/n_samples)*100:.1f}%)"
            )
            self.logger.info(f"{'‚îÄ' * 80}")

            # Total to generate (for progress bar)
            # ‚úÖ FIX: Account for STEP 4 multi-noise generation
            base_samples = n_regular_single + n_premerger + n_regular_overlap + total_edge
            total_to_generate = base_samples * multi_noise_k if multi_noise_k > 1 else base_samples

            self.logger.info(f"Total samples to generate:     {total_to_generate:>6,}")
            if multi_noise_k > 1:
                self.logger.info(f"  ({base_samples:,} base √ó {multi_noise_k} noise variants)")
            self.logger.info("")
            self.logger.info("Note: Probabilistic extremes are generated WITHIN regular samples")
            self.logger.info("      based on extreme_fraction probability. Actual count may vary.")
            self.logger.info("=" * 80)
            self.logger.info("")
        else:
            # No probabilistic extremes - counts are exact
            # ‚úÖ FIX: Account for STEP 4 multi-noise generation
            base_samples = n_regular_single + n_premerger + n_regular_overlap + total_edge
            total_to_generate = base_samples * multi_noise_k if multi_noise_k > 1 else base_samples

            self.logger.info("=" * 80)
            self.logger.info("GENERATION PLAN (deterministic)")
            self.logger.info("=" * 80)
            self.logger.info(f"Regular single:    {n_regular_single:>6,}")
            self.logger.info(f"Pre-merger:        {n_premerger:>6,}")
            self.logger.info(f"Regular overlaps:  {n_regular_overlap:>6,}")
            self.logger.info(f"Edge cases:        {total_edge:>6,}")
            self.logger.info(f"{'‚îÄ' * 80}")
            self.logger.info(f"Total:             {total_to_generate:>6,}")
            if multi_noise_k > 1:
                self.logger.info(f"  ({base_samples:,} base √ó {multi_noise_k} noise variants)")
            self.logger.info("=" * 80)
            self.logger.info("")

        self.preprocess_enabled = preprocess

        # Quota mode configuration: when enabled, enforce SNR-regime and event-type
        # marginals by selecting regimes/types from computed quotas.
        quota_mode = bool(self.config.get("quota_mode", False))
        quota_max_attempts = int(self.config.get("quota_max_attempts", 10))

        # ‚úÖ Calibrate sampler empirically if quota mode is enabled
        # This estimates P(snr_regime | event_type) for conditional sampling in overlaps
        if quota_mode:
            self.logger.info("Calibrating parameter sampler for quota-aware event type sampling...")
            try:
                calibration = self.parameter_sampler.empirical_calibrate(
                    n_samples=int(self.config.get("calibration_samples", 2000)),
                    random_seed=(
                        int(self.config.get("random_seed", 42))
                        if self.config.get("random_seed")
                        else None
                    ),
                )
                self.logger.info(
                    "‚úì Calibration complete: P(snr_regime | event_type) ready for conditional sampling"
                )
            except Exception as e:
                self.logger.warning(
                    f"Calibration failed: {e}. Falling back to marginal distributions."
                )

        # Quota bookkeeping (populated only if quota_mode=True)
        quotas_snr = {}
        quotas_event = {}

        if quota_mode:
            # Estimate total signals to allocate quotas across. Use expected overlap size 2.5
            expected_signals_per_overlap = float(
                self.config.get("expected_signals_per_overlap", 2.5)
            )
            # By default we enforce quotas only on the regular single + overlapping signals.
            # Pre-merger samples and explicit edge-case samples are generated in separate
            # loops and may not consume quotas unless configured via 'quota_include_extremes'.
            include_extremes = bool(self.config.get("quota_include_extremes", False))
            total_signals_est = int(
                round(
                    n_regular_single
                    + n_regular_overlap * expected_signals_per_overlap
                    + (total_edge if include_extremes else 0)
                )
            )

            # Per-regime quotas (rounding and distribute remainder)
            regimes = list(SNR_DISTRIBUTION.keys())
            for r in regimes:
                quotas_snr[r] = int(round(total_signals_est * float(SNR_DISTRIBUTION.get(r, 0.0))))
            # Balance rounding error
            rem = total_signals_est - sum(quotas_snr.values())
            idx = 0
            while rem > 0:
                quotas_snr[regimes[idx % len(regimes)]] += 1
                idx += 1
                rem -= 1

            # Per-event-type quotas
            types = list(EVENT_TYPE_DISTRIBUTION.keys())
            for t in types:
                quotas_event[t] = int(
                    round(total_signals_est * float(EVENT_TYPE_DISTRIBUTION.get(t, 0.0)))
                )
            rem2 = total_signals_est - sum(quotas_event.values())
            idx = 0
            while rem2 > 0:
                quotas_event[types[idx % len(types)]] += 1
                idx += 1
                rem2 -= 1

            # Build joint quotas using iterative proportional fitting (IPF)
            # This enforces joint (snr_regime x event_type) quotas consistent with the marginal
            # quotas_snr and quotas_event. The result is an integer table summing to total_signals_est.
            def _compute_joint_quotas(
                row_counts: Dict[str, int], col_counts: Dict[str, int], total: int
            ):
                # Order rows and cols for deterministic behavior
                rows = list(row_counts.keys())
                cols = list(col_counts.keys())
                r = np.array([row_counts[k] for k in rows], dtype=float)
                c = np.array([col_counts[k] for k in cols], dtype=float)

                # Start with outer product of marginals as initial guess (avoid zeros)
                # Use small epsilon to avoid division by zero
                eps = 1e-12
                M = np.outer(r + eps, c + eps)

                # Normalize to have sum = total (floating)
                if M.sum() <= 0:
                    M = np.ones_like(M)
                M = M / M.sum() * float(total)

                # IPF iterations: alternate row/col scaling
                for _ in range(2000):
                    # scale rows
                    row_sums = M.sum(axis=1)
                    # avoid division by zero
                    row_scale = np.where(row_sums > 0, r / row_sums, 0.0)
                    M = (M.T * row_scale).T

                    # scale cols
                    col_sums = M.sum(axis=0)
                    col_scale = np.where(col_sums > 0, c / col_sums, 0.0)
                    M = M * col_scale

                    # convergence check (max absolute diff of margins)
                    if (np.max(np.abs(M.sum(axis=1) - r)) < 1e-6) and (
                        np.max(np.abs(M.sum(axis=0) - c)) < 1e-6
                    ):
                        break

                # Now round to integers while preserving total via largest fractional parts
                floored = np.floor(M).astype(int)
                remainder = int(total - floored.sum())
                if remainder > 0:
                    # distribute remaining counts by fractional parts
                    fracs = (M - np.floor(M)).flatten()
                    indices = np.argsort(-fracs)[:remainder]
                    for idx in indices:
                        i = idx // M.shape[1]
                        j = idx % M.shape[1]
                        floored[i, j] += 1

                # Build dict mapping (row, col) -> count
                joint = {}
                for i, row in enumerate(rows):
                    for j, col in enumerate(cols):
                        joint[(row, col)] = int(floored[i, j])
                return joint

            joint_quotas = _compute_joint_quotas(quotas_snr, quotas_event, total_signals_est)

            # Diagnostic formatter for joint_quotas
            def _format_joint_table(joint_table):
                rows = sorted(set(r for r, _ in joint_table.keys()))
                cols = sorted(set(c for _, c in joint_table.keys()))
                header = ["SNR\\Event"] + cols
                lines = ["\t".join(header)]
                for r in rows:
                    row = [r] + [str(joint_table.get((r, c), 0)) for c in cols]
                    lines.append("\t".join(row))
                return "\n".join(lines)

            if bool(self.config.get("quota_verbose", False)) or bool(
                self.config.get("quota_debug", False)
            ):
                try:
                    self.logger.info(
                        "Joint quotas (snr_regime x event_type):\n"
                        + _format_joint_table(joint_quotas)
                    )
                except Exception:
                    self.logger.info("Joint quotas: " + str(joint_quotas))

            # Helper: select a (snr_regime, event_type) cell from joint quotas (proportional to remaining counts)
            def _select_joint_cell():
                available = {k: v for k, v in joint_quotas.items() if v > 0}
                if not available:
                    # fallback to prior sampling
                    sr = self._sample_snr_regime()
                    try:
                        et = self.parameter_sampler.event_type_given_snr(sr)
                    except Exception:
                        et = self._sample_event_type()
                    return sr, et

                # Deterministic selection mode: pick the cell with largest remaining count
                deterministic = bool(self.config.get("quota_deterministic", False))
                if deterministic:
                    # stable sorting: sort by (remaining, row, col) and pick max
                    keys = sorted(
                        available.keys(), key=lambda k: (available[k], k[0], k[1]), reverse=True
                    )
                    chosen = keys[0]
                    joint_quotas[chosen] -= 1
                    return chosen

                # Default: proportional sampling by remaining counts
                keys = list(available.keys())
                vals = np.array([available[k] for k in keys], dtype=float)
                probs = vals / vals.sum()
                idx_choice = np.random.choice(len(keys), p=probs)
                chosen = keys[idx_choice]
                joint_quotas[chosen] -= 1
                return chosen

            # Keep deprecated helpers for compatibility (they will consult joint_quotas)
            def _select_snr_from_quota():
                # pick joint cell then return its snr
                snr_choice, _ = _select_joint_cell()
                return snr_choice

            def _select_event_for_snr(snr_regime):
                # Prefer event types in joint_quotas for the given snr_regime
                # Try to find any (snr_regime, event) with positive quota
                candidates = [
                    (k, v) for k, v in joint_quotas.items() if k[0] == snr_regime and v > 0
                ]
                if candidates:
                    # choose proportional to remaining
                    keys = [k for k, _ in candidates]
                    vals = np.array([v for _, v in candidates], dtype=float)
                    probs = vals / vals.sum()
                    idx = np.random.choice(len(keys), p=probs)
                    chosen = keys[idx][1]
                    joint_quotas[(snr_regime, chosen)] -= 1
                    return chosen

                # fallback to selecting any joint cell
                sr, et = _select_joint_cell()
                return et

        # ========================================================================
        # MEMORY-OPTIMIZED GENERATION WITH TRACKING
        # ========================================================================
        samples = []
        batch_id = len(existing_batch_files)
        sample_id = existing_sample_count
        total_generated = 0

        last_log_time = time.time()
        log_interval = 300  # Log stats every 5 minutes
        checkpoint_interval = 10000  # Detailed stats every 10000 samples

        self.logger.info("=" * 80)
        self.logger.info("STARTING SAMPLE GENERATION")
        self.logger.info("=" * 80)
        self.logger.info("")
        
        # ‚úÖ STEP 4 Status
        if multi_noise_k > 1:
            self.logger.info(f"üîÑ STEP 4 ENABLED: Generating {multi_noise_k}√ó noise realizations per parameter")
            self.logger.info(f"   Expected dataset size: {n_samples} base samples √ó {multi_noise_k} = {n_samples * multi_noise_k} total samples")
            self.logger.info("")
        else:
            self.logger.info("‚ÑπÔ∏è  STEP 4 DISABLED (K=1): Single noise realization per parameter")
            self.logger.info("")

        with tqdm(
            total=total_to_generate,
            desc="Generating samples",
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]",
        ) as pbar:

            # [1/4] Generate regular single samples
            if n_regular_single > 0:
                self.logger.info(f"[1/4] Generating {n_regular_single:,} regular single samples...")

                for i in range(n_regular_single):
                    is_extreme, extreme_type = self._should_generate_extreme_case()

                    if is_extreme and extreme_type:
                        sample = self._generate_extreme_case(sample_id, extreme_type)
                    else:
                        # Quota-mode: force SNR regime and event-type selection per quotas
                        if quota_mode:
                            # Do not enforce quotas on explicit extreme samples
                            snr_choice, evt_choice = _select_joint_cell()

                            # ‚úÖ STEP 4: Generate with multi-noise if enabled
                            samples_multi = self._generate_single_sample_multi_noise(
                                sample_id=sample_id,
                                is_edge_case=False,
                                add_glitches=add_glitches,
                                preprocess=preprocess,
                                k=multi_noise_k,
                                snr_regime=snr_choice,
                                forced_event_type=evt_choice,
                            )
                            # Process each sample variant
                            for sample in samples_multi:
                                sample = self._ensure_sample_priorities(sample)
                                _track_sample(sample)
                                _normalize_sample(sample)
                                samples.append(sample)
                                sample_id += 1
                                total_generated += 1  # ‚úÖ FIX: Track progress
                                pbar.update(1)  # ‚úÖ FIX: Update progress bar
                            continue  # Skip normal append below
                        else:
                            #  Use simulator directly
                            if self.simulation is not None:
                                try:
                                    sample = self._generate_sample_with_simulator(
                                        sample_id, n_signals=1  # Single signal
                                    )
                                except Exception as e:
                                    self.logger.warning(
                                        f"Simulator failed for sample {sample_id}: {e}"
                                    )

                                    # ‚úÖ STEP 4: Fallback with multi-noise
                                    samples_multi = self._generate_single_sample_multi_noise(
                                        sample_id=sample_id,
                                        is_edge_case=False,
                                        add_glitches=add_glitches,
                                        preprocess=preprocess,
                                        k=multi_noise_k,
                                    )
                                    for sample in samples_multi:
                                        sample = self._ensure_sample_priorities(sample)
                                        _track_sample(sample)
                                        _normalize_sample(sample)
                                        samples.append(sample)
                                        sample_id += 1
                                        total_generated += 1  # ‚úÖ FIX: Track progress
                                        pbar.update(1)  # ‚úÖ FIX: Update progress bar
                                    continue
                            else:
                                # No simulator - use legacy with multi-noise
                                samples_multi = self._generate_single_sample_multi_noise(
                                    sample_id=sample_id,
                                    is_edge_case=False,
                                    add_glitches=add_glitches,
                                    preprocess=preprocess,
                                    k=multi_noise_k,
                                )

                                # Process each sample variant
                                for sample in samples_multi:
                                    # Add priority
                                    priorities = []
                                    parameters = sample.get("parameters", [])

                                    if not isinstance(parameters, list):
                                        parameters = [parameters] if parameters else []

                                    for params in parameters:
                                        if not isinstance(params, dict):
                                            continue

                                        priority = self._estimate_snr_from_params(params)
                                        if "target_snr" not in params:
                                            self._set_target_snr(
                                                params, priority, reason="regular_single_assign"
                                            )
                                        priorities.append(priority)

                                    sample["priorities"] = [self._normalize_priority_to_01(p) for p in priorities] if priorities else [self._normalize_priority_to_01(15.0)]

                                    _normalize_sample(sample)
                                    _track_sample(sample)
                                    samples.append(sample)
                                    sample_id += 1
                                    total_generated += 1
                                    pbar.update(1)

                    if total_generated % checkpoint_interval == 0:
                        _log_current_stats(total_generated // checkpoint_interval, total_generated)

                    if len(samples) >= save_batch_size:
                        self._save_batch(batch_id, samples)
                        batch_id += 1
                        samples = []
                        gc.collect()

                    current_time = time.time()
                    if current_time - last_log_time >= log_interval:
                        self._log_progress(start_time, total_generated, total_to_generate)
                        last_log_time = current_time

            # [2/4] Generate pre-merger samples
            if n_premerger > 0:
                self.logger.info(f"[2/4] Generating {n_premerger:,} pre-merger samples...")

                for i in range(n_premerger):
                    # ‚úÖ STEP 4: Generate with multi-noise if enabled
                    samples_multi = self._generate_pre_merger_sample_multi_noise(
                        sample_id=sample_id,
                        k=multi_noise_k,
                    )
                    # Process each sample variant
                    for sample in samples_multi:
                        sample = self._ensure_sample_priorities(sample)
                        _track_sample(sample)
                        _normalize_sample(sample)
                        samples.append(sample)
                        sample_id += 1
                        total_generated += 1  # ‚úÖ FIX: Track progress
                        pbar.update(1)  # ‚úÖ FIX: Update progress bar

                    if total_generated % checkpoint_interval == 0:
                        _log_current_stats(total_generated // checkpoint_interval, total_generated)

                    if len(samples) >= save_batch_size:
                        self._save_batch(batch_id, samples)
                        batch_id += 1
                        samples = []
                        gc.collect()

                    current_time = time.time()
                    if current_time - last_log_time >= log_interval:
                        self._log_progress(start_time, total_generated, total_to_generate)
                        last_log_time = current_time

            # [3/4] Generate regular overlapping samples
            if n_regular_overlap > 0:
                self.logger.info(f"[3/4] Generating {n_regular_overlap:,} overlapping samples...")

                for i in range(n_regular_overlap):
                    # Quota-mode: construct per-signal forced regimes/types when enabled
                    if quota_mode:
                        n_sigs = sample_overlap_size(max_overall=self.max_overlapping_signals)
                        forced_signals = []
                        for j in range(n_sigs):
                            snr_choice, evt_choice = _select_joint_cell()
                            forced_signals.append(
                                {"snr_regime": snr_choice, "event_type": evt_choice}
                            )

                        # ‚úÖ STEP 4: Bypass simulator to honor quotas with multi-noise
                        samples_multi = self._generate_overlapping_sample_multi_noise(
                            sample_id=sample_id,
                            is_edge_case=False,
                            add_glitches=add_glitches,
                            preprocess=preprocess,
                            k=multi_noise_k,
                            forced_signals=forced_signals,
                        )
                        # Process each sample variant
                        for sample in samples_multi:
                            sample = self._ensure_sample_priorities(sample)
                            _track_sample(sample)
                            _normalize_sample(sample)
                            samples.append(sample)
                            sample_id += 1
                            total_generated += 1
                            pbar.update(1)
                    else:
                         #  Use simulator for overlaps
                        if self.simulation is not None:
                            try:
                                n_sigs = sample_overlap_size(max_overall=self.max_overlapping_signals)
                                sample = self._generate_sample_with_simulator(
                                    sample_id, n_signals=n_sigs
                                )
                                # Single simulator sample, wrap in multi-noise
                                samples_multi = [sample]
                            except Exception as e:
                                self.logger.warning(
                                    f"Simulator overlap failed for {sample_id}: {e}"
                                )
                                # ‚úÖ STEP 4: Fallback to legacy with multi-noise
                                samples_multi = self._generate_overlapping_sample_multi_noise(
                                    sample_id=sample_id,
                                    is_edge_case=False,
                                    add_glitches=add_glitches,
                                    preprocess=preprocess,
                                    k=multi_noise_k,
                                )
                        else:
                            # ‚úÖ STEP 4: No simulator - use legacy with multi-noise
                            samples_multi = self._generate_overlapping_sample_multi_noise(
                                sample_id=sample_id,
                                is_edge_case=False,
                                add_glitches=add_glitches,
                                preprocess=preprocess,
                                k=multi_noise_k,
                            )
                        
                        # Process each sample variant
                        for sample in samples_multi:
                            sample = self._ensure_sample_priorities(sample)
                            _track_sample(sample)
                            _normalize_sample(sample)
                            samples.append(sample)
                            sample_id += 1
                            total_generated += 1
                            pbar.update(1)

                    if total_generated % checkpoint_interval == 0:
                        _log_current_stats(total_generated // checkpoint_interval, total_generated)

                    if len(samples) >= save_batch_size:
                        self._save_batch(batch_id, samples)
                        batch_id += 1
                        samples = []
                        gc.collect()

                    current_time = time.time()
                    if current_time - last_log_time >= log_interval:
                        self._log_progress(start_time, total_generated, total_to_generate)
                        last_log_time = current_time

            # [4/4] Generate edge cases
            if edge_case_counts:
                self.logger.info(f"[4/4] Generating {total_edge:,} edge case samples...")

                for edge_type, n_type in edge_case_counts.items():
                    self.logger.info(f"  Generating {n_type:,} '{edge_type}' samples...")

                    for i in range(n_type):
                        # ‚úÖ STEP 4: All edge cases now support multi-noise
                        samples_multi = self._generate_edge_case_multi_noise(
                            sample_id=sample_id,
                            edge_type=edge_type,
                            config=self.config,
                            k=multi_noise_k,
                        )
                        
                        # Process each sample variant
                        for sample in samples_multi:
                            if sample:
                                sample = self._ensure_sample_priorities(sample)
                                _track_sample(sample)
                                _normalize_sample(sample)
                                samples.append(sample)
                                sample_id += 1
                                total_generated += 1
                                pbar.update(1)

                            if total_generated % checkpoint_interval == 0:
                                _log_current_stats(
                                    total_generated // checkpoint_interval, total_generated
                                )

                            if len(samples) >= save_batch_size:
                                self._save_batch(batch_id, samples)
                                batch_id += 1
                                samples = []
                                gc.collect()

        # Save remaining
        if samples:
            self._save_batch(batch_id, samples)
            samples = []
            gc.collect()

        generation_time = time.time() - start_time
        total_samples = existing_sample_count + total_generated

        # ========================================================================
        # FINAL STATISTICS
        # ========================================================================
        self.logger.info("")
        self.logger.info("=" * 80)
        self.logger.info("FINAL GENERATION STATISTICS")
        self.logger.info("=" * 80)
        self.logger.info(
            f"‚úì Generated {total_generated:,} new samples in {generation_time/60:.1f}m"
        )
        self.logger.info(f"  Rate: {total_generated/generation_time:.2f} samples/sec")
        self.logger.info("")

        # Event type final summary: show signal-level (individual signals) and sample-level
        self.logger.info("Event Type Distribution (Final):")
        total_signals = sum(signal_type_counts.values())
        self.logger.info("  Signal-level (individual signals):")
        for event_type in ["BBH", "BNS", "NSBH", "noise"]:
            count = signal_type_counts.get(event_type, 0)
            pct = (count / total_signals * 100) if total_signals > 0 else 0
            self.logger.info(f"    {event_type:8s}: {count:6,} ({pct:5.1f}%)")

        self.logger.info("  Sample-level (samples):")
        for event_type in sorted(event_type_counts.keys()):
            count = event_type_counts[event_type]
            pct = (count / total_generated * 100) if total_generated > 0 else 0
            self.logger.info(f"    {event_type:8s}: {count:6,} ({pct:5.1f}%)")

        # SNR final summary - clarify this is SAMPLE-level distribution
        total_snr = sum(snr_regime_counts.values())
        self.logger.info("")
        self.logger.info("SNR Distribution (Sample-level - First Signal in Each Sample):")
        self.logger.info("  (For overlaps, counts only brightest signal's SNR regime)")
        # Use configured distribution instead of hardcoded values
        expected_dist = {
            "weak": float(SNR_DISTRIBUTION.get("weak", 0.05)) * 100,
            "low": float(SNR_DISTRIBUTION.get("low", 0.35)) * 100,
            "medium": float(SNR_DISTRIBUTION.get("medium", 0.45)) * 100,
            "high": float(SNR_DISTRIBUTION.get("high", 0.12)) * 100,
            "loud": float(SNR_DISTRIBUTION.get("loud", 0.03)) * 100,
        }
        for regime in ["weak", "low", "medium", "high", "loud"]:
            count = snr_regime_counts.get(regime, 0)
            pct = (count / total_snr * 100) if total_snr > 0 else 0
            expected_pct = expected_dist[regime]
            diff = abs(pct - expected_pct)
            status = "‚úì" if diff < 5 else "‚ö†"
            self.logger.info(
                f"  {regime.capitalize():10s}: {count:6,} ({pct:5.1f}%) [expect {expected_pct}%] {status}"
            )

        # Edge/Extreme case summary
        if edge_case_type_counts:
            self.logger.info("")
            self.logger.info("Edge Case Summary:")
            for edge_type, count in sorted(edge_case_type_counts.items()):
                self.logger.info(f"  {edge_type:30s}: {count:4,}")

        if extreme_case_type_counts:
            self.logger.info("")
            self.logger.info("Extreme Case Summary:")
            for extreme_type, count in sorted(extreme_case_type_counts.items()):
                self.logger.info(f"  {extreme_type:30s}: {count:4,}")

        self.logger.info("=" * 80)
        self.logger.info("")

        # CREATE SPLITS
        if create_splits:
            self.logger.info("")
            self.logger.info("=" * 80)
            self.logger.info("CREATING SPLITS FROM BATCHES")
            self.logger.info("=" * 80)
            self._create_splits_from_batches(
                train_frac, val_frac, test_frac, chunk_size, noise_augmentation_k
            )

        # Save PSDs
        self.logger.info("")
        self.logger.info("Saving detector PSDs...")
        psd_dir = self.output_dir / "detector_psds"
        psd_dir.mkdir(exist_ok=True)

        for detector_name, psd_info in self.psds.items():
            psd_file = psd_dir / f"{detector_name}_psd.npz"
            psd_array = psd_info["psd"]
            if hasattr(psd_array, "numpy"):
                psd_array = psd_array.numpy()

            np.savez(
                psd_file,
                frequencies=psd_info["frequencies"],
                psd=psd_array,
                source=psd_info["source"],
                name=psd_info["name"],
            )

        self.logger.info("‚úì PSDs saved")

        # Generate summary with statistics
        summary = {
            "n_samples": total_samples,
            "n_batches": batch_id + 1,
            "generation_time": generation_time,
            "samples_per_second": total_generated / generation_time if generation_time > 0 else 0,
            "output_dir": str(self.output_dir),
            "output_format": self.output_format,
            "elapsed_time": generation_time,
            "resumed": existing_sample_count > 0,
            "configuration": {
                "sample_rate": self.sample_rate,
                "duration": self.duration,
                "detectors": self.detectors,
                "overlap_fraction": overlap_fraction,
                "edge_case_fraction": edge_case_fraction,
                "premerger_fraction": premerger_fraction,
                "edge_cases": edge_case_counts,
            },
            "statistics": {
                "event_types": dict(event_type_counts),
                "snr_regimes": dict(snr_regime_counts),
                "edge_cases": dict(edge_case_type_counts),
                "extreme_cases": dict(extreme_case_type_counts),
            },
        }

        # Attach joint_quotas to summary if present (convert to nested mapping)
        try:
            if quota_mode and "joint_quotas" in locals():
                jq = locals().get("joint_quotas")
                nested = {}
                for (r, e), v in jq.items():
                    nested.setdefault(r, {})[e] = int(v)
                summary["statistics"]["joint_quotas"] = nested
        except Exception:
            # best-effort: skip adding joint quotas
            pass

        self.logger.info("")
        self.logger.info("=" * 80)
        self.logger.info("DATASET GENERATION COMPLETE")
        self.logger.info("=" * 80)
        self.logger.info(f"Total samples: {total_samples:,}")
        self.logger.info(f"Generation time: {generation_time/60:.1f}m")
        
        # Log noise source statistics
        if sum(self.noise_source_stats.values()) > 0:
            total_noise_calls = sum(self.noise_source_stats.values())
            self.logger.info(f"\nNoise Source Statistics (Total: {total_noise_calls:,} calls):")
            for source, count in sorted(self.noise_source_stats.items(), key=lambda x: x[1], reverse=True):
                pct = (count / total_noise_calls) * 100 if total_noise_calls > 0 else 0
                self.logger.info(f"  - {source:12s}: {count:6,} calls ({pct:5.1f}%)")
            
            # Add to summary
            summary["statistics"]["noise_sources"] = self.noise_source_stats
        
        self.logger.info("=" * 80)

        self.writer.save_json("generation_summary.json", summary)
        return summary

    def _generate_edge_case_sample(self, sample_id: int, edge_type: str, config: Dict) -> Dict:
        """Route to appropriate edge case generator."""

        # Physical extremes
        if edge_type == "high_mass_ratio":
            return self._generate_high_mass_ratio_sample(sample_id, config)
        elif edge_type == "extreme_spins":
            return self._generate_extreme_spin_sample(sample_id, config)
        elif edge_type == "eccentric_mergers":
            return self._generate_eccentric_merger_sample(sample_id, config)
        elif edge_type == "precessing_systems":
            return self._generate_precessing_system_sample(sample_id, config)
        elif edge_type == "short_duration_high_mass":
            return self._generate_short_duration_high_mass_sample(sample_id, config)
        elif edge_type == "low_snr_threshold":
            return self._generate_low_snr_threshold_sample(sample_id, config)

        # Observational extremes
        elif edge_type == "strong_glitches":
            return self._generate_strong_glitch_sample(sample_id, config)
        elif edge_type == "detector_dropout":
            return self._generate_detector_dropout_sample(sample_id, config)
        elif edge_type == "psd_drift":
            return self._generate_psd_drift_sample(sample_id, config)
        elif edge_type == "sky_position_extremes":
            return self._generate_sky_position_extreme_sample(sample_id, config)

        # Statistical extremes
        elif edge_type == "multimodal_posteriors":
            return self._generate_multimodal_posterior_sample(sample_id, config)
        elif edge_type == "heavy_tailed_regions":
            return self._generate_heavy_tailed_sample(sample_id, config)
        elif edge_type == "uninformative_priors":
            return self._generate_uninformative_prior_sample(sample_id, config)

        # Overlapping extremes
        elif edge_type == "subtle_ranking":
            return self._generate_subtle_ranking_overlap(sample_id, config)
        elif edge_type == "heavy_overlaps":
            return self._generate_heavy_overlap(sample_id, config)
        elif edge_type == "partial_overlaps":
            return self._generate_partial_overlap_sample(sample_id, config)

        else:
            self.logger.warning(f"Unknown edge case type: {edge_type}")
            return None

    def _generate_edge_case_multi_noise(self, sample_id: int, edge_type: str, config: Dict, k: int = 1) -> List[Dict]:
        """
        Generate K edge case samples with SAME parameters but DIFFERENT noise realizations.
        
        This is a generic wrapper that routes to specific multi-noise generators if available,
        otherwise wraps single-sample generators and applies multi-noise internally.
        
        Args:
            sample_id (int): Unique sample identifier
            edge_type (str): Type of edge case to generate
            config (Dict): Configuration dictionary
            k (int): Number of noise realizations (default: 1 = no STEP 4)
            
        Returns:
            List[Dict]: K samples with identical parameters but different noise
            
        Note:
            ‚úÖ STEP 4: Fixes amplitude-distance entanglement by showing network same Œ∏ with different amplitudes
        """
        if k <= 1:
            # No STEP 4, return single sample wrapped in list
            sample = self._generate_edge_case_sample(sample_id, edge_type, config)
            return [sample] if sample else []
        
        # ‚úÖ STEP 4: Route to specific multi-noise generators if available
        if edge_type == "psd_drift":
            return self._generate_psd_drift_sample_multi_noise(
                sample_id=sample_id,
                config=config,
                k=k,
            )
        
        # For other edge types, generate single sample and apply multi-noise wrapper
        sample_base = self._generate_edge_case_sample(sample_id, edge_type, config)
        if not sample_base:
            return []
        
        # Extract parameters and priority from base sample
        params = sample_base.get("parameters", [None])[0]
        priority = sample_base.get("priorities", [15.0])[0]
        event_type = sample_base.get("type", "unknown")
        
        if not params:
            self.logger.warning(f"Edge case {edge_type} sample {sample_id} has no parameters")
            return [sample_base]  # Return original if extraction fails
        
        samples_list = []
        
        # Generate K variants with DIFFERENT noise
        for noise_idx in range(k):
            detector_data = {}
            noise_types = {}
            
            for detector_name in self.detectors:
                psd_dict = self.psds[detector_name]
                
                # üîÑ KEY: Different noise for each variant (new random realization)
                noise, noise_type = self._get_noise_for_detector(detector_name, psd_dict)
                noise_types[detector_name] = noise_type
                
                try:
                    strain = self.waveform_generator.generate_waveform(params, detector_name)
                    injected = strain + noise
                except Exception as e:
                    self.logger.warning(f"Failed to generate waveform for edge case {edge_type}: {e}")
                    injected = noise.copy()
                
                if np.any(~np.isfinite(injected)):
                    injected = noise.copy()
                
                detector_data[detector_name] = {
                    "strain": injected.astype(np.float32),
                    "noise": noise.astype(np.float32),
                    "psd": psd_dict.get("psd"),
                    "frequencies": psd_dict.get("frequencies"),
                    "noise_type": noise_type,
                }
            
            # Copy base sample and update with new detector_data and metadata
            sample = {
                "sample_id": f"{edge_type}_{sample_id:06d}_noise{noise_idx}",
                "type": event_type,
                "is_overlap": False,
                "n_signals": 1,
                "is_edge_case": True,
                "edge_case_type": edge_type,
                "parameters": [params],  # Same parameters for all K variants
                "priorities": [priority],  # Same priority for all K variants
                "detector_data": detector_data,  # Different noise
                "metadata": {
                    "sample_id": sample_id,
                    "noise_variant": noise_idx,
                    "n_noise_variants": k,
                    "edge_case_type": edge_type,
                    "event_type": event_type,
                    "noise_sources": noise_types,
                    # FIX #3: Missing SNR in Metadata (edge cases) - Jan 27, 2026
                    "target_snr": float(params.get("target_snr", 0.0)) if params else 0.0,
                    "luminosity_distance": float(params.get("luminosity_distance", 0.0)) if params else 0.0,
                    "chirp_mass": float(params.get("chirp_mass", 0.0)) if params else 0.0,
                },
            }
            
            samples_list.append(sample)
        
        return samples_list

    def _log_progress(self, start_time, generated, total):
        """Log generation progress."""
        elapsed = time.time() - start_time
        rate = generated / elapsed if elapsed > 0 else 0
        eta = (total - generated) / rate if rate > 0 else 0

        self.logger.info(
            f"Rate: {rate:.2f} samples/s | " f"ETA: {eta/60:.1f}m | " f"Elapsed: {elapsed/60:.1f}m"
        )

    def _create_splits_from_batches(
        self, train_frac, val_frac, test_frac, chunk_size, noise_augmentation_k
    ):
        """Load batches and create splits (memory efficient)."""
        import gc

        self.logger.info("Creating splits from saved batches...")

        # Load all batches
        all_samples = self._load_all_batches()

        if not all_samples:
            self.logger.error("No samples to split!")
            return

        self.logger.info(f"Loaded {len(all_samples):,} samples total")

        # Create splits
        splits = self._create_splits(all_samples, train_frac, val_frac, test_frac, stratify=True)

        # Clear all_samples from memory
        del all_samples
        gc.collect()

        # Save splits in chunks
        compress = self.output_format == "pkl_compressed"

        for split_name in ["train", "validation", "test"]:
            split_samples = splits[split_name]["samples"]

            split_metadata = {
                "split": split_name,
                "sample_rate": self.sample_rate,
                "duration": self.duration,
                "detectors": self.detectors,
                "total_samples": len(split_samples),
                "chunk_size": chunk_size,
            }

            self.logger.info(f"Saving {split_name} split: {len(split_samples):,} samples...")

            self.writer.save_split_chunks(
                split_name, split_samples, split_metadata, chunk_size=chunk_size, compress=compress
            )

            # Clear samples from memory
            splits[split_name]["samples"] = []
            gc.collect()

        # Save split indices
        split_indices = {
            "train": splits["train"]["indices"],
            "validation": splits["validation"]["indices"],
            "test": splits["test"]["indices"],
        }

        import json

        with open(self.output_dir / "split_indices.json", "w") as f:
            json.dump(split_indices, f, indent=2)

        self.logger.info("‚úì All splits saved")

    def _load_all_batches(self):
        """Load all batches from disk."""
        import glob
        import pickle

        batch_dir = self.output_dir / "batches"
        batch_files = sorted(glob.glob(str(batch_dir / "batch*.pkl")))

        if not batch_files:
            self.logger.error(f"No batch files found in {batch_dir}")
            return []

        self.logger.info(f"Loading {len(batch_files)} batches...")
        all_samples = []

        for batch_file in tqdm(batch_files, desc="Loading batches"):
            try:
                with open(batch_file, "rb") as f:
                    batch_data = pickle.load(f)
                    if isinstance(batch_data, list):
                        all_samples.extend(batch_data)
                    elif isinstance(batch_data, dict) and "samples" in batch_data:
                        all_samples.extend(batch_data["samples"])
            except Exception as e:
                self.logger.warning(f"Failed to load {batch_file}: {e}")

        self.logger.info(f"Loaded {len(all_samples):,} samples")
        return all_samples

    def _create_splits(
        self,
        all_samples: List[Dict],
        train_frac: float = 0.8,
        val_frac: float = 0.1,
        test_frac: float = 0.1,
        stratify: bool = True,
    ) -> Dict:
        """
        Split dataset into train/val/test with optional stratification.
        """

        import random

        #  ADD THIS: Filter out None samples
        all_samples = [s for s in all_samples if s is not None]

        if len(all_samples) == 0:
            raise ValueError("No valid samples to create splits!")

        n_total = len(all_samples)
        self.logger.info(f"Creating splits from {n_total} valid samples")

        if stratify:
            # Group samples by event type
            type_groups = {}
            for i, sample in enumerate(all_samples):
                #  Extra safety check (though should be unnecessary now)
                if sample is None:
                    continue

                event_type = sample.get("type", "unknown")
                if event_type not in type_groups:
                    type_groups[event_type] = []
                type_groups[event_type].append(i)

            # Split each group proportionally (with exact fraction enforcement)
            train_indices = []
            val_indices = []
            test_indices = []

            for event_type, indices in type_groups.items():
                random.shuffle(indices)
                n_type = len(indices)

                # Use round() instead of int() to minimize rounding errors
                # Then adjust the last bin to ensure exact totals
                n_train = round(n_type * train_frac)
                n_val = round(n_type * val_frac)
                n_test = n_type - n_train - n_val  # Remaining goes to test (no rounding loss)

                train_indices.extend(indices[:n_train])
                val_indices.extend(indices[n_train : n_train + n_val])
                test_indices.extend(indices[n_train + n_val :])

            # Shuffle the splits
            random.shuffle(train_indices)
            random.shuffle(val_indices)
            random.shuffle(test_indices)

        else:
            # Simple random split (with exact fraction enforcement)
            indices = list(range(n_total))
            random.shuffle(indices)

            # Use round() to minimize rounding errors, test gets remainder
            n_train = round(n_total * train_frac)
            n_val = round(n_total * val_frac)
            n_test = n_total - n_train - n_val  # Remaining goes to test (no rounding loss)

            train_indices = indices[:n_train]
            val_indices = indices[n_train : n_train + n_val]
            test_indices = indices[n_train + n_val :]

        #  Filter None when creating splits (extra safety)
        splits = {
            "train": {
                "samples": [all_samples[i] for i in train_indices if all_samples[i] is not None],
                "indices": train_indices,
                "n_samples": len(train_indices),
            },
            "validation": {
                "samples": [all_samples[i] for i in val_indices if all_samples[i] is not None],
                "indices": val_indices,
                "n_samples": len(val_indices),
            },
            "test": {
                "samples": [all_samples[i] for i in test_indices if all_samples[i] is not None],
                "indices": test_indices,
                "n_samples": len(test_indices),
            },
        }

        self.logger.info("Dataset splits created:")
        self.logger.info(
            f"  Train:      {len(train_indices)} samples ({len(train_indices)/n_total*100:.1f}%)"
        )
        self.logger.info(
            f"  Validation: {len(val_indices)} samples ({len(val_indices)/n_total*100:.1f}%)"
        )
        self.logger.info(
            f"  Test:       {len(test_indices)} samples ({len(test_indices)/n_total*100:.1f}%)"
        )

        return splits

    # ============================================================================
    # 1. PHYSICAL EXTREMES
    # ============================================================================

    def _generate_high_mass_ratio_sample(self, sample_id: int, config: Dict) -> Dict:
        """Generate sample with extreme mass ratio (q << 1)."""
        q = np.random.uniform(0.01, 0.1)
        mass_1 = np.random.uniform(5.0, 50.0)
        mass_2 = mass_1 * q

        params = self.parameter_sampler.sample_bbh_parameters("medium", False)
        params["mass_1"] = float(mass_1)
        params["mass_2"] = float(mass_2)
        params["mass_ratio"] = float(q)

        return self._generate_sample_from_params(sample_id, params, "high_mass_ratio")

    def _generate_psd_drift_sample(self, sample_id: int, config: Dict) -> Dict:
        """Generate sample with PSD drift."""

        edge_cases = config.get("edge_cases", {})
        obs_config = edge_cases.get("observational_extremes", {})
        types = obs_config.get("types", {})
        psd_config = types.get("psd_drift", {})

        use_multiple_epochs = psd_config.get("use_multiple_epochs", True)

        event_type = self._sample_event_type()
        params = self._sample_parameters(event_type)

        detector_data = {}

        for detector_name in self.detectors:
            psd_info = self.psds[detector_name]

            if use_multiple_epochs:
                half_duration = self.duration / 2
                half_samples = int(half_duration * self.sample_rate)

                noise1_full, noise1_type = self._get_noise_for_detector(detector_name, psd_info)
                noise1 = noise1_full[:half_samples]

                drift_factor = np.random.uniform(0.5, 2.0)
                psd_drifted = psd_info["psd"] * drift_factor
                psd_info_drift = {"psd": psd_drifted, "frequencies": psd_info["frequencies"]}
                noise2_full, noise2_type = self._get_noise_for_detector(
                    detector_name, psd_info_drift
                )
                noise2 = noise2_full[:half_samples]

                combined_noise = np.concatenate([noise1, noise2])
                # Track first noise type for metadata
                noise_type = noise1_type
            else:
                combined_noise = np.zeros(int(self.duration * self.sample_rate))
                n_samples = len(combined_noise)

                for i in range(n_samples):
                    drift_factor = 1.0 + (i / n_samples) * np.random.uniform(-0.5, 0.5)
                    psd_t = psd_info["psd"] * drift_factor
                    combined_noise[i] = np.random.normal(0, np.sqrt(np.mean(psd_t)))
                noise_type = "synthetic"  # Manually generated

            signal = self.waveform_generator.generate_waveform(params, detector_name)
            combined_strain = combined_noise + signal

            if self.preprocess_enabled:
                combined_strain = self.preprocessor.preprocess(combined_strain, psd_info)

            detector_data[detector_name] = {
                "strain": combined_strain.astype(np.float32),
                "noise": combined_noise.astype(np.float32),
                "psd": psd_info["psd"],
                "frequencies": psd_info["frequencies"],
                "noise_type": noise_type,
            }

        #  Calculate priority
        priority = self._estimate_snr_from_params(params)
        if "target_snr" not in params:
            self._set_target_snr(params, priority, reason="psd_drift_assign")

        # Attach network_snr to params
        attach_network_snr_safe(params)

        return {
            "sample_id": f"psd_drift_{sample_id:06d}",  #  Fixed
            "type": event_type,
            "is_overlap": False,
            "n_signals": 1,  #  Added
            "is_edge_case": True,
            "edge_case_type": "psd_drift",
            "parameters": [params],  #  Must be list
            "priorities": [self._normalize_priority_to_01(priority)],  # Normalized
            "detector_data": detector_data,
            "metadata": {
                "sample_id": sample_id,  #  Fixed
                "event_type": event_type,
                "psd_drift": True,
                "multiple_epochs": use_multiple_epochs,
            },
        }

    def _generate_psd_drift_sample_multi_noise(self, sample_id: int, config: Dict, k: int = 1) -> List[Dict]:
        """
        Generate K PSD drift samples with SAME parameters but DIFFERENT noise realizations.
        
        Args:
            k (int): Number of noise realizations (default: 1 = no STEP 4)
            
        Returns:
            List[Dict]: K samples with identical parameters but different noise
        """
        if k <= 1:
            # No STEP 4, return single sample wrapped in list
            return [self._generate_psd_drift_sample(sample_id, config)]
        
        # ‚úÖ STEP 4: Generate parameters ONCE
        edge_cases = config.get("edge_cases", {})
        obs_config = edge_cases.get("observational_extremes", {})
        types = obs_config.get("types", {})
        psd_config = types.get("psd_drift", {})
        use_multiple_epochs = psd_config.get("use_multiple_epochs", True)
        
        event_type = self._sample_event_type()
        params = self._sample_parameters(event_type)
        
        priority = self._estimate_snr_from_params(params)
        if "target_snr" not in params:
            self._set_target_snr(params, priority, reason="psd_drift_assign")
        
        attach_network_snr_safe(params)
        
        samples_list = []
        
        # Generate K variants with DIFFERENT noise
        for noise_idx in range(k):
            detector_data = {}
            noise_types = {}
            
            for detector_name in self.detectors:
                psd_info = self.psds[detector_name]
                
                if use_multiple_epochs:
                    half_duration = self.duration / 2
                    half_samples = int(half_duration * self.sample_rate)
                    
                    # üîÑ KEY: Different noise for each variant
                    noise1_full, noise1_type = self._get_noise_for_detector(detector_name, psd_info)
                    noise1 = noise1_full[:half_samples]
                    
                    drift_factor = np.random.uniform(0.5, 2.0)
                    psd_drifted = psd_info["psd"] * drift_factor
                    psd_info_drift = {"psd": psd_drifted, "frequencies": psd_info["frequencies"]}
                    noise2_full, noise2_type = self._get_noise_for_detector(
                        detector_name, psd_info_drift
                    )
                    noise2 = noise2_full[:half_samples]
                    
                    combined_noise = np.concatenate([noise1, noise2])
                    noise_type = noise1_type
                else:
                    combined_noise = np.zeros(int(self.duration * self.sample_rate))
                    n_samples = len(combined_noise)
                    
                    for i in range(n_samples):
                        drift_factor = 1.0 + (i / n_samples) * np.random.uniform(-0.5, 0.5)
                        psd_t = psd_info["psd"] * drift_factor
                        combined_noise[i] = np.random.normal(0, np.sqrt(np.mean(psd_t)))
                    noise_type = "synthetic"
                
                signal = self.waveform_generator.generate_waveform(params, detector_name)
                combined_strain = combined_noise + signal
                
                if self.preprocess_enabled:
                    combined_strain = self.preprocessor.preprocess(combined_strain, psd_info)
                
                detector_data[detector_name] = {
                    "strain": combined_strain.astype(np.float32),
                    "noise": combined_noise.astype(np.float32),
                    "psd": psd_info["psd"],
                    "frequencies": psd_info["frequencies"],
                    "noise_type": noise_type,
                }
                noise_types[detector_name] = noise_type
            
            sample = {
                "sample_id": f"psd_drift_{sample_id:06d}_noise{noise_idx}",
                "type": event_type,
                "is_overlap": False,
                "n_signals": 1,
                "is_edge_case": True,
                "edge_case_type": "psd_drift",
                "parameters": [params],
                "priorities": [self._normalize_priority_to_01(priority)],
                "detector_data": detector_data,
                "metadata": {
                    "sample_id": sample_id,
                    "noise_variant": noise_idx,
                    "n_noise_variants": k,
                    "event_type": event_type,
                    "psd_drift": True,
                    "multiple_epochs": use_multiple_epochs,
                    "noise_sources": noise_types,
                },
            }
            
            samples_list.append(sample)
        
        return samples_list

    def _generate_sky_position_extreme_sample(self, sample_id: int, config: Dict) -> Dict:
        """Generate sample with extreme sky position."""

        edge_cases = config.get("edge_cases", {})
        obs_config = edge_cases.get("observational_extremes", {})
        types = obs_config.get("types", {})
        sky_config = types.get("sky_position_extremes", {})

        use_uniform_sky = sky_config.get("use_uniform_sky", True)

        event_type = self._sample_event_type()
        params = self._sample_parameters(event_type)

        if use_uniform_sky:
            params["ra"] = np.random.uniform(0, 2 * np.pi)
            params["dec"] = np.arcsin(np.random.uniform(-1, 1))
        else:
            if np.random.random() < 0.5:
                params["dec"] = np.random.uniform(np.pi / 2 - 0.2, np.pi / 2)
            else:
                params["dec"] = np.random.uniform(-np.pi / 2, -np.pi / 2 + 0.2)
            params["ra"] = np.random.uniform(0, 2 * np.pi)

        detector_data = {}

        for detector_name in self.detectors:
            psd_info = self.psds[detector_name]
            noise, noise_type = self._get_noise_for_detector(detector_name, psd_info)
            signal = self.waveform_generator.generate_waveform(params, detector_name)
            combined_strain = noise + signal

            if self.preprocess_enabled:
                combined_strain = self.preprocessor.preprocess(combined_strain, psd_info)

            detector_data[detector_name] = {
                "strain": combined_strain.astype(np.float32),
                "noise": noise.astype(np.float32),
                "psd": psd_info["psd"],
                "frequencies": psd_info["frequencies"],
                "noise_type": noise_type,
            }

        #  Calculate priority
        priority = self._estimate_snr_from_params(params)
        if "target_snr" not in params:
            self._set_target_snr(params, priority, reason="sky_extreme_assign")

        # Attach network_snr to params
        attach_network_snr_safe(params)

        return {
            "sample_id": f"sky_extreme_{sample_id:06d}",  #  Fixed
            "type": event_type,
            "is_overlap": False,
            "n_signals": 1,  #  Added
            "is_edge_case": True,
            "edge_case_type": "sky_position_extremes",
            "parameters": [params],  #  Must be list
            "priorities": [self._normalize_priority_to_01(priority)],  # Normalized
            "detector_data": detector_data,
            "metadata": {
                "sample_id": sample_id,  #  Fixed
                "event_type": event_type,
                "ra": params["ra"],
                "dec": params["dec"],
                "near_pole": abs(params["dec"]) > np.pi / 2 - 0.2,
            },
        }

    def _generate_heavy_tailed_sample(self, sample_id: int, config: Dict) -> Dict:
        """
        Generate sample with parameters from heavy-tailed distributions.
        Tests model's ability to handle rare, extreme parameter values.
        """
        # Extract config
        edge_cases = config.get("edge_cases", {})
        stat_config = edge_cases.get("statistical_extremes", {})
        types = stat_config.get("types", {})
        heavy_config = types.get("heavy_tailed_regions", {})

        log_uniform_distance = heavy_config.get("log_uniform_distance", True)

        # Sample event
        event_type = self._sample_event_type()
        params = self._sample_parameters(event_type)

        # Override with heavy-tailed distributions
        if log_uniform_distance:
             # Log-uniform distance (more distant events)
            params["luminosity_distance"] = 10 ** np.random.uniform(
                 np.log10(100), np.log10(5000)  # 100 Mpc  # 5 Gpc
             )
             
             # CRITICAL FIX (Jan 19, 2026): Add jitter after log-uniform sampling
             # Prevents 59.46% duplicate distances that cause gradient collapse
            params["luminosity_distance"] *= np.exp(np.random.normal(0, 0.03))
            
            # CRITICAL FIX (Dec 29, 18:45 UTC): Apply physics-realistic distance caps
            # Log-uniform can create undetectable outliers (10,000+ Mpc)
            event_type = params.get('type', 'BBH')
            if event_type == 'BNS':
                params["luminosity_distance"] = min(params["luminosity_distance"], 400.0)
            elif event_type == 'BBH':
                params["luminosity_distance"] = min(params["luminosity_distance"], 8000.0)
            elif event_type == 'NSBH':
                params["luminosity_distance"] = min(params["luminosity_distance"], 2500.0)
            
            # FIX #1: DO NOT recompute SNR after clipping
            # Reason: Original SNR was sampled from regime distribution
            # Clipping is a physical constraint, not measurement error
            # Validate SNR is still valid after distance modification
            params['target_snr'] = self._validate_snr(
                params.get('target_snr', self.reference_snr),
                context="psd_drift_distance_clipping"
            )

            # Heavy-tailed mass distribution (Student-t like)
            if event_type == "BBH":
                # Sample from tails
                if np.random.random() < 0.5:
                    # High mass tail
                    params["mass_1"] = np.random.uniform(50, 100)
                    params["mass_2"] = np.random.uniform(30, params["mass_1"])
                else:
                    # Low mass tail (unusual for BBH)
                    params["mass_1"] = np.random.uniform(3, 8)
                    params["mass_2"] = np.random.uniform(2, params["mass_1"])

            # FIX #1: Keep original SNR even after mass modification
            # Mass extremes modify Mc, but this is already accounted for during sampling
            params['target_snr'] = self._validate_snr(
                params.get('target_snr', self.reference_snr),
                context="psd_drift_mass_extremes"
            )

        return self._generate_sample_from_params(sample_id, params, "heavy_tailed")

    def _generate_uninformative_prior_sample(self, sample_id: int, config: Dict) -> Dict:
        """
        Generate sample with very broad parameter ranges (uninformative priors).
        Tests model's ability to infer parameters with minimal prior information.
        """
        # Extract config
        edge_cases = config.get("edge_cases", {})
        stat_config = edge_cases.get("statistical_extremes", {})
        types = stat_config.get("types", {})
        prior_config = types.get("uninformative_priors", {})

        broad_multiplier = prior_config.get("broad_prior_multiplier", 10.0)

        # Sample event
        event_type = self._sample_event_type()
        params = self._sample_parameters(event_type)

        # Expand parameter ranges (simulate uninformative prior)
        # In practice, this means sampling from very broad distributions
        # Scale the distance range by broad_multiplier
        max_distance = 5000 * broad_multiplier
        params["luminosity_distance"] = np.random.uniform(10, max_distance)  # Very broad
        
        # CRITICAL FIX (Jan 19, 2026): Add jitter after broad uniform sampling
        # Prevents 59.46% duplicate distances that cause gradient collapse
        params["luminosity_distance"] *= np.exp(np.random.normal(0, 0.03))
        
        # CRITICAL FIX (Dec 29, 18:45 UTC): Apply physics-realistic distance caps
        # Broad uniform sampling can exceed detection horizon
        event_type = params.get('type', 'BBH')
        if event_type == 'BNS':
            params["luminosity_distance"] = min(params["luminosity_distance"], 400.0)
        elif event_type == 'BBH':
            params["luminosity_distance"] = min(params["luminosity_distance"], 8000.0)
        elif event_type == 'NSBH':
            params["luminosity_distance"] = min(params["luminosity_distance"], 2500.0)
        
        # FIX #1: Preserve original SNR-distance relationship
        # Broad uniform sampling can produce unusual distance, but SNR was
        # correctly sampled upfront and should not change
        params['target_snr'] = self._validate_snr(
            params.get('target_snr', self.reference_snr),
            context="uninformative_prior"
        )
        params["ra"] = np.random.uniform(0, 2 * np.pi)
        params["dec"] = np.arcsin(np.random.uniform(-1, 1))
        params["psi"] = np.random.uniform(0, np.pi)
        params["phase"] = np.random.uniform(0, 2 * np.pi)

        # Generate sample
        return self._generate_sample_from_params(sample_id, params, "uninformative_prior")

    def _generate_subtle_ranking_overlap(self, sample_id: int, config: Dict) -> Dict:
        """
        Generate overlapping signals with very similar SNRs (hard to rank).
        Tests prioritization network's ability to distinguish close SNRs.
        """
        # Extract config
        edge_cases = config.get("edge_cases", {})
        overlap_config = edge_cases.get("overlapping_extremes", {})
        subtle_config = overlap_config.get("subtle_ranking", {})

        snr_diff_range = subtle_config.get("snr_difference_range", [0.5, 2.5])

        # Generate 2-3 signals with similar SNRs
        n_signals = np.random.choice([2, 3], p=[0.7, 0.3])

        # Base SNR
        base_snr = np.random.uniform(10, 20)

        parameters_list = []
        for i in range(n_signals):
            event_type = self._sample_event_type()
            params = self._sample_parameters(event_type)

            # Set similar SNRs
            snr_offset = np.random.uniform(snr_diff_range[0], snr_diff_range[1])
            if "target_snr" not in params:
                self._set_target_snr(
                    params, base_snr + (i - 1) * snr_offset, reason="subtle_ranking_base_snr"
                )

            # Slightly offset times
            params["geocent_time"] = i * 0.5  # 0.5s apart

            parameters_list.append(params)

        # Generate combined sample
        return self.generate_custom_overlap(parameters_list, sample_id, is_edge_case=True)

    def _generate_pre_merger_sample(self, sample_id: int) -> Dict:
        """
        Generate pre-merger sample for early warning training.

        The data window ends BEFORE the merger happens, containing only the
        inspiral phase. The model must learn to:
        - Detect that a signal is present (inspiral)
        - Predict that a merger is coming soon
        - Estimate time_to_merger
        - Infer parameters from incomplete data

        This is critical for early warning systems.
        """

        premerger_config = self.config.get("premerger_config", {})

        # Check if enabled
        if not premerger_config.get("enabled", True):
            sample = self._generate_single_sample(
                sample_id=sample_id, is_edge_case=False, add_glitches=False, preprocess=True
            )
            # Ensure priorities and normalization
            sample = self._ensure_sample_priorities(sample)
            return sample

        # Extract configuration parameters
        time_to_merger_range = premerger_config.get("time_to_merger_range", [0.5, 5.0])
        event_types = premerger_config.get("event_types", ["BBH", "BNS", "NSBH"])
        min_snr = premerger_config.get("min_snr", 8)

        # Sample event type and parameters (use configured global distribution but restricted to allowed list)
        event_type = self._sample_event_type_subset(event_types)
        params = self._sample_parameters(event_type)

        # Ensure detectable SNR: if sampler provided one, enforce minimum; otherwise set to min_snr
        if "target_snr" in params:
            try:
                self._set_target_snr(
                    params,
                    max(float(params.get("target_snr", 0)), min_snr),
                    reason="premerger_min_enforce",
                )
            except Exception:
                self._set_target_snr(params, min_snr, reason="premerger_min_enforce_fail")
        else:
            self._set_target_snr(params, min_snr, reason="premerger_min_default")

        # Sample time to merger
        time_to_merger = np.random.uniform(time_to_merger_range[0], time_to_merger_range[1])

        # Shift merger time outside window
        params["geocent_time"] = self.duration / 2 + time_to_merger

        # Generate detector data
        detector_data = {}

        for detector_name in self.detectors:
            psd_dict = self.psds[detector_name]
            noise, noise_type = self._get_noise_for_detector(detector_name, psd_dict)

            try:
                strain = self.waveform_generator.generate_waveform(params, detector_name)

                expected_length = int(self.duration * self.sample_rate)
                if len(strain) > expected_length:
                    strain = strain[:expected_length]
                elif len(strain) < expected_length:
                    strain = np.pad(strain, (0, expected_length - len(strain)), mode="constant")

                combined = noise + strain

                if self.preprocess_enabled:
                    combined = self.preprocessor.preprocess(combined, psd_dict)

                detector_data[detector_name] = {
                    "strain": combined.astype(np.float32),
                    "noise": noise.astype(np.float32),
                    "psd": psd_dict["psd"],
                    "frequencies": psd_dict["frequencies"],
                    "noise_type": noise_type,
                }

            except Exception as e:
                self.logger.warning(
                    f"Failed to generate pre-merger waveform for {detector_name}: {e}"
                )
                detector_data[detector_name] = {
                    "strain": noise.astype(np.float32),
                    "noise": noise.astype(np.float32),
                    "psd": psd_dict["psd"],
                    "frequencies": psd_dict["frequencies"],
                }

        #  Calculate priority
        priority = self._estimate_snr_from_params(params)
        # Only set target_snr if not already present (preserve sampled values)
        if "target_snr" not in params:
            self._set_target_snr(params, priority, reason="pre_merger_assign")

        # Attach network_snr to params
        attach_network_snr_safe(params)

        # Construct sample
        # Track noise types for each detector
        noise_types = {
            det: detector_data[det].get("noise_type", "synthetic") for det in self.detectors
        }

        sample = {
            "sample_id": f"pre_merger_{sample_id:06d}",  #  Changed from 'id' to 'sample_id'
            "type": event_type,
            "is_overlap": False,
            "n_signals": 1,  #  Added
            "is_edge_case": False,
            "is_premerger": True,
            "parameters": [params],  #  Must be LIST
            "priorities": [self._normalize_priority_to_01(priority)],  # Normalized
            "detector_data": detector_data,
            "noise_type": noise_types,  # Track which detectors used real vs synthetic noise
            "metadata": {
                "sample_id": f"pre_merger_{sample_id:06d}",
                "event_type": event_type,
                "signal_parameters": [params],  # ‚úÖ ADD: Match new format for consistency
                "time_to_merger": time_to_merger,
                "merger_in_window": False,
                "phase": "inspiral_only",
                "window_end_to_merger_seconds": time_to_merger,
                "merger_time": params["geocent_time"],
                "window_duration": self.duration,
                "contains_merger": False,
                "is_complete_signal": False,
            },
        }

        # Optional debug SNR logging: compare sampled target_snr vs pre-injection estimate and actual injected SNR
        try:
            debug_enabled = bool(self.config.get("debug_snr_diagnostic", False))
        except Exception:
            debug_enabled = False

        if debug_enabled and self._debug_snr_count < self._debug_snr_limit:
            try:
                ref_det = self.detectors[0]
                ref_psd = self.psds.get(ref_det)
                # For pre-merger, single parameter set
                try:
                    wf = self.waveform_generator.generate_waveform(params, ref_det)
                    pre_calc = float(self.injector._compute_optimal_snr(wf, ref_psd)) if ref_psd is not None else float("nan")
                except Exception:
                    pre_calc = float("nan")

                # Target SNR from params
                target_snr = float(params.get("target_snr", float("nan")))

                self.logger.info(
                    f"[SNR-DIAG] {sample['sample_id']}: target_snr={target_snr} pre_estimate={pre_calc}"
                )
            except Exception as e:
                self.logger.debug(f"SNR debug logging failed for {sample.get('sample_id')}: {e}")

            self._debug_snr_count += 1

        return sample

    def _generate_pre_merger_sample_multi_noise(self, sample_id: int, k: int = 1) -> List[Dict]:
        """
        Generate K pre-merger samples with SAME parameters but DIFFERENT noise realizations.
        
        Args:
            k (int): Number of noise realizations (default: 1 = no STEP 4)
            
        Returns:
            List[Dict]: K samples with identical parameters but different noise
        """
        if k <= 1:
            # No STEP 4, return single sample wrapped in list
            return [self._generate_pre_merger_sample(sample_id)]
        
        # ‚úÖ STEP 4: Generate parameters ONCE
        premerger_config = self.config.get("premerger_config", {})
        
        if not premerger_config.get("enabled", True):
            return [self._generate_pre_merger_sample(sample_id)]
        
        time_to_merger_range = premerger_config.get("time_to_merger_range", [0.5, 5.0])
        event_types = premerger_config.get("event_types", ["BBH", "BNS", "NSBH"])
        min_snr = premerger_config.get("min_snr", 8)
        
        event_type = self._sample_event_type_subset(event_types)
        params = self._sample_parameters(event_type)
        
        if "target_snr" in params:
            try:
                self._set_target_snr(
                    params,
                    max(float(params.get("target_snr", 0)), min_snr),
                    reason="premerger_min_enforce",
                )
            except Exception:
                self._set_target_snr(params, min_snr, reason="premerger_min_enforce_fail")
        else:
            self._set_target_snr(params, min_snr, reason="premerger_min_default")
        
        time_to_merger = np.random.uniform(time_to_merger_range[0], time_to_merger_range[1])
        params["geocent_time"] = self.duration / 2 + time_to_merger
        
        attach_network_snr_safe(params)
        
        priority = self._estimate_snr_from_params(params)
        
        samples_list = []
        
        # Generate K variants with DIFFERENT noise
        for noise_idx in range(k):
            detector_data = {}
            noise_types = {}
            
            for detector_name in self.detectors:
                psd_dict = self.psds[detector_name]
                
                # üîÑ KEY: Different noise for each variant (new random realization)
                noise, noise_type = self._get_noise_for_detector(detector_name, psd_dict)
                noise_types[detector_name] = noise_type
                
                try:
                    strain = self.waveform_generator.generate_waveform(params, detector_name)
                    injected = strain + noise
                except Exception:
                    injected = noise
                
                if np.any(~np.isfinite(injected)):
                    injected = noise.copy()
                
                detector_data[detector_name] = {
                    "strain": injected.astype(np.float32),
                    "noise": noise.astype(np.float32),
                    "psd": psd_dict["psd"],
                    "frequencies": psd_dict["frequencies"],
                    "noise_type": noise_type,
                }
            
            sample = {
                "sample_id": f"pre_merger_{sample_id:06d}_noise{noise_idx}",
                "type": event_type,
                "is_overlap": False,
                "n_signals": 1,
                "is_edge_case": False,
                "is_premerger": True,
                "parameters": [params],
                "priorities": [self._normalize_priority_to_01(priority)],
                "detector_data": detector_data,
                "noise_type": noise_types,
                "metadata": {
                    "sample_id": sample_id,
                    "noise_variant": noise_idx,
                    "n_noise_variants": k,
                    "event_type": event_type,
                    "phase": "inspiral_only",  
                    "is_premerger": True,
                    "time_to_merger": time_to_merger,  
                    "window_end_to_merger_seconds": time_to_merger,
                    "merger_in_window": False,  
                    "merger_time": params["geocent_time"],
                    "window_duration": self.duration,
                    "contains_merger": False,
                    "is_complete_signal": False,
                    "noise_sources": noise_types,
                },
            }
            
            samples_list.append(sample)
        
        return samples_list

    def _generate_extreme_spin_sample(self, sample_id: int, config: Dict) -> Dict:
        """Generate sample with near-maximal spins."""
        spin_config = (
            config.get("edge_cases", {})
            .get("physical_extremes", {})
            .get("types", {})
            .get("extreme_spins", {})
        )

        params = self.parameter_sampler.sample_bbh_parameters("medium", False)

        # Near-maximal spins (clipped to 0.99 physical limit)
        params["a1"] = float(np.clip(np.random.uniform(0.9, 0.99), 0.0, 0.99))
        params["a2"] = float(np.clip(np.random.uniform(0.9, 0.99), 0.0, 0.99))

        # Random alignment
        if np.random.random() < spin_config["aligned_fraction"]:
            params["tilt1"] = 0.0
            params["tilt2"] = 0.0
        else:
            params["tilt1"] = float(np.random.uniform(0, np.pi))
            params["tilt2"] = float(np.random.uniform(0, np.pi))

        return self._generate_sample_from_params(sample_id, params, "extreme_spins")

    def _generate_heavy_overlap(self, sample_id: int, config: Dict) -> Dict:
        """
        Generate sample with many (5-8) overlapping signals.
        Tests model's ability to detect and separate many simultaneous events.
        """
        # Extract config
        edge_cases = config.get("edge_cases", {})
        overlap_config = edge_cases.get("overlapping_extremes", {})
        heavy_config = overlap_config.get("heavy_overlaps", {})

        n_signals_range = heavy_config.get("n_signals_range", [5, 8])
        n_signals = np.random.randint(n_signals_range[0], n_signals_range[1] + 1)

        # Generate many signals
        parameters_list = []
        for i in range(n_signals):
            # Use configured EVENT_TYPE_DISTRIBUTION for sampling event types
            event_type = self._sample_event_type()
            params = self._sample_parameters(event_type)

            # Spread out in time
            params["geocent_time"] = np.random.uniform(-1, 1)

            parameters_list.append(params)

        return self.generate_custom_overlap(parameters_list, sample_id, is_edge_case=True)

    def _generate_eccentric_merger_sample(self, sample_id: int, config: Dict) -> Dict:
        """Generate sample with residual eccentricity."""
        ecc_config = (
            config.get("edge_cases", {})
            .get("physical_extremes", {})
            .get("types", {})
            .get("eccentric_mergers", {})
        )

        params = self.parameter_sampler.sample_bbh_parameters("medium", False)
        params["eccentricity"] = float(
            np.random.uniform(
                ecc_config["eccentricity_range"][0], ecc_config["eccentricity_range"][1]
            )
        )

        return self._generate_sample_from_params(sample_id, params, "eccentric")

    def _generate_precessing_system_sample(self, sample_id: int, config: Dict) -> Dict:
        """Generate sample with random spin orientations (precession)."""
        params = self.parameter_sampler.sample_bbh_parameters("medium", False)

        # Random spin magnitudes
        params["a1"] = float(np.random.uniform(0.3, 0.9))
        params["a2"] = float(np.random.uniform(0.3, 0.9))

        # Random orientations (isotropic)
        params["tilt1"] = float(np.arccos(np.random.uniform(-1, 1)))
        params["tilt2"] = float(np.arccos(np.random.uniform(-1, 1)))
        params["phi12"] = float(np.random.uniform(0, 2 * np.pi))
        params["phi_jl"] = float(np.random.uniform(0, 2 * np.pi))

        return self._generate_sample_from_params(sample_id, params, "precessing")

    def _generate_short_duration_high_mass_sample(self, sample_id: int, config: Dict) -> Dict:
        """Generate high-mass BBH with few inspiral cycles."""
        mass_config = (
            config.get("edge_cases", {})
            .get("physical_extremes", {})
            .get("types", {})
            .get("short_duration_high_mass", {})
        )

        params = self.parameter_sampler.sample_bbh_parameters("medium", False)
        params["mass_1"] = float(
            np.random.uniform(mass_config["mass_range"][0], mass_config["mass_range"][1])
        )
        params["mass_2"] = float(np.random.uniform(20, params["mass_1"]))

        # Higher starting frequency ‚Üí fewer cycles
        params["f_lower"] = float(np.random.uniform(40.0, 100.0))

        return self._generate_sample_from_params(sample_id, params, "short_duration_high_mass")

    def _generate_low_snr_threshold_sample(self, sample_id: int, config: Dict) -> Dict:
        """Generate near-threshold low-SNR sample."""
        snr_config = (
            config.get("edge_cases", {})
            .get("physical_extremes", {})
            .get("types", {})
            .get("low_snr_threshold", {})
        )

        params = self.parameter_sampler.sample_bbh_parameters("low", False)
        if "target_snr" not in params:
            self._set_target_snr(
                params,
                float(np.random.uniform(snr_config["snr_range"][0], snr_config["snr_range"][1])),
                reason="low_snr_threshold",
            )

        return self._generate_sample_from_params(sample_id, params, "low_snr_threshold")

    def _generate_sample_from_params(self, sample_id: int, params: Dict, edge_type: str) -> Dict:
        """
        Generate detector data from given parameters.
        Used by edge case generators that need specific parameter control.
        """

        detector_data = {}

        for detector_name in self.detectors:
            psd_dict = self.psds[detector_name]

            # Generate noise
            noise, _ = self._get_noise_for_detector(detector_name, psd_dict)

            # Generate and inject signal
            signal = self.waveform_generator.generate_waveform(params, detector_name)

            # Combine
            combined = noise + signal

            # Preprocess
            if self.preprocess_enabled:
                combined = self.preprocessor.preprocess(combined, psd_dict)

            detector_data[detector_name] = {
                "strain": combined.astype(np.float32),
                "noise": noise.astype(np.float32),
                "psd": psd_dict["psd"],
                "frequencies": psd_dict["frequencies"],
            }

        #  Calculate priority BEFORE creating sample
        priority = self._estimate_snr_from_params(params)
        # Preserve any existing sampled target_snr
        if "target_snr" not in params:
            self._set_target_snr(params, priority, reason="_generate_sample_from_params")

        # Attach network_snr to params
        attach_network_snr_safe(params)

        #  Create sample with all required fields
        sample = {
            "sample_id": f"{edge_type}_{sample_id:06d}",
            "type": params.get("type", "BBH"),
            "is_overlap": False,
            "n_signals": 1,
            "is_edge_case": True,
            "edge_case_type": edge_type,
            "parameters": [params],  # Must be list
            "priorities": [self._normalize_priority_to_01(priority)],  # Normalized
            "detector_data": detector_data,
            "metadata": {
                "sample_id": sample_id,
                "edge_case_type": edge_type,
                "signal_parameters": params,
            },
        }

        # Optional debug SNR logging for single events
        try:
            debug_enabled = bool(self.config.get("debug_snr_diagnostic", False))
        except Exception:
            debug_enabled = False

        if debug_enabled and self._debug_snr_count < self._debug_snr_limit:
            try:
                ref_det = self.detectors[0]
                ref_psd = self.psds.get(ref_det)
                if params:
                    # Pre-injection SNR estimate from waveform
                    try:
                        wf = self.waveform_generator.generate_waveform(params, ref_det)
                        pre = (
                            float(self.injector._compute_optimal_snr(wf, ref_psd))
                            if ref_psd is not None
                            else float("nan")
                        )
                    except Exception:
                        pre = float("nan")

                    # actual SNR reported by injector for reference detector
                    meta = detector_data.get(ref_det, {}).get("metadata", {})
                    actual = meta.get("actual_snr", meta.get("target_snr", float("nan")))

                    # If metadata is missing, use target_snr as fallback (for direct waveform injection)
                    if np.isnan(actual):
                        actual = params.get("target_snr", float("nan"))

                    self.logger.info(
                        f"[SNR-DIAG] {sample['sample_id']}: target={params.get('target_snr')} pre_estimate={pre} actual={actual}"
                    )
                else:
                    self.logger.info(f"[SNR-DIAG] {sample['sample_id']}: noise-only")
            except Exception as e:
                self.logger.debug(f"SNR debug logging failed for {sample.get('sample_id')}: {e}")

            self._debug_snr_count += 1

        return sample

    # ============================================================================
    # 2. OBSERVATIONAL EXTREMES
    # ============================================================================

    def _generate_strong_glitch_sample(self, sample_id: int, config: Dict) -> Dict:
        """Generate sample with strong glitches."""

        #  Safe config navigation with defaults
        edge_cases = config.get("edge_cases", {})
        obs_extremes = edge_cases.get("observational_extremes", {})
        types_config = obs_extremes.get("types", {})
        glitch_config = types_config.get(
            "strong_glitches",
            {"glitch_prob": 0.8, "glitch_types": ["blip", "whistle", "scattered_light"]},
        )

        # Extract parameters with defaults
        glitch_prob = glitch_config.get("glitch_prob", 0.8)
        glitch_types = glitch_config.get("glitch_types", ["blip", "whistle", "scattered_light"])

        # Generate base sample
        sample = self._generate_single_sample(
            sample_id=sample_id,
            is_edge_case=True,
            add_glitches=False,  # We'll add manually
            preprocess=True,
        )

        # Add strong glitch if probability check passes
        if np.random.random() < glitch_prob:
            glitch_type = np.random.choice(glitch_types)

            # Add glitch to detector data
            for det_name in sample.get("detector_data", {}).keys():
                # Your glitch injection logic here
                pass

            sample["has_glitch"] = True
            sample["glitch_type"] = glitch_type

        sample["edge_case_type"] = "strong_glitches"

        # Ensure priorities exist and are normalized
        sample = self._ensure_sample_priorities(sample)

        return sample

    def _inject_glitch(self, strain, glitch_type):
        """Inject specific glitch type into strain data."""
        N = len(strain)
        t = np.arange(N) / self.sample_rate
        # Estimate local noise floor from the provided strain (use as scale)
        noise_std = float(np.std(strain)) if np.std(strain) > 0 else 1e-12

        if glitch_type == "blip":
            # Short Gaussian transient scaled to local noise
            t_glitch = np.random.uniform(0.5, 3.5)
            sigma = np.random.uniform(0.01, 0.05)
            # amplitude in noise-relative units
            amplitude_scale = np.random.uniform(5, 20)
            amplitude = amplitude_scale * noise_std
            glitch = amplitude * np.exp(-((t - t_glitch) ** 2) / (2 * sigma**2))

        elif glitch_type == "whistle":
            # Chirping noise artifact scaled to local noise
            f0 = np.random.uniform(100, 500)
            f1 = np.random.uniform(f0, 1000)
            t_start = np.random.uniform(0.5, 2.0)
            duration = np.random.uniform(0.1, 0.5)
            mask = (t >= t_start) & (t < t_start + duration)
            phase = (
                2 * np.pi * (f0 * (t - t_start) + 0.5 * (f1 - f0) / duration * (t - t_start) ** 2)
            )
            glitch = np.zeros_like(strain)
            # per-sample amplitude scale relative to noise
            whistle_scale = np.random.uniform(2, 10)
            glitch[mask] = (whistle_scale * noise_std) * np.sin(phase[mask])

        elif glitch_type == "scattered_light":
            # Low-frequency modulation scaled to local noise
            f_scatter = np.random.uniform(10, 60)
            scatter_scale = np.random.uniform(1, 5)
            amplitude = scatter_scale * noise_std
            glitch = amplitude * np.sin(2 * np.pi * f_scatter * t)

        return strain + glitch

    def _generate_detector_dropout_sample(self, sample_id: int, config: Dict) -> Dict:
        """
        Generate sample with detector dropout.

        Simulates scenarios where one or more detectors are offline or have
        data quality issues. This tests the model's ability to work with
        incomplete detector networks.

        Args:
            sample_id: Unique sample identifier
            config: Full configuration dict

        Returns:
            Sample dict with some detectors zeroed out
        """

        # Safe config navigation
        edge_cases = config.get("edge_cases", {})
        obs_extremes = edge_cases.get("observational_extremes", {})
        types = obs_extremes.get("types", {})
        dropout_config = types.get("detector_dropout", {})

        # Extract parameters with defaults
        dropout_prob = dropout_config.get("dropout_prob", 0.3)
        min_active = dropout_config.get("min_active_detectors", 1)

        # Generate base sample with all detectors
        sample = self._generate_single_sample(
            sample_id=sample_id, is_edge_case=True, add_glitches=False, preprocess=True
        )

        # Determine if we apply dropout
        if np.random.random() < dropout_prob:
            # Calculate how many detectors to drop
            n_detectors = len(self.detectors)

            if n_detectors > min_active:
                # Drop 1 to (n_detectors - min_active) detectors
                max_dropout = n_detectors - min_active
                n_to_drop = np.random.randint(1, max_dropout + 1)

                # Randomly select which detectors to keep active
                n_active = n_detectors - n_to_drop
                active_detectors = list(
                    np.random.choice(self.detectors, size=n_active, replace=False)
                )

                # Zero out dropped detectors
                for det_name in sample["detector_data"].keys():
                    if det_name not in active_detectors:
                        # Set strain to zeros
                        strain_shape = sample["detector_data"][det_name]["strain"].shape
                        sample["detector_data"][det_name]["strain"] = np.zeros(
                            strain_shape, dtype=np.float32
                        )

                # Update metadata
                sample["active_detectors"] = active_detectors
                sample["dropped_detectors"] = [
                    d for d in self.detectors if d not in active_detectors
                ]
                sample["n_active_detectors"] = n_active
                sample["n_dropped_detectors"] = n_to_drop
                sample["has_dropout"] = True

                self.logger.debug(
                    f"Sample {sample_id}: Detector dropout - "
                    f"{n_to_drop} dropped, {n_active} active: {active_detectors}"
                )
            else:
                # Can't drop any without violating min_active constraint
                sample["active_detectors"] = list(self.detectors)
                sample["dropped_detectors"] = []
                sample["n_active_detectors"] = n_detectors
                sample["n_dropped_detectors"] = 0
                sample["has_dropout"] = False
        else:
            # No dropout applied
            sample["active_detectors"] = list(self.detectors)
            sample["dropped_detectors"] = []
            sample["n_active_detectors"] = len(self.detectors)
            sample["n_dropped_detectors"] = 0
            sample["has_dropout"] = False

        # Mark as edge case
        sample["edge_case_type"] = "detector_dropout"

        # Update metadata
        if "metadata" not in sample:
            sample["metadata"] = {}

        sample["metadata"].update(
            {
                "edge_case_type": "detector_dropout",
                "active_detectors": sample["active_detectors"],
                "dropped_detectors": sample["dropped_detectors"],
                "n_active": sample["n_active_detectors"],
            }
        )

        # Ensure priorities exist and are normalized
        sample = self._ensure_sample_priorities(sample)

        return sample

    # ============================================================================
    # 3. STATISTICAL EXTREMES
    # ============================================================================

    def _generate_multimodal_posterior_sample(self, sample_id: int, config: Dict) -> Dict:
        """
        Generate sample with multimodal posterior distribution.

        Creates scenarios with parameter degeneracies that lead to multiple
        posterior modes during inference. This tests the model's ability to
        handle ambiguous parameter combinations that produce similar signals.

        Scientific basis:
        - Inclination degeneracy: Œ∏_JN ‚Üî œÄ - Œ∏_JN with distance compensation
        - Spin-inclination degeneracy: aligned vs anti-aligned with Œ∏ flip
        - Mass-distance degeneracy: chirp mass determines SNR scaling

        Args:
            sample_id: Unique sample identifier
            config: Full configuration dict

        Returns:
            Sample dict with parameters that create posterior multimodality
        """

        # Safe config navigation
        edge_cases = config.get("edge_cases", {})
        stat_config = edge_cases.get("statistical_extremes", {})
        types = stat_config.get("types", {})
        multimodal_config = types.get("multimodal_posteriors", {})

        # Configuration
        degeneracy_type = multimodal_config.get("degeneracy_type", "inclination")

        # Generate base sample
        base_sample = self._generate_single_sample(
            sample_id=sample_id, is_edge_case=True, add_glitches=False, preprocess=True
        )

        if base_sample is None:
            self.logger.warning("Failed to generate multimodal posterior sample")
            return None

        # Handle noise samples (no signal to create degeneracy with)
        if base_sample.get("type") == "noise" or not base_sample.get("parameters"):
            base_sample["edge_case_type"] = "multimodal_posteriors"
            existing_priorities = base_sample.get("priorities", [10.0])
            # Normalize raw SNR values to [0, 1] range
            base_sample["priorities"] = [self._normalize_priority_to_01(p) for p in (existing_priorities if isinstance(existing_priorities, list) else [existing_priorities])]
            base_sample["n_signals"] = 0
            self.logger.debug(
                f"Sample {sample_id}: Noise sample, skipping multimodal posterior generation"
            )
            return base_sample

        # Extract parameters - HANDLE LIST FORMAT
        params_list = base_sample.get("parameters", [])

        # Ensure params_list is actually a list
        if not isinstance(params_list, list):
            params_list = [params_list] if params_list else []

        # Skip if no valid parameters
        if not params_list or not params_list[0]:
            base_sample["edge_case_type"] = "multimodal_posteriors"
            existing_priorities = base_sample.get("priorities", [10.0])
            # Normalize raw SNR values to [0, 1] range
            base_sample["priorities"] = [self._normalize_priority_to_01(p) for p in (existing_priorities if isinstance(existing_priorities, list) else [existing_priorities])]
            base_sample["n_signals"] = len(params_list)
            return base_sample

        # Get first (and typically only) param dict from list
        params = params_list[0]

        # Apply specific degeneracy based on type
        if degeneracy_type == "inclination":
            # FIX #3: Inclination degeneracy MUST be re-injected for SNR consistency
            # Current flow: create degenerate params ‚Üí compute new target_snr
            # Problem: New target_snr doesn't guarantee injection matches it
            # Solution: Re-inject signal with degenerate params, validate actual_snr
            
            params["theta_jn"] = np.random.uniform(0.0, 0.3)  # Nearly face-on

            # Create degenerate parameter set
            degenerate_params = params.copy()
            degenerate_params["theta_jn"] = np.pi - params["theta_jn"]  # Face-off

            # Distance scaling (note: amplitude_ratio = 1.0 mathematically, no-op)
            cos_theta_original = np.cos(params["theta_jn"])
            cos_theta_degenerate = np.cos(degenerate_params["theta_jn"])
            amplitude_ratio = (1 + cos_theta_original**2) / (1 + cos_theta_degenerate**2)
            degenerate_params["luminosity_distance"] = (
                params["luminosity_distance"] * amplitude_ratio
            )
            
            # Apply physics-realistic distance caps
            event_type = degenerate_params.get('type', params.get('type', 'BBH'))
            if event_type == 'BNS':
                degenerate_params["luminosity_distance"] = min(degenerate_params["luminosity_distance"], 400.0)
            elif event_type == 'BBH':
                degenerate_params["luminosity_distance"] = min(degenerate_params["luminosity_distance"], 8000.0)
            elif event_type == 'NSBH':
                degenerate_params["luminosity_distance"] = min(degenerate_params["luminosity_distance"], 2500.0)
            
            # FIX #1: DON'T recompute - degenerate inclination doesn't change distance/mass
            # Inclination affects waveform shape, not SNR scaling
            degenerate_params['target_snr'] = self._validate_snr(
                degenerate_params.get('target_snr', self.reference_snr),
                context="inclination_degeneracy"
            )
            
            # FIX #3: CRITICAL - Re-inject degenerate waveform to ensure SNR matches target
            # This is the ONLY way to guarantee actual_snr is consistent with degenerate params
            # Without this, degenerate params have different SNR-distance coupling than primary
            try:
                # Get PSD for this detector (if available)
                psd_dict = self._get_psd(degenerate_params.get('detector', 'H1')) if hasattr(self, '_get_psd') else None
                
                # Re-generate and re-inject degenerate waveform
                degenerate_wf = self.waveform_generator.generate_waveform(degenerate_params, 'H1')
                degenerate_wf = self.injector._resize_signal(degenerate_wf, 16384)  # Standard duration
                
                # Scale to match degenerate target_snr
                target_snr_degen = degenerate_params.get('target_snr', 15.0)
                scaled_degen, actual_snr_degen = self.injector._scale_to_target_snr(
                    degenerate_wf, 
                    np.zeros(16384),  # Placeholder noise for SNR calculation
                    target_snr_degen,
                    psd_dict
                )
                
                # Validate SNR match (FIX #1 check)
                if target_snr_degen > 0:
                    snr_error = abs(actual_snr_degen - target_snr_degen) / target_snr_degen
                    if snr_error > 0.05:
                        raise ValueError(
                            f"Degenerate waveform SNR mismatch: "
                            f"target={target_snr_degen:.2f}, actual={actual_snr_degen:.2f}, "
                            f"error={snr_error*100:.1f}%"
                        )
                
                # Update degenerate params with actual SNR achieved
                degenerate_params['actual_snr'] = float(actual_snr_degen)
                
            except Exception as e:
                raise RuntimeError(
                    f"Failed to re-inject degenerate inclination waveform: {e}"
                ) from e

        elif degeneracy_type == "spin_inclination":
            # Aligned spin with inclination flip
            params["a_1"] = np.random.uniform(0.7, 0.95)  # High spin
            params["tilt_1"] = np.random.uniform(0.0, 0.2)  # Nearly aligned
            params["theta_jn"] = np.random.uniform(0.0, 0.4)

            degenerate_params = params.copy()
            degenerate_params["tilt_1"] = np.pi - params["tilt_1"]  # Anti-aligned
            degenerate_params["theta_jn"] = np.pi - params["theta_jn"]

        elif degeneracy_type == "mass_distance":
            # Chirp mass-distance degeneracy
            # Only set target_snr if sampler hasn't provided one
            if "target_snr" not in params:
                self._set_target_snr(
                    params, np.random.uniform(8, 12), reason="mass_distance_degeneracy"
                )

            degenerate_params = params.copy()
            mass_scale = np.random.uniform(0.9, 1.1)
            degenerate_params["mass_1"] = params["mass_1"] * mass_scale
            degenerate_params["mass_2"] = params["mass_2"] * mass_scale

            # Recompute chirp mass
            M1 = degenerate_params["mass_1"]
            M2 = degenerate_params["mass_2"]
            degenerate_params["chirp_mass"] = (M1 * M2) ** (3 / 5) / (M1 + M2) ** (1 / 5)

            # Adjust distance - compute original chirp mass if not present
            if "chirp_mass" in params:
                original_Mc = params["chirp_mass"]
            else:
                m1, m2 = params["mass_1"], params["mass_2"]
                original_Mc = (m1 * m2) ** (3 / 5) / (m1 + m2) ** (1 / 5)

            new_Mc = degenerate_params["chirp_mass"]
            degenerate_params["luminosity_distance"] = params["luminosity_distance"] * (
                new_Mc / original_Mc
            ) ** (5 / 6)
            
            # CRITICAL FIX (Dec 29, 18:45 UTC): Apply physics-realistic distance caps
            # Chirp mass scaling can exceed detection horizon
            event_type = degenerate_params.get('type', params.get('type', 'BBH'))
            if event_type == 'BNS':
                degenerate_params["luminosity_distance"] = min(degenerate_params["luminosity_distance"], 400.0)
            elif event_type == 'BBH':
                degenerate_params["luminosity_distance"] = min(degenerate_params["luminosity_distance"], 8000.0)
            elif event_type == 'NSBH':
                degenerate_params["luminosity_distance"] = min(degenerate_params["luminosity_distance"], 2500.0)
            
            # FIX #1: Preserve original SNR even for mass-distance degeneracy
            # Both objects have same SNR but different Mc/distance combinations
            degenerate_params['target_snr'] = self._validate_snr(
                degenerate_params.get('target_snr', self.reference_snr),
                context="mass_distance_degeneracy"
            )

        else:
            # Default: simple inclination flip
            degenerate_params = params.copy()
            if "theta_jn" in degenerate_params:
                degenerate_params["theta_jn"] = np.pi - degenerate_params["theta_jn"]

        # IMPORTANT: Update the params_list with modified params
        params_list[0] = params
        base_sample["parameters"] = params_list

        # Attach network_snr to all params (including modified ones)
        attach_network_snr_safe(params_list)

        # Update sample with multimodal metadata
        base_sample["edge_case_type"] = "multimodal_posteriors"

        if "metadata" not in base_sample:
            base_sample["metadata"] = {}

        base_sample["metadata"].update(
            {
                "multimodal": True,
                "degeneracy_type": degeneracy_type,
                "primary_parameters": params,
                "degenerate_parameters": degenerate_params,
                "n_modes": 2,
                "description": f"{degeneracy_type} degeneracy creates bimodal posterior",
            }
        )

        self.logger.debug(
            f"Sample {sample_id}: Multimodal posterior with {degeneracy_type} degeneracy"
        )

        # Ensure priorities exist
        if "priorities" not in base_sample or not base_sample.get("priorities"):
            priorities = []
            parameters = base_sample.get("parameters", [])

            if not isinstance(parameters, list):
                parameters = [parameters]
                base_sample["parameters"] = parameters

            for p in parameters:
                if isinstance(p, dict):
                    priority = self._estimate_snr_from_params(p)
                    # Respect any existing sampled target_snr (don't overwrite)
                    if "target_snr" not in p:
                        self._set_target_snr(p, priority, reason="multimodal_posteriors")
                    priorities.append(priority)

            base_sample["priorities"] = [self._normalize_priority_to_01(p) for p in priorities] if priorities else [self._normalize_priority_to_01(15.0)]
            base_sample["n_signals"] = len(priorities)

        return base_sample

    # ============================================================================
    # 4. OVERLAPPING EXTREMES
    # ============================================================================

    def _generate_partial_overlap_sample(self, sample_id: int, config: Dict) -> Dict:
        """Generate sample with partial temporal overlap (50-200ms)."""

        edge_cases = config.get("edge_cases", {})
        overlap_config = edge_cases.get("overlapping_extremes", {})
        types = overlap_config.get("types", {})
        partial_config = types.get("partial_overlaps", {})

        n_signals = partial_config.get("n_signals", 2)
        overlap_time_ms = partial_config.get("overlap_time_ms", [50, 200])

        time_offset = np.random.uniform(overlap_time_ms[0], overlap_time_ms[1]) / 1000.0

        parameters_list = []
        for i in range(n_signals):
            event_type = self._sample_event_type()
            params = self._sample_parameters(event_type)

            if i > 0:
                params["geocent_time"] = time_offset * i
            else:
                params["geocent_time"] = 0.0

            parameters_list.append(params)

        detector_data = {}

        for detector_name in self.detectors:
            psd_dict = self.psds[detector_name]
            noise, _ = self._get_noise_for_detector(detector_name, psd_dict)
            combined = noise.copy()

            metadata_list = []
            for params in parameters_list:
                signal = self.waveform_generator.generate_waveform(params, detector_name)

                shift_samples = int(params["geocent_time"] * self.sample_rate)
                if shift_samples > 0:
                    signal = np.roll(signal, shift_samples)
                    signal[:shift_samples] = 0

                combined += signal
                metadata_list.append({"time_offset": params["geocent_time"]})

            if self.preprocess_enabled:
                combined = self.preprocessor.preprocess(combined, psd_dict)

            detector_data[detector_name] = {
                "strain": combined.astype(np.float32),
                "noise": noise.astype(np.float32),
                "psd": psd_dict["psd"],
                "frequencies": psd_dict["frequencies"],
                "metadata": metadata_list,
            }

        #  Calculate priorities
        priorities = []
        for params in parameters_list:
            priority = self._estimate_snr_from_params(params)
            # Respect any existing sampled target_snr (don't overwrite)
            if "target_snr" not in params:
                self._set_target_snr(params, priority, reason="partial_overlap")
            priorities.append(priority)

        return {
            "sample_id": f"partial_overlap_{sample_id:06d}",  #  Fixed
            "type": "overlap",
            "is_overlap": True,
            "n_signals": n_signals,  #  Added
            "is_edge_case": True,
            "edge_case_type": "partial_overlap",
            "parameters": parameters_list,  #  Already list
            "priorities": priorities,  #  Added
            "detector_data": detector_data,
            "metadata": {
                "sample_id": sample_id,  #  Fixed
                "overlap_time_ms": time_offset * 1000,
                "n_signals": n_signals,
                "edge_case_type": "partial_overlap",
                "temporal_separation": "partial",
                # FIX #3: Missing SNR in Metadata (overlaps) - Jan 27, 2026
                # For overlaps, include SNR/distance of primary (first) signal
                "target_snr": float(parameters_list[0].get("target_snr", 0.0)) if parameters_list else 0.0,
                "luminosity_distance": float(parameters_list[0].get("luminosity_distance", 0.0)) if parameters_list else 0.0,
                "chirp_mass": float(parameters_list[0].get("chirp_mass", 0.0)) if parameters_list else 0.0,
            },
        }

    # ============================================================================
    # HELPER METHOD
    # ============================================================================

    def _save_batch(self, batch_id: int, samples: List[Dict]):
        """Save batch to disk in specified format"""

        metadata = {
            "batch_id": batch_id,
            "n_samples": len(samples),
            "sample_rate": self.sample_rate,
            "duration": self.duration,
            "detectors": self.detectors,
        }

        # Integrity diagnostic: detect any parameters with clipped/high target_snr
        clipped_entries = []
        try:
            import traceback

            for si, sample in enumerate(samples):
                params_list = sample.get("parameters", [])
                for pi, params in enumerate(params_list):
                    if isinstance(params, dict) and "target_snr" in params:
                        try:
                            val = float(params.get("target_snr", 0.0))
                        except Exception:
                            val = 0.0
                        if val > 200.1:
                            entry = {
                                "sample_index": si,
                                "sample_id": sample.get("sample_id"),
                                "param_index": pi,
                                "target_snr": val,
                            }
                            clipped_entries.append(entry)
                            # Log a short stack for diagnostics (where save was triggered)
                            self.logger.warning(
                                f"[SNR-PRE-SAVE] Detected clipped target_snr={val} in batch {batch_id} "
                                f"sample_index={si} param_index={pi} sample_id={sample.get('sample_id')}"
                            )
                            stack = "".join(traceback.format_stack(limit=6))
                            self.logger.warning(stack)
        except Exception:
            # Ignore diagnostics failures ‚Äî do not block saving
            clipped_entries = clipped_entries

        # Attach simple diagnostic summary to metadata
        if clipped_entries:
            metadata["clipped_target_snr_count"] = len(clipped_entries)
            # Keep a small sample of clipped entries
            metadata["clipped_entries"] = clipped_entries[:50]

        if self.output_format == "hdf5":
            self.writer.save_batch_hdf5(batch_id, samples, metadata)
        elif self.output_format == "pkl":
            self.writer.save_batch_pkl(batch_id, samples, metadata, compress=False)
        elif self.output_format == "pkl_compressed":
            self.writer.save_batch_pkl(batch_id, samples, metadata, compress=True)
        elif self.output_format == "both":
            self.writer.save_batch_hdf5(batch_id, samples, metadata)
            self.writer.save_batch_pkl(batch_id, samples, metadata, compress=True)

    def _estimate_snr_from_params(self, params: Dict) -> float:
        """
        Estimate expected SNR from signal parameters.

        Uses simplified chirp mass scaling:
            SNR ‚àù M_chirp^(5/6) / D

        Where:
            M_chirp = chirp mass in solar masses
            D = luminosity distance in Mpc

        Args:
            params: Signal parameters dict with mass_1, mass_2, luminosity_distance

        Returns:
            Estimated SNR (float)

        Raises:
            ValueError: If required parameters missing or invalid

        Examples:
            >>> params = {'mass_1': 30, 'mass_2': 30, 'luminosity_distance': 400}
            >>> snr = self._estimate_snr_from_params(params)
            >>> print(f"Expected SNR: {snr:.1f}")
            Expected SNR: 15.0
        """
        try:
            # Extract parameters
            m1 = params.get("mass_1")
            m2 = params.get("mass_2")
            distance = params.get("luminosity_distance")

            # Validate required parameters
            if m1 is None or m2 is None:
                raise ValueError("Missing mass_1 or mass_2")
            if distance is None or distance <= 0:
                raise ValueError(f"Invalid luminosity_distance: {distance}")

            # Ensure masses are physical
            if m1 <= 0 or m2 <= 0:
                raise ValueError(f"Non-physical masses: m1={m1}, m2={m2}")

            # Calculate chirp mass
            # M_chirp = (m1 * m2)^(3/5) / (m1 + m2)^(1/5)
            M_total = m1 + m2
            M_chirp = (m1 * m2) ** (3 / 5) / M_total ** (1 / 5)

            #  SNR scaling formula
            # Reference: M_chirp^(5/6) / D_L (physical scaling)
            # CRITICAL FIX (Jan 19, 2026): Use event-type-specific reference parameters
            # Different event types have different characteristic masses and distances
            # Using single reference (1500 Mpc) was inflating SNR for BNS/NSBH
            
            # Determine event type from parameters or mass ratio inference
            event_type = params.get('type', None)
            if event_type is None:
                # Infer event type from mass boundaries
                # BNS: both masses < 2.5 M‚òâ (NS upper mass limit)
                # NSBH: one mass < 2.5, other > 2.5 (extreme mass ratio)
                # BBH: both masses > 2.5 M‚òâ (both likely BH)
                
                if m1 <= 2.5 and m2 <= 2.5:
                    event_type = 'BNS'
                elif (m1 <= 2.5 and m2 > 2.5) or (m2 <= 2.5 and m1 > 2.5):
                    event_type = 'NSBH'
                else:
                    event_type = 'BBH'
            
            # Event-type-specific reference parameters (from ParameterSampler)
            reference_params = {
                'BBH': {'snr': 35.0, 'mass': 30.0, 'distance': 1300.0},
                'BNS': {'snr': 25.0, 'mass': 1.4, 'distance': 130.0},
                'NSBH': {'snr': 20.0, 'mass': 6.0, 'distance': 400.0}
            }
            
            ref = reference_params.get(event_type, reference_params['BBH'])
            reference_snr = ref['snr']
            reference_mass = ref['mass']
            reference_distance = ref['distance']

            snr_estimate = (
                reference_snr
                * (M_chirp / reference_mass) ** (5 / 6)
                * (reference_distance / distance)
            )

            # ‚úÖ PHYSICS-CORRECT SNR BOUNDS (Jan 27, 2026)
            # Allow SNR to vary naturally up to 200 based on physics
            # SNR > 100 is rare but NOT forbidden - it happens for nearby sources
            # Hard-clipping at 100 breaks SNR-distance correlation (r stalls at -0.35)
            # Solution: Allow high SNR, make it rare via sampling distribution
            
            # Minimum SNR: 5 (detection threshold)
            # Maximum SNR: 200 (extremely rare but physics-allowed)
            unclipped = float(snr_estimate)
            clipped = float(np.clip(unclipped, 5.0, 200.0))
            
            # Diagnostic: log when SNR is unusually high (rare event)
            try:
                if unclipped > 100.0:
                    import traceback

                    short_params = {
                        k: params.get(k) for k in ("mass_1", "mass_2", "luminosity_distance")
                    }
                    self.logger.debug(
                        f"[SNR-HIGH] _estimate_snr_from_params: {unclipped:.2f} SNR (rare but allowed); params={short_params}"
                    )
            except Exception:
                pass

            return clipped

        except (TypeError, ValueError, ZeroDivisionError) as e:
            # Log error and return default
            self.logger.debug(f"SNR estimation failed: {e}")
            return 15.0  # Default fallback

    def _normalize_priority_to_01(self, snr_value: float) -> float:
        """
        Normalize SNR value to priority range [0, 1].
        
        Uses exponential mapping to ensure weak signals (SNR < 5) get non-zero priorities.
        - SNR < 5: Maps to priority [0.05, 0.2] (weak but detectable)
        - SNR 5-20: Maps to priority [0.2, 0.5]
        - SNR 20-50: Maps to priority [0.5, 0.8]
        - SNR > 50: Maps to priority [0.8, 1.0] (loud events)
        
        Args:
            snr_value: Raw SNR value (typically 3-200)
            
        Returns:
            Normalized priority in [0, 1]
        """
        # Handle invalid inputs
        try:
            snr_val = float(snr_value)
        except (ValueError, TypeError):
            # Invalid input - return minimum priority
            self.logger.debug(f"Invalid SNR value: {snr_value}, using minimum priority")
            return 0.05  # Minimum non-zero priority
        
        # Use logarithmic scaling to handle wide SNR range
        # Clip raw SNR to reasonable bounds: [1, 1000]
        clipped_snr = float(np.clip(snr_val, 1.0, 1000.0))
        
        # Log-scale: log(1) = 0, log(1000) ‚âà 6.9
        log_snr = np.log(clipped_snr)
        log_snr_min = np.log(1.0)  # 0
        log_snr_max = np.log(1000.0)  # ~6.9
        
        # Normalize to [0, 1]
        normalized = (log_snr - log_snr_min) / (log_snr_max - log_snr_min)
        
        # Apply sigmoid-like correction to expand lower end:
        # Use power < 1 to expand values near 0
        # This ensures SNR=1 (min) maps to ~0.05, not 0.0
        priority = 0.05 + 0.95 * (normalized ** 0.7)
        
        # Double-check the result is in [0, 1]
        result = float(np.clip(priority, 0.05, 1.0))
        
        # Validate result
        if not (0.05 <= result <= 1.0):
            self.logger.warning(
                f"Priority normalization produced out-of-range value: {result}, "
                f"clipping to [0.05, 1.0]"
            )
            result = float(np.clip(result, 0.05, 1.0))
        
        return result

    def _ensure_sample_priorities(self, sample: Dict) -> Dict:
        """
        Ensure that a sample has 'priorities' and 'n_signals' populated.

        This normalizes parameters to a list, reads existing 'target_snr' from params
        (or computes if missing), and sets sample['priorities'] and sample['n_signals'].

        IMPORTANT: Does NOT overwrite existing target_snr values!
        """
        if sample is None:
            return sample

        # Normalize parameters to a list so n_signals reflects real number of signals
        parameters = sample.get("parameters", [])
        if parameters is None:
            parameters = []
        if not isinstance(parameters, list):
            parameters = [parameters]

        # Ensure sample stores normalized parameters
        sample["parameters"] = parameters

        if "priorities" not in sample or not sample.get("priorities"):
            priorities = []

            for params in parameters:
                if not isinstance(params, dict):
                    continue

                try:
                    #  USE EXISTING target_snr if available (don't recompute!)
                    priority = params.get("target_snr")

                    if priority is None:
                        # Only compute if target_snr doesn't exist
                        priority = float(self._estimate_snr_from_params(params))
                        if "target_snr" not in params:
                            self._set_target_snr(
                                params, priority, reason="_ensure_sample_priorities_compute"
                            )  # Set it ONLY if missing
                    else:
                        # Already exists, use it as-is
                        priority = float(priority)

                except Exception as e:
                    self.logger.debug(f"Priority computation failed: {e}")
                    priority = params.get("target_snr", 15.0)
                    if "target_snr" not in params:
                        self._set_target_snr(
                            params, priority, reason="_ensure_sample_priorities_exception"
                        )

                # Ensure priority is a valid number (not NaN or Inf)
                if not np.isfinite(priority):
                    self.logger.warning(f"Non-finite priority detected: {priority}, using fallback SNR=15.0")
                    priority = 15.0
                
                priorities.append(priority)

            # Normalize all priorities to [0, 1] range
            normalized_priorities = [self._normalize_priority_to_01(p) for p in priorities] if priorities else [self._normalize_priority_to_01(15.0)]
            
            # Validate all normalized priorities are in [0, 1]
            for i, p in enumerate(normalized_priorities):
                if not (0.0 <= p <= 1.0):
                    self.logger.warning(f"Out-of-range priority at index {i}: {p}, clipping to [0, 1]")
                    normalized_priorities[i] = float(np.clip(p, 0.0, 1.0))
            
            sample["priorities"] = normalized_priorities

        # Derive n_signals from the normalized parameters list (true signal count)
        sample["n_signals"] = len(parameters) if parameters else 1

        return sample

    def _set_target_snr(self, params: Dict, value, reason: str = None):
        """Helper to assign target_snr and log a stack trace when a clipped/very large value is set.

        This centralizes assignments so we can diagnose where clipped 80.0 values originate.
        """
        try:
            val = float(value)
        except Exception:
            val = value

        # Assign into params dict
        if isinstance(params, dict):
            params["target_snr"] = val

        # If value is effectively clipped/high, log stack for diagnostics
        try:
            if isinstance(val, (int, float)) and val >= 200.0:
                import traceback

                self.logger.warning(
                    f"[SNR-TRACE] Assigned clipped target_snr={val} reason={reason}"
                )
                stack = "".join(traceback.format_stack(limit=6))
                self.logger.warning(stack)
        except Exception:
            # Never fail generation due to diagnostics
            pass

        return val

    def _generate_single_sample_multi_noise(
        self,
        sample_id: int,
        is_edge_case: bool,
        add_glitches: bool,
        preprocess: bool,
        k: int = 1,
        snr_regime: Optional[str] = None,
        forced_event_type: Optional[str] = None,
    ) -> List[Dict]:
        """
        Generate K samples with SAME parameters but DIFFERENT noise realizations.
        
        This fixes amplitude-distance entanglement by showing the network that the same
        physical parameters can appear with different amplitudes (due to different noise).
        
        Args:
            k (int): Number of noise realizations to generate (default: 1 = no STEP 4)
            Other args: Same as _generate_single_sample
            
        Returns:
            List[Dict]: K samples with identical parameters but different noise/strain
                       Each sample has sample_id_noise_{i} identifier
        
        ‚úÖ STEP 4 Benefits:
           - Network learns: amplitude_variation ‚ä• distance_variation
           - Eliminates: amplitude‚Üídistance shortcuts
           - Result: ¬±60 Mpc bias ‚Üí ¬±20 Mpc bias
        """
        if k <= 1:
            # No STEP 4, return single sample wrapped in list
            return [self._generate_single_sample(
                sample_id, is_edge_case, add_glitches, preprocess, snr_regime, forced_event_type
            )]
        
        # ‚úÖ STEP 4: Generate parameters ONCE
        if snr_regime is None:
            snr_regime = self._sample_snr_regime()
        
        if forced_event_type is not None:
            event_type = forced_event_type
        else:
            try:
                event_type = self.parameter_sampler.event_type_given_snr(snr_regime)
            except Exception:
                event_type = self._sample_event_type()
        
        # Generate parameters ONCE (same for all K variants)
        if event_type == "BBH":
            params = self.parameter_sampler.sample_bbh_parameters(snr_regime, is_edge_case)
        elif event_type == "BNS":
            params = self.parameter_sampler.sample_bns_parameters(snr_regime, is_edge_case)
        elif event_type == "NSBH":
            params = self.parameter_sampler.sample_nsbh_parameters(snr_regime, is_edge_case)
        else:
            params = None
        
        samples_list = []
        
        # Generate K variants with DIFFERENT noise
        for noise_idx in range(k):
            detector_data = {}
            noise_types = {}
            
            for detector_name in self.detectors:
                psd_dict = self.psds[detector_name]
                
                # üîÑ KEY: Different noise for each variant (new random realization)
                noise, noise_type = self._get_noise_for_detector(detector_name, psd_dict)
                noise_types[detector_name] = noise_type
                
                # Add glitches
                if add_glitches:
                    noise = self.noise_generator.add_glitches(noise, glitch_prob=0.3)
                
                # Inject SAME signal into DIFFERENT noise
                if params:
                    injected, metadata = self.injector.inject_signal(
                        noise, params, detector_name, psd_dict
                    )
                else:
                    injected = noise
                    metadata = {"detector": detector_name, "noise_only": True}
                
                # ‚úÖ PHYSICS CORRECTION: Enforce SNR-distance coupling AFTER noise injection
                # Issue: Clipping breaks correlation between achieved SNR and distance
                # Solution: Recompute distance from achieved SNR to maintain physics
                # Physics rule: distance √ó SNR = constant (up to noise variation)
                if params and metadata.get("actual_snr", 0) > 0:
                    target_snr = params.get("target_snr", 0)
                    achieved_snr = metadata.get("actual_snr", 0)
                    
                    if target_snr > 0 and achieved_snr > 0:
                        # Compute corrected distance based on achieved SNR
                        original_distance = params.get("luminosity_distance", 0)
                        corrected_distance = original_distance * (target_snr / achieved_snr)
                        
                        # Clip to valid range once at the END
                        corrected_distance = np.clip(
                            corrected_distance,
                            10.0,
                            5000.0
                        )
                        
                        # Update both params and metadata to reflect actual physics
                        params["luminosity_distance"] = float(corrected_distance)
                        metadata["corrected_distance"] = float(corrected_distance)
                        metadata["distance_correction_factor"] = float(target_snr / achieved_snr)
                
                # Preprocess
                if preprocess:
                    injected = self.preprocessor.preprocess(injected, psd_dict)
                
                # Validation
                strain_final = injected.astype(np.float32)
                noise_final = noise.astype(np.float32)
                
                if np.any(~np.isfinite(noise_final)):
                    self.logger.warning(
                        f"NaN/Inf in noise [{detector_name}, noise_idx={noise_idx}], using fallback"
                    )
                    noise_final = np.random.randn(len(noise_final)).astype(np.float32) * 1e-20
                
                if np.any(~np.isfinite(strain_final)):
                    self.logger.warning(
                        f"NaN/Inf in strain [{detector_name}, noise_idx={noise_idx}], using noise"
                    )
                    strain_final = noise_final.copy()
                
                detector_data[detector_name] = {
                    "strain": strain_final,
                    "noise": noise_final,
                    "psd": psd_dict.get("psd"),
                    "frequencies": psd_dict.get("frequencies"),
                    "metadata": metadata,
                }
            
            # Create sample with unique ID for this noise variant
            sample = {
                "sample_id": f"{sample_id:06d}_noise{noise_idx}",
                "type": event_type,
                "event_type": event_type,
                "parameters": params if params else {},
                "detector_data": detector_data,
                "metadata": {
                    "sample_id": sample_id,
                    "noise_variant": noise_idx,
                    "n_noise_variants": k,
                    "event_type": event_type,
                    "snr_regime": snr_regime,
                    "is_edge_case": is_edge_case,
                    "noise_sources": noise_types,
                    # FIX #3: Missing SNR in Metadata - Issue #3 FIXED (Jan 27, 2026)
                    # CRITICAL: Added actual SNR values to metadata for training diagnostics
                    # Problem: Only regime name stored, not actual numeric SNR values
                    # Solution: Include target_snr and luminosity_distance for SNR-distance analysis
                    "target_snr": float(params.get("target_snr", 0.0)) if params else 0.0,
                    "luminosity_distance": float(params.get("luminosity_distance", 0.0)) if params else 0.0,
                    "chirp_mass": float(params.get("chirp_mass", 0.0)) if params else 0.0,
                },
            }
            
            if params:
                raw_snr = self._estimate_snr_from_params(params)
                sample["priorities"] = [self._normalize_priority_to_01(raw_snr)]
            
            samples_list.append(sample)
        
        return samples_list

    def _generate_overlapping_sample_multi_noise(
        self,
        sample_id: int,
        is_edge_case: bool,
        add_glitches: bool,
        preprocess: bool,
        k: int = 1,
        forced_signals: Optional[List[Dict]] = None,
    ) -> List[Dict]:
        """
        Generate K overlapping samples with SAME parameters but DIFFERENT noise realizations.
        
        Args:
            k (int): Number of noise realizations (default: 1 = no STEP 4)
            Other args: Same as _generate_overlapping_sample
            
        Returns:
            List[Dict]: K samples with identical signal parameters but different noise
        """
        if k <= 1:
            # No STEP 4, return single sample wrapped in list
            return [self._generate_overlapping_sample(
                sample_id, is_edge_case, add_glitches, preprocess, forced_signals
            )]
        
        # ‚úÖ STEP 4: Generate parameters ONCE
        n_signals = sample_overlap_size(max_overall=self.max_overlapping_signals)
        signal_params_list = []
        
        # Generate clustered geocent_time for overlapping signals
        if n_signals > 1:
            clustered_times = sample_clustered_times(
                np.random, n_signals, self.duration, overlap_window=0.6
            )
        else:
            clustered_times = [0.0]
        
        # Generate parameters for each signal (ONCE for all K variants)
        for i in range(n_signals):
            if forced_signals and i < len(forced_signals):
                info = forced_signals[i]
                snr_regime = info.get("snr_regime")
                event_type = info.get("event_type")
            else:
                snr_regime = self.parameter_sampler._sample_snr_regime()
                try:
                    event_type = self.parameter_sampler.event_type_given_snr(snr_regime)
                except Exception:
                    event_type = self._sample_event_type()
            
            if i > 0 and n_signals > 1 and event_type == signal_params_list[0]["type"]:
                first_type = signal_params_list[0]["type"]
                available_types = [t for t in ["BBH", "BNS", "NSBH"] if t != first_type]
                if available_types:
                    weights = [EVENT_TYPE_DISTRIBUTION.get(t, 0.0) for t in available_types]
                    total_weight = sum(weights)
                    if total_weight > 0:
                        probs = [w / total_weight for w in weights]
                        event_type = np.random.choice(available_types, p=probs)
                    else:
                        event_type = np.random.choice(available_types)
            
            if event_type == "BBH":
                params = self.parameter_sampler.sample_bbh_parameters(snr_regime, is_edge_case)
            elif event_type == "BNS":
                params = self.parameter_sampler.sample_bns_parameters(snr_regime, is_edge_case)
            else:
                params = self.parameter_sampler.sample_nsbh_parameters(snr_regime, is_edge_case)
            
            params["geocent_time"] = clustered_times[i]
            signal_params_list.append(params)
        
        attach_network_snr_safe(signal_params_list)
        
        samples_list = []
        
        # Generate K variants with DIFFERENT noise
        for noise_idx in range(k):
            detector_data = {}
            noise_types = {}
            
            for detector_name in self.detectors:
                psd_dict = self.psds[detector_name]
                
                # üîÑ KEY: Different noise for each variant (new random realization)
                noise, noise_type = self._get_noise_for_detector(detector_name, psd_dict)
                noise_types[detector_name] = noise_type
                
                if add_glitches:
                    noise = self.noise_generator.add_glitches(noise, glitch_prob=0.2)
                
                # Inject SAME signals into DIFFERENT noise
                injected, metadata_list = self.injector.inject_overlapping_signals(
                    noise, signal_params_list, detector_name, psd_dict
                )
                
                # ‚úÖ PHYSICS CORRECTION: Enforce SNR-distance coupling for each overlapping signal
                # Issue: Clipping breaks correlation between achieved SNR and distance
                # Solution: Recompute distance from achieved SNR to maintain physics
                # Physics rule: distance √ó SNR = constant (up to noise variation)
                for i, (metadata, params) in enumerate(zip(metadata_list, signal_params_list)):
                    if metadata.get("actual_snr", 0) > 0:
                        target_snr = params.get("target_snr", 0)
                        achieved_snr = metadata.get("actual_snr", 0)
                        
                        if target_snr > 0 and achieved_snr > 0:
                            # Compute corrected distance based on achieved SNR
                            original_distance = params.get("luminosity_distance", 0)
                            corrected_distance = original_distance * (target_snr / achieved_snr)
                            
                            # Clip to valid range once at the END
                            corrected_distance = np.clip(
                                corrected_distance,
                                10.0,
                                5000.0
                            )
                            
                            # Update both params and metadata to reflect actual physics
                            params["luminosity_distance"] = float(corrected_distance)
                            metadata["corrected_distance"] = float(corrected_distance)
                            metadata["distance_correction_factor"] = float(target_snr / achieved_snr)
                
                if preprocess:
                    injected = self.preprocessor.preprocess(injected, psd_dict)
                
                strain_final = injected.astype(np.float32)
                noise_final = noise.astype(np.float32)
                
                if np.any(~np.isfinite(noise_final)):
                    self.logger.warning(
                        f"NaN/Inf in overlap noise [{detector_name}, noise_idx={noise_idx}], using fallback"
                    )
                    noise_final = np.random.randn(len(noise_final)).astype(np.float32) * 1e-20
                
                if np.any(~np.isfinite(strain_final)):
                    self.logger.warning(
                        f"NaN/Inf in overlap strain [{detector_name}, noise_idx={noise_idx}], using noise"
                    )
                    strain_final = noise_final.copy()
                
                detector_data[detector_name] = {
                    "strain": strain_final,
                    "noise": noise_final,
                    "psd": psd_dict.get("psd"),
                    "frequencies": psd_dict.get("frequencies"),
                    "metadata": metadata_list if metadata_list else [{}],
                }
            
            # Create sample with unique ID for this noise variant
            sample = {
                "sample_id": f"{sample_id:06d}_overlap_noise{noise_idx}",
                "type": signal_params_list[0]["type"] if signal_params_list else "unknown",
                "event_type": signal_params_list[0]["type"] if signal_params_list else "unknown",
                "parameters": signal_params_list,
                "detector_data": detector_data,
                "metadata": {
                    "sample_id": sample_id,
                    "noise_variant": noise_idx,
                    "n_noise_variants": k,
                    "n_signals": len(signal_params_list),
                    "is_edge_case": is_edge_case,
                    "noise_sources": noise_types,
                    "overlapping": True,
                },
            }
            
            samples_list.append(sample)
        
        return samples_list

    def _generate_single_sample(
        self,
        sample_id: int,
        is_edge_case: bool,
        add_glitches: bool,
        preprocess: bool,
        snr_regime: Optional[str] = None,
        forced_event_type: Optional[str] = None,
    ) -> Dict:
        """Generate single non-overlapping sample

        Optional arguments allow callers (quota mode) to force the SNR regime
        and/or the event type. If not provided, the method falls back to the
        usual sampler-driven behavior.
        """
        # Prefer caller-provided regime/type when present (quota-mode integration)
        if snr_regime is None:
            snr_regime = self._sample_snr_regime()

        if forced_event_type is not None:
            event_type = forced_event_type
        else:
            try:
                event_type = self.parameter_sampler.event_type_given_snr(snr_regime)
            except Exception:
                # Fallback to prior sampling if the sampler doesn't provide inversion
                event_type = self._sample_event_type()

        # Generate parameters
        if event_type == "BBH":
            params = self.parameter_sampler.sample_bbh_parameters(snr_regime, is_edge_case)
        elif event_type == "BNS":
            params = self.parameter_sampler.sample_bns_parameters(snr_regime, is_edge_case)
        elif event_type == "NSBH":
            params = self.parameter_sampler.sample_nsbh_parameters(snr_regime, is_edge_case)
        else:
            params = None

        # Generate data for each detector
        detector_data = {}
        noise_types = {}
        for detector_name in self.detectors:
            psd_dict = self.psds[detector_name]

            # Generate noise
            noise, noise_type = self._get_noise_for_detector(detector_name, psd_dict)
            noise_types[detector_name] = noise_type

            # Add glitches
            if add_glitches:
                noise = self.noise_generator.add_glitches(noise, glitch_prob=0.3)

            # Inject signal
            if params:
                injected, metadata = self.injector.inject_signal(
                    noise, params, detector_name, psd_dict
                )
            else:
                injected = noise
                metadata = {"detector": detector_name, "noise_only": True}

            # ‚úÖ PHYSICS CORRECTION: Enforce SNR-distance coupling AFTER noise injection
            # Issue: Clipping breaks correlation between achieved SNR and distance
            # Solution: Recompute distance from achieved SNR to maintain physics
            # Physics rule: distance √ó SNR = constant (up to noise variation)
            if params and metadata.get("actual_snr", 0) > 0:
                target_snr = params.get("target_snr", 0)
                achieved_snr = metadata.get("actual_snr", 0)
                
                if target_snr > 0 and achieved_snr > 0:
                    # Compute corrected distance based on achieved SNR
                    original_distance = params.get("luminosity_distance", 0)
                    corrected_distance = original_distance * (target_snr / achieved_snr)
                    
                    # Clip to valid range once at the END
                    corrected_distance = np.clip(
                        corrected_distance,
                        10.0,
                        5000.0
                    )
                    
                    # Update both params and metadata to reflect actual physics
                    params["luminosity_distance"] = float(corrected_distance)
                    metadata["corrected_distance"] = float(corrected_distance)
                    metadata["distance_correction_factor"] = float(target_snr / achieved_snr)

            # Preprocess
            if preprocess:
                injected = self.preprocessor.preprocess(injected, psd_dict)

            # Final validation: ensure no NaN/Inf in strain or noise
            strain_final = injected.astype(np.float32)
            noise_final = noise.astype(np.float32)
            
            # Check noise first - if noise has NaN, generate fallback BEFORE using it as strain fallback
            if np.any(~np.isfinite(noise_final)):
                self.logger.warning(
                    f"NaN/Inf in noise for {detector_name}, using Gaussian fallback"
                )
                noise_final = np.random.randn(len(noise_final)).astype(np.float32) * 1e-20
            
            # If strain has NaN, replace with noise (now guaranteed to be valid)
            if np.any(~np.isfinite(strain_final)):
                self.logger.warning(
                    f"NaN/Inf in final strain for {detector_name}, using noise-only"
                )
                strain_final = np.copy(noise_final)

            detector_data[detector_name] = {
                "strain": strain_final,
                "noise": noise_final,
                "metadata": metadata,
                "noise_type": noise_type,
            }

        attach_network_snr_safe(params)

        # Create sample with comprehensive metadata (like original script)
        sample = {
            "edge_type_id": encode_edge_type([params] if params is not None else []),
            "sample_id": f"single_{sample_id:06d}",
            "type": event_type,
            "is_overlap": False,
            "is_edge_case": is_edge_case,
            "parameters": [params],
            "detector_data": detector_data,
            "noise_type": noise_types,  # Track which detectors used real vs synthetic noise
            "metadata": {  # ‚Üê ADD THIS BLOCK
                "sample_id": f"single_{sample_id:06d}",
                "event_type": event_type,
                "detector_network": self.detectors,
                "snr_regime": snr_regime,
                "signal_parameters": [params] if params else [],  # ‚Üê KEY FIX
                "is_edge_case": is_edge_case,
                "overlap_type": "single",
            },
        }

        return sample

    def _generate_overlapping_sample(
        self,
        sample_id: int,
        is_edge_case: bool,
        add_glitches: bool,
        preprocess: bool,
        forced_signals: Optional[List[Dict]] = None,
    ) -> Dict:
        """Generate sample with overlapping signals"""

        n_signals = sample_overlap_size(max_overall=self.max_overlapping_signals)

        signal_params_list = []
        target_snrs = []

        # Generate clustered geocent_time for overlapping signals
        if n_signals > 1:
            clustered_times = sample_clustered_times(
                np.random, n_signals, self.duration, overlap_window=0.6
            )
        else:
            clustered_times = [0.0]

        # Generate parameters for each signal
        for i in range(n_signals):
            # If caller provided forced_signals info, use it for this index
            if forced_signals and i < len(forced_signals):
                info = forced_signals[i]
                snr_regime = info.get("snr_regime")
                event_type = info.get("event_type")
            else:
                # Default behavior: sample regime from configured distribution (not quota-forced)
                # This path is taken when forced_signals is not provided (non-quota overlaps)
                snr_regime = self.parameter_sampler._sample_snr_regime()
                try:
                    event_type = self.parameter_sampler.event_type_given_snr(snr_regime)
                except Exception:
                    event_type = self._sample_event_type()

            # Force event type diversity in overlaps: if not the first signal, avoid the first signal's type
            if i > 0 and n_signals > 1 and event_type == signal_params_list[0]["type"]:
                first_type = signal_params_list[0]["type"]
                available_types = [t for t in ["BBH", "BNS", "NSBH"] if t != first_type]
                if available_types:
                    # Resample from available types weighted by global distribution
                    weights = [EVENT_TYPE_DISTRIBUTION.get(t, 0.0) for t in available_types]
                    total_weight = sum(weights)
                    if total_weight > 0:
                        probs = [w / total_weight for w in weights]
                        event_type = np.random.choice(available_types, p=probs)
                    else:
                        event_type = np.random.choice(available_types)

            if event_type == "BBH":
                params = self.parameter_sampler.sample_bbh_parameters(snr_regime, is_edge_case)
            elif event_type == "BNS":
                params = self.parameter_sampler.sample_bns_parameters(snr_regime, is_edge_case)
            else:
                params = self.parameter_sampler.sample_nsbh_parameters(snr_regime, is_edge_case)

            # Use clustered time instead of random offset
            params["geocent_time"] = clustered_times[i]
            signal_params_list.append(params)

            # Compute SNR using the first detector's PSD (outside detector loop)
            waveform = self.waveform_generator.generate_waveform(params)
            reference_psd = self.psds[self.detectors[0]]  # Use first detector as reference
            target_snr = self.injector._compute_optimal_snr(waveform, reference_psd)
            target_snrs.append(float(target_snr))

        attach_network_snr_safe(signal_params_list)

        # Generate data for each detector (NOW psd_dict is defined here)
        detector_data = {}
        noise_types = {}
        for detector_name in self.detectors:
            psd_dict = self.psds[detector_name]

            noise, noise_type = self._get_noise_for_detector(detector_name, psd_dict)
            noise_types[detector_name] = noise_type

            if add_glitches:
                noise = self.noise_generator.add_glitches(noise, glitch_prob=0.2)

            injected, metadata_list = self.injector.inject_overlapping_signals(
                noise, signal_params_list, detector_name, psd_dict
            )

            # ‚úÖ PHYSICS CORRECTION: Enforce SNR-distance coupling for each overlapping signal
            # Issue: Clipping breaks correlation between achieved SNR and distance
            # Solution: Recompute distance from achieved SNR to maintain physics
            # Physics rule: distance √ó SNR = constant (up to noise variation)
            for i, (metadata, params) in enumerate(zip(metadata_list, signal_params_list)):
                if metadata.get("actual_snr", 0) > 0:
                    target_snr = params.get("target_snr", 0)
                    achieved_snr = metadata.get("actual_snr", 0)
                    
                    if target_snr > 0 and achieved_snr > 0:
                        # Compute corrected distance based on achieved SNR
                        original_distance = params.get("luminosity_distance", 0)
                        corrected_distance = original_distance * (target_snr / achieved_snr)
                        
                        # Clip to valid range once at the END
                        corrected_distance = np.clip(
                            corrected_distance,
                            10.0,
                            5000.0
                        )
                        
                        # Update both params and metadata to reflect actual physics
                        params["luminosity_distance"] = float(corrected_distance)
                        metadata["corrected_distance"] = float(corrected_distance)
                        metadata["distance_correction_factor"] = float(target_snr / achieved_snr)

            if preprocess:
                injected = self.preprocessor.preprocess(injected, psd_dict)

            # Final validation: ensure no NaN/Inf in strain or noise
            strain_final = injected.astype(np.float32)
            noise_final = noise.astype(np.float32)
            
            # Check noise first - if noise has NaN, generate fallback BEFORE using it as strain fallback
            if np.any(~np.isfinite(noise_final)):
                self.logger.warning(
                    f"NaN/Inf in noise for {detector_name}, using Gaussian fallback"
                )
                noise_final = np.random.randn(len(noise_final)).astype(np.float32) * 1e-20
            
            # If strain has NaN, replace with noise (now guaranteed to be valid)
            if np.any(~np.isfinite(strain_final)):
                self.logger.warning(
                    f"NaN/Inf in final strain for {detector_name}, using noise-only"
                )
                strain_final = np.copy(noise_final)
            
            detector_data[detector_name] = {
                "strain": strain_final,
                "noise": noise_final,
                "metadata": metadata_list,
                "noise_type": noise_type,
            }
            
            # ‚úÖ FIX (Nov 11, 2025): Don't inject decoys into training data
        # Decoys cause n_signals ‚â† len(parameters) mismatch (12.9% of samples affected)
        # Decoys should only be used for evaluation/validation, not training
        # signal_params_list = maybe_inject_decoy(signal_params_list, p=0.30)

        # Compute priorities BEFORE decoy injection (so len(priorities) == n_signals)
        priorities = []
        for params in signal_params_list:
            # Estimate raw SNR from parameters
            raw_snr = self._estimate_snr_from_params(params)
            # Normalize to [0, 1] using log-scale to avoid near-zero values
            priority = self._normalize_priority_to_01(raw_snr)
            priorities.append(priority)

        sample = {
            "edge_type_id": encode_edge_type(signal_params_list),  # Multiple signals
            "sample_id": f"overlap_{sample_id:06d}",
            "type": "overlap",
            "is_overlap": True,
            "n_signals": n_signals,
            "is_edge_case": is_edge_case,
            "parameters": signal_params_list,
            "priorities": priorities,
            "detector_data": detector_data,
            "noise_type": noise_types,  # Track which detectors used real vs synthetic noise
            "metadata": {
                "sample_id": f"overlap_{sample_id:06d}",
                "event_type": "overlap",
                "detector_network": self.detectors,
                "n_signals": n_signals,
                "signal_parameters": signal_params_list,
                "is_edge_case": is_edge_case,
                "overlap_type": "multi_signal",
            },
        }

        return sample

    def _sample_event_type(self) -> str:
        """Sample event type from distribution"""
        types = list(EVENT_TYPE_DISTRIBUTION.keys())
        probs = list(EVENT_TYPE_DISTRIBUTION.values())
        return np.random.choice(types, p=probs)

    def _sample_event_type_subset(self, subset: List[str]) -> str:
        """Sample an event type restricted to a given subset but weighted by the global distribution.

        If the subset is empty or None, falls back to the global distribution.
        """
        if not subset:
            return self._sample_event_type()
        # Build weighted probabilities from global distribution for the subset
        global_dist = EVENT_TYPE_DISTRIBUTION
        filtered = {k: global_dist.get(k, 0.0) for k in subset}
        total = sum(filtered.values())
        if total <= 0:
            # Fall back to uniform over subset
            return np.random.choice(subset)
        probs = [filtered[k] / total for k in subset]
        return np.random.choice(subset, p=probs)

    def _sample_snr_regime(self) -> str:
        """Sample SNR regime from distribution"""
        regimes = list(SNR_DISTRIBUTION.keys())
        probs = list(SNR_DISTRIBUTION.values())
        return np.random.choice(regimes, p=probs)

    def _calculate_type_counts(self):
        """Calculate how many samples of each extreme type to generate."""
        self.type_counts = {}

        if not self.enabled:
            return

        for type_name, type_config in self.extreme_types_config.items():
            if type_config.get("enabled", True):
                fraction = type_config.get("fraction", 0.1)
                self.type_counts[type_name] = fraction

    def _validate_snr(self, snr: float, context: str = "") -> float:
        """
        Validate SNR is finite and positive, with logging.
        
        Edge case samples can produce invalid SNR values (NaN, Inf, negative).
        This method catches and replaces them with reference_snr.
        
        FIX #6: Missing SNR Validation
        """
        if snr is None:
            self.logger.warning(
                f"SNR is None in {context}, replacing with reference_snr={self.reference_snr}"
            )
            return self.reference_snr
        
        if not np.isfinite(snr):
            self.logger.warning(
                f"Invalid SNR (NaN/Inf) in {context}: {snr}, replacing with reference_snr={self.reference_snr}"
            )
            return self.reference_snr
        
        if snr <= 0:
            self.logger.warning(
                f"Negative/zero SNR in {context}: {snr}, replacing with reference_snr={self.reference_snr}"
            )
            return self.reference_snr
        
        if snr < 8.0:
            self.logger.warning(f"SNR below minimum in {context}: {snr:.2f}, clamping to 8.0")
            return 8.0
        
        if snr > 200.0:
            self.logger.warning(f"SNR above maximum in {context}: {snr:.2f}, clamping to 200.0")
            return 200.0
        
        return float(snr)

    def _sample_parameters(self, event_type: str) -> Dict:
        """Sample parameters for given event type."""
        snr_regime = self._sample_snr_regime()

        if event_type == "BBH":
            return self.parameter_sampler.sample_bbh_parameters(snr_regime, False)
        elif event_type == "BNS":
            return self.parameter_sampler.sample_bns_parameters(snr_regime, False)
        elif event_type == "NSBH":
            return self.parameter_sampler.sample_nsbh_parameters(snr_regime, False)
        else:
            return self.parameter_sampler.sample_bbh_parameters(snr_regime, False)

    def _should_generate_extreme_case(self) -> Tuple[bool, Optional[str]]:
        """Determine if current sample should be an extreme case."""
        if not self.extreme_enabled:
            return False, None

        if np.random.random() > self.extreme_fraction:
            return False, None

        enabled_types = {
            name: config
            for name, config in self.extreme_types_config.items()
            if config.get("enabled", True)
        }

        if not enabled_types:
            return False, None

        types = list(enabled_types.keys())
        fractions = [enabled_types[t].get("fraction", 0.1) for t in types]

        total = sum(fractions)
        if total == 0:
            return False, None

        probs = [f / total for f in fractions]
        extreme_type = np.random.choice(types, p=probs)

        return True, extreme_type

    # ========================================================================
    # 1Ô∏è‚É£ NEAR-SIMULTANEOUS MERGERS (Œît < 0.2s)
    # ========================================================================

    def _generate_near_simultaneous_mergers(self, sample_id: int) -> Dict:
        """
        Generate overlapping signals with merger times within 0.2 seconds.
        Critical for training PriorityNet to disentangle closely-spaced events.
        """
        config = self.extreme_types_config.get("near_simultaneous_mergers", {})

        delta_t_range = config.get("delta_t_range", [0.05, 0.2])
        min_overlap = config.get("min_overlap", 2)
        max_overlap = config.get("max_overlap", 3)
        event_pairs = config.get("event_type_pairs", [["BBH", "BBH"]])

        # Determine number of overlapping signals
        n_signals = np.random.randint(min_overlap, max_overlap + 1)

        # Generate base parameters for each signal
        parameters_list = []

        # Reference merger time
        t_ref = 1187008882.0  # GW150914-like GPS time

        for i in range(n_signals):
            # Select event type pair
            event_type = np.random.choice([pair[i % len(pair)] for pair in event_pairs])

            # Generate standard parameters
            params = self._sample_parameters(event_type)

            # Set merger time with small separation
            if i == 0:
                params["geocent_time"] = t_ref
            else:
                # Add small time offset
                delta_t = np.random.uniform(delta_t_range[0], delta_t_range[1])
                params["geocent_time"] = t_ref + delta_t * (1 if np.random.random() > 0.5 else -1)

            # Ensure SNRs are reasonable (not too weak)
            target_snr = np.random.uniform(10, 30)
            if "target_snr" not in params:
                self._set_target_snr(params, target_snr, reason="near_simultaneous_target")

            # Attach network_snr to params
            attach_network_snr_safe(params)

            parameters_list.append(params)

        # Generate overlapping sample
        sample = self.generate_custom_overlap(parameters_list, sample_id, is_edge_case=False)

        # Mark as extreme case
        sample["is_extreme_case"] = True
        sample["extreme_case_type"] = "extreme_near_simultaneous_mergers"
        sample["extreme_metadata"] = {
            "delta_t_actual": max([p["geocent_time"] for p in parameters_list])
            - min([p["geocent_time"] for p in parameters_list]),
            "n_signals": n_signals,
        }

        return sample

    # ========================================================================
    # 2Ô∏è‚É£ EXTREME MASS-RATIO INSPIRALS (q < 0.05)
    # ========================================================================

    def _generate_extreme_mass_ratio(self, sample_id: int) -> Dict:
        """
        Generate extreme mass-ratio binaries (q < 0.1, M_total < 30 M_‚òâ).

        Scientific Motivation:
            Tests waveform models and parameter estimation for asymmetric systems.
            Challenges spin-orbit coupling models and higher-order PN corrections.

        Physical Scenario:
            Stellar-mass BH + stellar-mass compact object (e.g., GW190814 with
            q ~ 0.112 [Abbott et al., ApJL 2020]), or potential IMBH-stellar systems.

        Astrophysical Context:
            Expected from globular cluster dynamics and hierarchical mergers.
            Formation channels: dynamical capture, primordial binaries.

        Parameter Ranges:
            - Mass ratio: q ‚àà [0.01, 0.1]
            - Total mass: M_total ‚àà [10, 30] M_‚òâ
            - Primary mass: m‚ÇÅ ‚àà [9, 27] M_‚òâ
            - Secondary mass: m‚ÇÇ = q √ó m‚ÇÅ

        Returns:
            Sample with extreme q, enhanced higher harmonics
        """

        config = self.extreme_types_config.get("extreme_mass_ratio", {})
        q_range = config.get("q_range", [0.01, 0.1])
        total_mass_range = config.get("total_mass_range", [10.0, 30.0])

        # Sample extreme mass ratio
        q = np.random.uniform(q_range[0], q_range[1])
        m_total = np.random.uniform(total_mass_range[0], total_mass_range[1])

        # Compute masses
        m1 = m_total / (1 + q)
        m2 = m1 * q

        # Derived quantities
        chirp_mass = (m1 * m2) ** (3 / 5) / (m1 + m2) ** (1 / 5)
        eta = (m1 * m2) / (m1 + m2) ** 2

        # Sample other parameters
        params = self._sample_parameters("BBH")

        # Override mass parameters
        params.update(
            {
                "mass_1": m1,
                "mass_2": m2,
                "chirp_mass": chirp_mass,
                "symmetric_mass_ratio": eta,
                "mass_ratio": q,
                "total_mass": m_total,
                "type": "BBH",
            }
        )

        # Generate sample
        sample = self._generate_sample_from_params(sample_id, params, "extreme_mass_ratio")

        # Ensure priorities exist and are normalized
        sample = self._ensure_sample_priorities(sample)

        sample["is_extreme_case"] = True
        sample["extreme_case_type"] = "extreme_mass_ratio"
        sample["extreme_metadata"] = {
            "mass_ratio": float(q),
            "total_mass_msun": float(m_total),
            "primary_mass_msun": float(m1),
            "secondary_mass_msun": float(m2),
            "chirp_mass_msun": float(chirp_mass),
            "symmetric_mass_ratio": float(eta),
            "higher_harmonics_enhanced": True,
        }

        return sample

    # ========================================================================
    # 3Ô∏è‚É£ HIGH-SPIN ALIGNED/ANTI-ALIGNED (|œá| > 0.95)
    # ========================================================================

    def _generate_high_spin_aligned(self, sample_id: int) -> Dict:
        """
        Generate systems with near-maximal aligned spins (œá_eff > 0.8).

        Scientific Motivation:
            Tests spin-orbit coupling, spin-induced precession, and higher-order
            spin effects in waveform models. Critical for distinguishing formation
            channels [Gerosa & Berti, PRD 2017].

        Physical Scenario:
            Rapid rotation inherited from stellar progenitors (isolated evolution)
            or dynamical assembly with spin alignment from accretion disks.

        Astrophysical Context:
            œá_eff distribution peaks near zero for field binaries [GWTC-3].
            High œá_eff (>0.7) suggests isolated evolution or hierarchical formation.

        Parameter Ranges:
            - Dimensionless spins: |a‚ÇÅ|, |a‚ÇÇ| ‚àà [0.8, 0.99] (physical limit)
            - Tilt angles: Œ∏‚ÇÅ, Œ∏‚ÇÇ ‚àà [0, 0.2] rad (nearly aligned)
            - Effective spin: œá_eff ‚àà [0.8, 0.95]

        Returns:
            Sample with extreme aligned spins, enhanced ringdown
        """

        config = self.extreme_types_config.get("high_spin_aligned", {})
        spin_range = config.get("spin_range", [0.8, 0.99])  # Physical limit at 0.99
        tilt_max = config.get("tilt_max", 0.2)  # radians

        # Sample parameters
        params = self._sample_parameters("BBH")

        # High aligned spins
        params["a1"] = np.random.uniform(spin_range[0], spin_range[1])
        params["a2"] = np.random.uniform(spin_range[0], spin_range[1])

        # Small tilt angles (aligned)
        params["tilt1"] = np.random.uniform(0.0, tilt_max)
        params["tilt2"] = np.random.uniform(0.0, tilt_max)

        # Compute effective spin
        m1 = params["mass_1"]
        m2 = params["mass_2"]
        chi_eff = (
            m1 * params["a1"] * np.cos(params["tilt1"])
            + m2 * params["a2"] * np.cos(params["tilt2"])
        ) / (m1 + m2)

        params["chi_eff"] = chi_eff

        # Generate sample
        sample = self._generate_sample_from_params(sample_id, params, "high_spin_aligned")

        # Ensure priorities exist and are normalized
        sample = self._ensure_sample_priorities(sample)

        sample["is_extreme_case"] = True
        sample["extreme_case_type"] = "high_spin_aligned"
        sample["extreme_metadata"] = {
            "spin_1": float(params["a1"]),
            "spin_2": float(params["a2"]),
            "tilt_1_rad": float(params["tilt1"]),
            "tilt_2_rad": float(params["tilt2"]),
            "chi_eff": float(chi_eff),
            "spin_alignment": "aligned",
            "formation_channel_hint": "isolated_evolution",
        }

        return sample

    # ========================================================================
    # 4Ô∏è‚É£ PRECESSION-DOMINATED (œá_p > 0.8)
    # ========================================================================

    def _generate_precession_dominated(self, sample_id: int) -> Dict:
        """
        Generate systems with strong spin-precession (œá_p > 0.5).

        Scientific Motivation:
            Tests precession-handling in waveform models and parameter estimation.
            Precession breaks degeneracies but complicates inference [Schmidt et al., PRD 2015].

        Physical Scenario:
            Misaligned spins from supernova kicks or dynamical capture.
            Causes orbital plane precession with period œÑ_prec ~ (c¬≥/GM) √ó (r/rg)¬≥.

        Astrophysical Context:
            œá_p distribution suggests ~20% of BBH have measurable precession [GWTC-3].
            Strong precession indicates dynamical formation or natal kicks.

        Parameter Ranges:
            - In-plane spin: œá_p ‚àà [0.5, 0.9]
            - Tilt angles: Œ∏‚ÇÅ, Œ∏‚ÇÇ ‚àà [œÄ/4, 3œÄ/4] rad (misaligned)
            - Azimuthal angles: œÜ‚ÇÅ‚ÇÇ ‚àà [0, 2œÄ] rad

        Returns:
            Sample with strong precession, modulated amplitude
        """

        config = self.extreme_types_config.get("precession_dominated", {})
        chi_p_range = config.get("chi_p_range", [0.5, 0.9])

        # Sample parameters
        params = self._sample_parameters("BBH")

        # Moderate to high spins
        params["a1"] = np.random.uniform(0.5, 0.95)
        params["a2"] = np.random.uniform(0.5, 0.95)

        # Misaligned tilts (for precession)
        params["tilt1"] = np.random.uniform(np.pi / 4, 3 * np.pi / 4)
        params["tilt2"] = np.random.uniform(np.pi / 4, 3 * np.pi / 4)

        # Azimuthal angles
        params["phi12"] = np.random.uniform(0, 2 * np.pi)
        params["phi_jl"] = np.random.uniform(0, 2 * np.pi)

        # Compute effective precession spin
        m1 = params["mass_1"]
        m2 = params["mass_2"]
        q = min(m1, m2) / max(m1, m2)

        # Schmidt et al. (2015) formula
        A1 = 2 + 3 * q / 2
        A2 = 2 + 3 / (2 * q)

        S1_perp = m1**2 * params["a1"] * np.sin(params["tilt1"])
        S2_perp = m2**2 * params["a2"] * np.sin(params["tilt2"])

        chi_p = max(A1 * S1_perp, A2 * S2_perp) / (A1 * m1**2)
        params["chi_p"] = min(chi_p, 1.0)  # Cap at 1

        # Ensure chi_p in desired range
        if params["chi_p"] < chi_p_range[0]:
            # Boost spin magnitudes (clipped to 0.99 physical limit)
            scale = chi_p_range[0] / params["chi_p"]
            params["a1"] = min(params["a1"] * scale, 0.99)
            params["a2"] = min(params["a2"] * scale, 0.99)
            params["chi_p"] = chi_p_range[0]

        # Generate sample
        sample = self._generate_sample_from_params(sample_id, params, "precession_dominated")

        # Ensure priorities exist and are normalized
        sample = self._ensure_sample_priorities(sample)

        sample["is_extreme_case"] = True
        sample["extreme_case_type"] = "precession_dominated"
        sample["extreme_metadata"] = {
            "chi_p": float(params["chi_p"]),
            "spin_1": float(params["a1"]),
            "spin_2": float(params["a2"]),
            "tilt_1_rad": float(params["tilt1"]),
            "tilt_2_rad": float(params["tilt2"]),
            "phi_12_rad": float(params.get("phi12", 0)),
            "precession_strength": "strong",
            "formation_channel_hint": "dynamical_or_kicks",
        }

        return sample

    # ========================================================================
    # 5Ô∏è‚É£ ECCENTRIC OVERLAPS (e > 0.3)
    # ========================================================================

    def _generate_eccentric_overlaps(self, sample_id: int) -> Dict:
        """
        Generate overlapping systems with measurable eccentricity (e‚ÇÄ > 0.1).

        Scientific Motivation:
            Tests handling of eccentric waveforms in overlapping scenarios.
            Eccentricity breaks degeneracies but requires specialized models.

        Physical Scenario:
            Dynamically formed binaries retain eccentricity at merger.
            Expected from dense stellar environments (GCs, AGN disks).

        Astrophysical Context:
            Current waveform models (IMRPhenom, SEOBNRv4) assume e=0.
            Eccentricity detection requires specialized approximants [Hinder et al.].

        Parameter Ranges:
            - Eccentricity at 10 Hz: e‚ÇÅ‚ÇÄ ‚àà [0.1, 0.4]
            - 2-3 overlapping signals
            - Enhanced higher harmonics

        Returns:
            Overlapping sample with eccentric systems
        """

        config = self.extreme_types_config.get("eccentric_overlaps", {})
        ecc_range = config.get("eccentricity_range", [0.1, 0.4])
        n_signals_range = config.get("n_signals_range", [2, 3])

        # Number of overlapping signals
        n_signals = np.random.randint(n_signals_range[0], n_signals_range[1] + 1)

        parameters_list = []
        for i in range(n_signals):
            event_type = self._sample_event_type()
            params = self._sample_parameters(event_type)

            # Add eccentricity
            params["eccentricity"] = np.random.uniform(ecc_range[0], ecc_range[1])

            # Time offset for overlap
            if i > 0:
                params["geocent_time"] = np.random.uniform(0.5, 1.5)

            parameters_list.append(params)

        # Generate overlap
        sample = self.generate_custom_overlap(parameters_list, sample_id, is_edge_case=False)

        # Ensure priorities exist and are normalized
        sample = self._ensure_sample_priorities(sample)

        sample["is_extreme_case"] = True
        sample["extreme_case_type"] = "eccentric_overlaps"
        sample["extreme_metadata"] = {
            "n_signals": n_signals,
            "eccentricities": [p.get("eccentricity", 0) for p in parameters_list],
            "mean_eccentricity": np.mean([p.get("eccentricity", 0) for p in parameters_list]),
            "formation_channel": "dynamical",
            "waveform_approximant_needed": "EccentricTD",
        }

        return sample

    # ========================================================================
    # 6Ô∏è‚É£ WEAK-STRONG SNR OVERLAPS
    # ========================================================================

    def _generate_weak_strong_overlaps(self, sample_id: int) -> Dict:
        """
        Generate overlapping signals with extreme SNR disparity (SNR_ratio > 5:1).

        Scientific Motivation:
            Tests PriorityNet's ability to detect weak signals in presence of
            loud foreground events. Critical for catalog completeness in high-rate
            scenarios where fainter signals may be masked [Nitz et al., ApJ 2020].

        Physical Scenario:
            Nearby loud BBH (SNR ~ 40-80) overlapping with distant weak event
            (SNR ~ 8-12). Challenges subtraction and residual analysis pipelines.

        Astrophysical Context:
            Volume scaling: N(SNR > œÅ) ‚àù œÅ‚Åª¬≥ for uniform spatial distribution.
            Expect ~10% of detected events to have sub-threshold companions.

        Parameter Ranges:
            - Strong signal: SNR ‚àà [40, 80], d_L ‚àà [100, 400] Mpc
            - Weak signal: SNR ‚àà [8, 12], d_L ‚àà [800, 2000] Mpc
            - SNR ratio: œÅ_strong / œÅ_weak ‚àà [5, 10]

        Returns:
            Overlapping sample with extreme SNR hierarchy
        """

        config = self.extreme_types_config.get("weak_strong_overlaps", {})
        strong_snr_range = config.get("strong_snr_range", [40, 80])
        weak_snr_range = config.get("weak_snr_range", [8, 12])

        parameters_list = []

        # Strong foreground signal (prefer compact-object types, bias from configured distribution)
        types = ["BBH", "NSBH"]
        probs = [EVENT_TYPE_DISTRIBUTION.get(t, 0.0) for t in types]
        total_p = sum(probs)
        if total_p <= 0:
            probs = [0.5, 0.5]
        else:
            probs = [p / total_p for p in probs]

        event_type_strong = np.random.choice(types, p=probs)
        params_strong = self._sample_parameters(event_type_strong)
        # Do not overwrite any existing sampler-provided target_snr
        if "target_snr" not in params_strong:
            try:
                self._set_target_snr(
                    params_strong,
                    np.random.uniform(strong_snr_range[0], strong_snr_range[1]),
                    reason="weak_strong_assign_strong",
                )
            except Exception:
                params_strong["target_snr"] = float(
                     np.random.uniform(strong_snr_range[0], strong_snr_range[1])
                 )
                params_strong["luminosity_distance"] = np.random.uniform(100, 400)
                
                # CRITICAL FIX (Jan 19, 2026): Add jitter to prevent duplicate distances
                # Weak-strong overlaps bypass standard sampler, need explicit jitter
                params_strong["luminosity_distance"] *= np.exp(np.random.normal(0, 0.03))
        # FIX #1: Preserve original SNR for overlap scenario
        params_strong['target_snr'] = self._validate_snr(
            params_strong.get('target_snr', self.reference_snr),
            context="weak_strong_overlap_strong"
        )
        params_strong["geocent_time"] = 0.0
        parameters_list.append(params_strong)

        # Weak background signal
        event_type_weak = self._sample_event_type()
        params_weak = self._sample_parameters(event_type_weak)
        if "target_snr" not in params_weak:
            try:
                self._set_target_snr(
                    params_weak,
                    np.random.uniform(weak_snr_range[0], weak_snr_range[1]),
                    reason="weak_strong_assign_weak",
                )
            except Exception:
                params_weak["target_snr"] = float(
                     np.random.uniform(weak_snr_range[0], weak_snr_range[1])
                 )
                params_weak["luminosity_distance"] = np.random.uniform(800, 2000)
                
                # CRITICAL FIX (Jan 19, 2026): Add jitter to prevent duplicate distances
                # Weak-strong overlaps bypass standard sampler, need explicit jitter
                params_weak["luminosity_distance"] *= np.exp(np.random.normal(0, 0.03))
        # FIX #1: Preserve original SNR for weak signal in overlap
        params_weak['target_snr'] = self._validate_snr(
            params_weak.get('target_snr', self.reference_snr),
            context="weak_strong_overlap_weak"
        )
        params_weak["geocent_time"] = np.random.uniform(-0.5, 0.5)  # Small offset
        parameters_list.append(params_weak)

        # Generate overlap
        sample = self.generate_custom_overlap(parameters_list, sample_id, is_edge_case=False)

        # Ensure priorities exist and are normalized
        sample = self._ensure_sample_priorities(sample)

        sample["is_extreme_case"] = True
        sample["extreme_case_type"] = "weak_strong_overlaps"
        sample["extreme_metadata"] = {
            "strong_snr": float(params_strong["target_snr"]),
            "weak_snr": float(params_weak["target_snr"]),
            "snr_ratio": float(params_strong["target_snr"] / params_weak["target_snr"]),
            "strong_distance_mpc": float(params_strong["luminosity_distance"]),
            "weak_distance_mpc": float(params_weak["luminosity_distance"]),
            "detection_challenge": "subtraction_required",
            "catalog_completeness_test": True,
        }

        return sample

    # ========================================================================
    # 7Ô∏è‚É£ NOISE-CONFUSED OVERLAPS
    # ========================================================================

    def _generate_noise_confused_overlaps(self, sample_id: int) -> Dict:
        """
        Generate overlapping signals with strong glitch contamination.

        Scientific Motivation:
            Tests robustness to non-Gaussian noise artifacts during multi-event
            scenarios. Real detectors exhibit glitches at ~1/min rate [Davis et al.].

        Physical Scenario:
            True GW signals overlapping with instrumental glitches (blips, whistles,
            scattered light). Requires vetoes and sophisticated noise characterization.

        Astrophysical Context:
            Glitch rate comparable to expected GW rate in O5 necessitates
            simultaneous multi-signal and glitch discrimination.

        Glitch Types:
            - Blips: Œ¥-function-like (Œît ~ 10-100 ms)
            - Whistles: Frequency-swept artifacts
            - Scattered light: Periodic modulation

        Returns:
            Overlapping sample with injected glitches
        """

        config = self.extreme_types_config.get("noise_confused_overlaps", {})
        n_signals = config.get("n_signals", 2)
        glitch_prob = config.get("glitch_probability", 0.8)
        glitch_types = config.get("glitch_types", ["blip", "whistle", "scattered_light"])

        # Generate overlapping signals
        parameters_list = []
        for i in range(n_signals):
            event_type = self._sample_event_type()
            params = self._sample_parameters(event_type)
            if i > 0:
                params["geocent_time"] = np.random.uniform(0.3, 1.0)
            parameters_list.append(params)

        sample = self.generate_custom_overlap(parameters_list, sample_id, is_edge_case=False)

        # Ensure priorities exist and are normalized
        sample = self._ensure_sample_priorities(sample)

        # Add glitches with high probability
        if np.random.random() < glitch_prob:
            glitch_type = np.random.choice(glitch_types)

            for detector_name in sample["detector_data"].keys():
                strain = sample["detector_data"][detector_name]["strain"]

                # Inject glitch based on type
                if glitch_type == "blip":
                    glitch_time = np.random.randint(len(strain) // 4, 3 * len(strain) // 4)
                    glitch_width = np.random.randint(20, 100)
                    glitch_amplitude = np.random.uniform(5, 15) * np.std(strain)
                    glitch = glitch_amplitude * np.exp(
                        -((np.arange(len(strain)) - glitch_time) ** 2) / (2 * glitch_width**2)
                    )
                    strain += glitch

                elif glitch_type == "whistle":
                    t = np.linspace(0, len(strain) / self.sample_rate, len(strain))
                    f0 = np.random.uniform(50, 200)
                    f1 = np.random.uniform(f0 + 50, 400)
                    freq = f0 + (f1 - f0) * (t / t[-1])
                    phase = 2 * np.pi * np.cumsum(freq) / self.sample_rate
                    glitch_amplitude = np.random.uniform(3, 10) * np.std(strain)
                    glitch = (
                        glitch_amplitude
                        * np.sin(phase)
                        * np.exp(-t / (len(strain) / (2 * self.sample_rate)))
                    )
                    strain += glitch

                elif glitch_type == "scattered_light":
                    freq_mod = np.random.uniform(10, 50)
                    phase_mod = 2 * np.pi * freq_mod * np.arange(len(strain)) / self.sample_rate
                    modulation = 1 + 0.3 * np.sin(phase_mod)
                    strain *= modulation

                sample["detector_data"][detector_name]["strain"] = strain.astype(np.float32)

            sample["has_glitch"] = True
            sample["glitch_type"] = glitch_type

        sample["is_extreme_case"] = True
        sample["extreme_case_type"] = "noise_confused_overlaps"
        sample["extreme_metadata"] = {
            "n_signals": n_signals,
            "glitch_present": sample.get("has_glitch", False),
            "glitch_type": sample.get("glitch_type", "none"),
            "data_quality_challenge": "high",
            "veto_requirement": "strict",
        }

        return sample

    # ========================================================================
    # 8Ô∏è‚É£ LONG-DURATION BNS OVERLAPS
    # ========================================================================

    def _generate_long_duration_bns_overlaps(self, sample_id: int) -> Dict:
        """
        Generate overlapping BNS systems with long inspiral in-band (T > 60s).

        Scientific Motivation:
            Tests computational efficiency and long-duration waveform handling.
            BNS signals spend minutes in LIGO band, challenging real-time analysis
            [Aasi et al., CQG 2015].

        Physical Scenario:
            Multiple BNS systems with f_low ~ 10-20 Hz entering band simultaneously.
            Requires tracking phase evolution over O(10¬≥) cycles.

        Astrophysical Context:
            BNS rate: R_BNS ~ 320 Gpc‚Åª¬≥yr‚Åª¬π [GWTC-3]
            Long duration enables early warning but increases overlap probability.

        Parameter Ranges:
            - Component masses: m ‚àà [1.0, 2.0] M_‚òâ
            - Starting frequency: f_low ‚àà [10, 20] Hz
            - In-band time: T_inspiral > 60 s
            - Tidal deformability: Œõ ‚àà [0, 5000]

        Returns:
            Overlapping BNS sample with tidal effects
        """

        config = self.extreme_types_config.get("long_duration_bns_overlaps", {})

        #  FIX: Read correct config keys
        min_signals = config.get("min_signals", 2)
        max_signals = config.get("max_signals", 3)
        n_signals = np.random.randint(min_signals, max_signals + 1)

        f_lower_max = config.get("f_lower_max", 25)
        overlap_required = config.get("overlap_required", True)

        # ‚ö†Ô∏è duration_min is conceptual - we can't fit 30s in 4s window
        # Instead: use low f_lower to simulate end of long inspiral

        parameters_list = []

        for i in range(n_signals):
            params = self._sample_parameters("BNS")

            # Low starting frequency ‚Üí represents end of long inspiral
            params["f_lower"] = np.random.uniform(10, f_lower_max)

            # BNS-specific masses
            params["mass_1"] = np.random.uniform(1.0, 2.0)
            params["mass_2"] = np.random.uniform(1.0, params["mass_1"])

            # Tidal parameters
            params["lambda_1"] = np.random.uniform(0, 5000)
            params["lambda_2"] = np.random.uniform(0, 5000)

            # Compute tidal deformability
            m1 = params["mass_1"]
            m2 = params["mass_2"]
            q = m2 / m1
            eta = (m1 * m2) / (m1 + m2) ** 2

            lambda_tilde = (8.0 / 13) * (
                (1 + 7 * eta - 31 * eta**2) * (params["lambda_1"] + params["lambda_2"])
                + np.sqrt(1 - 4 * eta)
                * (1 + 9 * eta - 11 * eta**2)
                * (params["lambda_1"] - params["lambda_2"])
            )

            params["lambda_tilde"] = lambda_tilde

            #  FIX: Create OVERLAPS if required
            if overlap_required and i > 0:
                # Overlap in last 2 seconds before merger
                params["geocent_time"] = np.random.uniform(-2.0, 2.0)
            else:
                # Well-separated
                params["geocent_time"] = i * 1.5 if i > 0 else 0.0

            params["type"] = "BNS"
            parameters_list.append(params)

        # Generate overlap
        sample = self.generate_custom_overlap(parameters_list, sample_id, is_edge_case=False)

        # Ensure priorities exist and are normalized
        sample = self._ensure_sample_priorities(sample)

        sample["is_extreme_case"] = True
        sample["extreme_case_type"] = "long_duration_bns_overlaps"
        sample["extreme_metadata"] = {
            "n_signals": n_signals,
            "f_lower_hz": [p["f_lower"] for p in parameters_list],
            "lambda_tilde": [p.get("lambda_tilde", 0) for p in parameters_list],
            "note": "Last 4s of long-duration BNS inspiral",
            "equivalent_full_duration_s": "30+",  # What it represents
            "computational_challenge": "low_frequency_long_inspiral",
            "early_warning_potential": True,
        }

        return sample

    # ========================================================================
    # 9Ô∏è‚É£ DETECTOR DROPOUTS
    # ========================================================================

    def _generate_detector_dropouts(self, sample_id: int) -> Dict:
        """
        Generate signals with 1-2 detector outages during event.

        Scientific Motivation:
            Tests robustness to incomplete detector networks. Real O3 experienced
            ~30% duty cycle per detector [Abbott et al., PRX 2021].

        Physical Scenario:
            Signal visible in H1 only (L1 offline), or H1+V1 (L1 dropout).
            Degrades sky localization and parameter estimation.

        Astrophysical Context:
            Future multi-detector networks (LIGO-Virgo-KAGRA-LIGO India) increase
            redundancy, but individual detector failures remain probable.

        Network Configurations:
            - H1 only: ŒîŒ∏ ~ 360¬∞ (no localization)
            - H1+L1: ŒîŒ∏ ~ 20 deg¬≤ (good)
            - H1+V1: ŒîŒ∏ ~ 100 deg¬≤ (moderate)
            - Full network: ŒîŒ∏ ~ 10 deg¬≤ (excellent)

        Returns:
            Sample with zeroed-out detector data
        """

        config = self.extreme_types_config.get("detector_dropouts", {})
        dropout_prob = config.get("dropout_probability", 0.5)
        min_active = config.get("min_active_detectors", 1)

        # Generate base sample
        sample = self._generate_single_sample(
            sample_id=sample_id, is_edge_case=False, add_glitches=False, preprocess=True
        )

        if np.random.random() < dropout_prob:
            # Drop 1-2 detectors
            n_detectors = len(self.detectors)
            n_to_drop = np.random.randint(1, min(3, n_detectors - min_active + 1))

            detectors_to_drop = list(
                np.random.choice(self.detectors, size=n_to_drop, replace=False)
            )

            # Zero out dropped detectors
            for det in detectors_to_drop:
                if det in sample["detector_data"]:
                    sample["detector_data"][det]["strain"] *= 0

            active_dets = [d for d in self.detectors if d not in detectors_to_drop]

            sample["active_detectors"] = active_dets
            sample["dropped_detectors"] = detectors_to_drop

            # Estimate sky localization degradation
            if len(active_dets) == 1:
                sky_area_deg2 = 41253  # Full sky
            elif len(active_dets) == 2:
                if "H1" in active_dets and "L1" in active_dets:
                    sky_area_deg2 = 20
                else:
                    sky_area_deg2 = 100
            else:
                sky_area_deg2 = 10

            sample["sky_localization_deg2"] = sky_area_deg2

        # Ensure priorities exist and are normalized
        sample = self._ensure_sample_priorities(sample)

        sample["is_extreme_case"] = True
        sample["extreme_case_type"] = "detector_dropouts"
        sample["extreme_metadata"] = {
            "n_active_detectors": len(sample.get("active_detectors", self.detectors)),
            "active_detectors": sample.get("active_detectors", self.detectors),
            "dropped_detectors": sample.get("dropped_detectors", []),
            "sky_localization_deg2": sample.get("sky_localization_deg2", 10),
            "parameter_estimation_degradation": "moderate_to_severe",
        }

        return sample

    # ========================================================================
    # üîü COSMOLOGICAL-SCALE DISTANCES
    # ========================================================================

    def _generate_cosmological_distance(self, sample_id: int) -> Dict:
        """
        Generate signals at cosmological distances (z > 0.5, d_L > 2 Gpc).

        Scientific Motivation:
            Tests low-SNR detection and Malmquist bias corrections.
            Probes cosmological merger rate evolution œÅ(z) [Fishbach & Holz, ApJL 2017].

        Physical Scenario:
            High-mass BBH (M_total > 80 M_‚òâ) at z ~ 0.5-1.0.
            Source-frame masses scaled by (1+z).

        Astrophysical Context:
            GWTC-3 reports z_max ~ 0.9 (GW190521).
            Future detectors (A+, Voyager) will probe z > 2.

        Parameter Ranges:
            - Redshift: z ‚àà [0.5, 1.0]
            - Luminosity distance: d_L ‚àà [2000, 5000] Mpc
            - Source-frame masses: M_source = M_detector / (1+z)
            - Detector-frame masses: M ‚àà [80, 150] M_‚òâ

        Cosmology:
            H‚ÇÄ = 67.4 km/s/Mpc, Œ©‚Çò = 0.315, Œ©Œõ = 0.685 [Planck 2018]

        Returns:
            Sample with cosmologically redshifted parameters
        """

        config = self.extreme_types_config.get("cosmological_distance", {})
        z_range = config.get("redshift_range", [0.5, 1.0])

        # Sample redshift
        z = np.random.uniform(z_range[0], z_range[1])

        # Compute luminosity distance (approximate)
        # For Œõ-CDM: d_L ‚âà (c/H‚ÇÄ) √ó (1+z) √ó ‚à´[0‚Üíz] dz' / sqrt(Œ©‚Çò(1+z')¬≥ + Œ©Œõ)
        # Simplified for z < 1:
        c = 299792.458  # km/s
        H0 = 67.4  # km/s/Mpc
        d_L = (c / H0) * z * (1 + z / 2)  # Approximate

        # Sample source-frame masses
        M_source_total = np.random.uniform(40, 75)  # Source frame
        q = np.random.uniform(0.5, 1.0)

        M1_source = M_source_total / (1 + q)
        M2_source = M1_source * q

        # Detector-frame masses (redshifted)
        M1_detector = M1_source * (1 + z)
        M2_detector = M2_source * (1 + z)

        # Generate parameters
        params = self._sample_parameters("BBH")

        # Override with cosmological values
        params.update(
            {
                "mass_1": M1_detector,
                "mass_2": M2_detector,
                "luminosity_distance": d_L,
                "redshift": z,
                "mass_1_source": M1_source,
                "mass_2_source": M2_source,
                "chirp_mass": (M1_detector * M2_detector) ** (3 / 5)
                / (M1_detector + M2_detector) ** (1 / 5),
                "chirp_mass_source": (M1_source * M2_source) ** (3 / 5)
                / (M1_source + M2_source) ** (1 / 5),
            }
        )

        # Low SNR expected - only set if sampler didn't specify
        if "target_snr" not in params:
            self._set_target_snr(
                params, np.random.uniform(8, 12), reason="cosmological_distance_default"
            )

        # Generate sample
        sample = self._generate_sample_from_params(sample_id, params, "cosmological_distance")

        # Ensure priorities exist and are normalized
        sample = self._ensure_sample_priorities(sample)

        sample["is_extreme_case"] = True
        sample["extreme_case_type"] = "cosmological_distance"
        sample["extreme_metadata"] = {
            "redshift": float(z),
            "luminosity_distance_mpc": float(d_L),
            "comoving_distance_mpc": float(d_L / (1 + z)),
            "source_frame_mass_1_msun": float(M1_source),
            "source_frame_mass_2_msun": float(M2_source),
            "detector_frame_mass_1_msun": float(M1_detector),
            "detector_frame_mass_2_msun": float(M2_detector),
            "time_dilation_factor": float(1 + z),
            "cosmology": "Planck2018",
            "H0_km_s_Mpc": 67.4,
            "low_snr_cosmological": True,
        }

        return sample

    # ========================================================================
    # MAIN GENERATION DISPATCHER
    # ========================================================================

    def _generate_extreme_case_multi_noise(
        self, sample_id: int, extreme_type: str, k: int = 1
    ) -> List[Dict]:
        """
        Generate K extreme case samples with SAME parameters but DIFFERENT noise realizations.
        
        Args:
            sample_id: Unique sample identifier
            extreme_type: One of the extreme case types
            k (int): Number of noise realizations (default: 1 = no STEP 4)
            
        Returns:
            List[Dict]: K samples with identical signal parameters but different noise
        """
        if k <= 1:
            # No STEP 4, return single sample wrapped in list
            return [self._generate_extreme_case(sample_id, extreme_type)]
        
        # ‚úÖ STEP 4: Generate base sample ONCE (with parameters)
        base_sample = self._generate_extreme_case(sample_id, extreme_type)
        
        # Extract parameters from sample
        if isinstance(base_sample.get("parameters"), list) and len(base_sample["parameters"]) > 0:
            signal_params_list = base_sample["parameters"]
        elif isinstance(base_sample.get("parameters"), dict):
            signal_params_list = [base_sample["parameters"]]
        else:
            # Can't extract params, return single sample
            return [base_sample]
        
        samples_list = []
        
        # Generate K variants with DIFFERENT noise
        for noise_idx in range(k):
            detector_data = {}
            noise_types = {}
            
            for detector_name in self.detectors:
                psd_dict = self.psds[detector_name]
                
                # üîÑ KEY: Different noise for each variant (new random realization)
                noise, noise_type = self._get_noise_for_detector(detector_name, psd_dict)
                noise_types[detector_name] = noise_type
                
                # Re-inject SAME signals into NEW noise
                if len(signal_params_list) > 1:
                    # Multiple signals: use overlapping injection
                    injected, metadata_list = self.injector.inject_overlapping_signals(
                        noise, signal_params_list, detector_name, psd_dict
                    )
                else:
                    # Single signal: use standard injection
                    injected, metadata = self.injector.inject_signal(
                        noise, signal_params_list[0], detector_name, psd_dict
                    )
                    metadata_list = [metadata]
                
                if self.preprocess_enabled:
                    injected = self.preprocessor.preprocess(injected, psd_dict)
                
                strain_final = injected.astype(np.float32)
                noise_final = noise.astype(np.float32)
                
                if np.any(~np.isfinite(noise_final)):
                    noise_final = np.random.randn(len(noise_final)).astype(np.float32) * 1e-20
                
                if np.any(~np.isfinite(strain_final)):
                    strain_final = noise_final.copy()
                
                detector_data[detector_name] = {
                    "strain": strain_final,
                    "noise": noise_final,
                    "psd": psd_dict.get("psd"),
                    "frequencies": psd_dict.get("frequencies"),
                    "metadata": metadata_list if metadata_list else [{}],
                }
            
            # Create sample with unique ID for this noise variant
            sample = {
                "sample_id": f"{base_sample['sample_id']}_noise{noise_idx}",
                "type": base_sample.get("type", "unknown"),
                "event_type": base_sample.get("event_type", "unknown"),
                "parameters": signal_params_list,
                "detector_data": detector_data,
                "is_extreme_case": True,
                "extreme_case_type": base_sample.get("extreme_case_type", extreme_type),
                "extreme_metadata": base_sample.get("extreme_metadata", {}),
                "metadata": {
                    "sample_id": sample_id,
                    "noise_variant": noise_idx,
                    "n_noise_variants": k,
                    "n_signals": len(signal_params_list),
                    "is_edge_case": True,
                    "is_extreme_case": True,
                    "extreme_type": extreme_type,
                    "noise_sources": noise_types,
                    "parent_sample_id": base_sample.get("sample_id"),
                },
            }
            
            # Preserve priorities if they exist
            if "priorities" in base_sample:
                sample["priorities"] = base_sample["priorities"]
            
            samples_list.append(sample)
        
        return samples_list

    def _generate_extreme_case(self, sample_id: int, extreme_type: str) -> Dict:
        """
        Generate extreme case of specified type.

        Args:
            sample_id: Unique sample identifier
            extreme_type: One of the 10 extreme case types

        Returns:
            Generated sample dictionary
        """
        dispatch = {
            "near_simultaneous_mergers": self._generate_near_simultaneous_mergers,
            "extreme_mass_ratio": self._generate_extreme_mass_ratio,
            "high_spin_aligned": self._generate_high_spin_aligned,
            "precession_dominated": self._generate_precession_dominated,
            "eccentric_overlaps": self._generate_eccentric_overlaps,
            "weak_strong_overlaps": self._generate_weak_strong_overlaps,
            "noise_confused_overlaps": self._generate_noise_confused_overlaps,
            "long_duration_bns_overlaps": self._generate_long_duration_bns_overlaps,
            "detector_dropouts": self._generate_detector_dropouts,
            "cosmological_distance": self._generate_cosmological_distance,
            "pre_merger_samples": self._generate_pre_merger_sample,
        }

        if extreme_type not in dispatch:
            self.logger.warning(
                f"Unknown extreme case type: {extreme_type}, generating regular sample"
            )
            return self._generate_single_sample(
                sample_id=sample_id, is_edge_case=False, add_glitches=False, preprocess=True
            )

        return dispatch[extreme_type](sample_id)

    def _generate_near_simultaneous_mergers(self, sample_id: int) -> Dict:
        """
        Generate overlapping signals with Œît < 0.2s between mergers.

        Scientific motivation: Tests PriorityNet's ability to resolve near-simultaneous
        events, critical for high-rate observing runs (O5+) where event rate may
        exceed 1 per minute.

        Physical scenario: Two independent binary mergers occurring within light-travel
        time across detectors (~20ms for H1-L1), requiring coherent multi-detector
        analysis for separation.

        Returns:
            Sample with 2-3 overlapping signals, Œît ‚àà [50ms, 200ms]
        """

        # Configuration
        config = self.extreme_types_config.get("near_simultaneous_mergers", {})
        delta_t_range = config.get("delta_t_range", [0.05, 0.2])
        n_signals_range = config.get("n_signals_range", [2, 3])

        # Sample number of signals
        n_signals = np.random.randint(n_signals_range[0], n_signals_range[1] + 1)

        # Get event type pairs from config
        event_pairs = config.get(
            "event_type_pairs", [["BBH", "BBH"], ["BBH", "BNS"], ["BNS", "BNS"]]
        )

        #  NEW (CORRECT - pick one pair, then extend to n_signals):
        selected_pair = event_pairs[np.random.randint(len(event_pairs))]

        # Extend to match n_signals
        event_types = []
        for i in range(n_signals):
            if i < len(selected_pair):
                event_types.append(selected_pair[i])
            else:
                # If need more signals than pair has, randomly pick from pair
                event_types.append(selected_pair[np.random.randint(len(selected_pair))])

        # Generate parameters for each signal
        parameters_list = []
        actual_delta_ts = []

        for i in range(n_signals):
            event_type = event_types[i]
            params = self._sample_parameters(event_type)

            # Set merger times
            if i == 0:
                params["geocent_time"] = 0.0
            else:
                delta_t = np.random.uniform(delta_t_range[0], delta_t_range[1])
                params["geocent_time"] = delta_t
                actual_delta_ts.append(delta_t)

            parameters_list.append(params)

        # Generate overlap
        sample = self.generate_custom_overlap(parameters_list, sample_id, is_edge_case=False)

        #  At end of EVERY extreme case function: ensure priorities and n_signals
        sample = self._ensure_sample_priorities(sample)

        # Add metadata
        sample["is_extreme_case"] = True
        sample["extreme_case_type"] = "near_simultaneous_mergers"
        sample["extreme_metadata"] = {
            "delta_t_max_ms": max(actual_delta_ts) * 1000 if actual_delta_ts else 0.0,
            "delta_t_min_ms": min(actual_delta_ts) * 1000 if actual_delta_ts else 0.0,
            "n_signals": n_signals,
            "event_types": event_types,
            "merger_times": [p["geocent_time"] for p in parameters_list],
            "temporal_overlap": "near_simultaneous",
        }

        return sample

    def generate_custom_overlap(
        self, parameters_list: List[Dict], sample_id: int, is_edge_case: bool = False
    ) -> Dict:
        """
        Generate overlapping scenario from parameter list.

        Args:
            parameters_list: List of parameter dicts for each signal (or None/empty for noise-only)
            sample_id: Unique sample identifier
            is_edge_case: Whether this is an edge case sample

        Returns:
            Sample dict with overlapping signals and proper priorities.
            If parameters_list is empty/None, returns noise-only overlap sample.

        Raises:
            TypeError: If parameters_list is not a list/tuple
        """

        # ========================================================================
        # INPUT VALIDATION
        # ========================================================================
        if parameters_list is None:
            self.logger.warning(
                f"generate_custom_overlap (sample {sample_id}): "
                f"received None - treating as empty list (noise-only)"
            )
            parameters_list = []

        if not isinstance(parameters_list, (list, tuple)):
            raise TypeError(
                f"generate_custom_overlap (sample {sample_id}): "
                f"parameters_list must be a list or tuple, got {type(parameters_list).__name__}"
            )

        n_signals = len(parameters_list)

        # ========================================================================
        # NOISE-ONLY FALLBACK
        # ========================================================================
        if n_signals == 0:
            self.logger.warning(
                f"generate_custom_overlap (sample {sample_id}): "
                f"empty parameters_list - generating noise-only overlap sample"
            )

            detector_data = {}

            for detector_name in self.detectors:
                psd_dict = self.psds[detector_name]

                # Generate noise-only strain
                noise, _ = self._get_noise_for_detector(detector_name, psd_dict)
                combined = noise

                # Apply preprocessing if enabled
                if self.preprocess_enabled:
                    try:
                        combined = self.preprocessor.preprocess(combined, psd_dict)
                    except Exception as e:
                        self.logger.debug(f"Preprocessing failed for detector {detector_name}: {e}")

                detector_data[detector_name] = {
                    "strain": combined.astype(np.float32),
                    "noise": noise.astype(np.float32),
                    "psd": psd_dict.get("psd", None),
                    "frequencies": psd_dict.get("frequencies", None),
                }

            sample = {
                "id": sample_id,  #  ADDED: raw integer ID
                "sample_id": f"overlap_{sample_id:06d}",
                "type": "overlap",
                "is_overlap": True,
                "n_signals": 0,
                "is_edge_case": is_edge_case,
                "parameters": [],
                "priorities": [],
                "detector_data": detector_data,
                "metadata": {
                    "sample_id": sample_id,
                    "n_signals": 0,
                    "mean_snr": 0.0,
                    "max_snr": 0.0,
                    "generator": "custom_overlap",
                    "noise_only": True,  #  ADDED: explicit flag
                },
            }

            return sample

        # ========================================================================
        # STANDARD OVERLAP GENERATION
        # ========================================================================
        detector_data = {}
        signal_contributions = {det: {} for det in self.detectors}

        for detector_name in self.detectors:
            psd_dict = self.psds[detector_name]

            # Generate noise
            noise, _ = self._get_noise_for_detector(detector_name, psd_dict)
            combined = noise.copy()

            # Inject all signals
            for i, params in enumerate(parameters_list):
                try:
                    # Generate waveform
                    strain = self.waveform_generator.generate_waveform(params, detector_name)

                    # Ensure correct length
                    expected_length = int(self.duration * self.sample_rate)
                    if len(strain) > expected_length:
                        strain = strain[:expected_length]
                    elif len(strain) < expected_length:
                        strain = np.pad(strain, (0, expected_length - len(strain)), mode="constant")

                    # Store individual contribution
                    signal_contributions[detector_name][i] = strain

                    # Add to combined signal
                    combined += strain

                except Exception as e:
                    self.logger.warning(
                        f"Failed to generate overlap signal {i} for {detector_name}: {e}"
                    )
                    signal_contributions[detector_name][i] = np.zeros(
                        int(self.duration * self.sample_rate)
                    )

            # Preprocess if enabled
            if self.preprocess_enabled:
                combined = self.preprocessor.preprocess(combined, psd_dict)

            # Store detector data
            detector_data[detector_name] = {
                "strain": combined.astype(np.float32),
                "noise": noise.astype(np.float32),
                "psd": psd_dict["psd"],
                "frequencies": psd_dict["frequencies"],
            }

        # ========================================================================
        # CALCULATE PRIORITIES
        # ========================================================================
        priorities = []
        for i, params in enumerate(parameters_list):
            # Try to compute SNR from individual contributions
            max_snr = 0.0
            for detector_name in self.detectors:
                if i in signal_contributions[detector_name]:
                    individual_strain = signal_contributions[detector_name][i]

                    # Estimate SNR
                    snr = self._estimate_snr_from_params(params)
                    max_snr = max(max_snr, snr)

            # Fallback if SNR calculation failed
            if max_snr < 5.0:
                max_snr = self._estimate_snr_from_params(params)

            raw_snr = float(np.clip(max_snr, 7.0, 80.0))
            # Do not overwrite an existing sampled target_snr ‚Äî only set if missing
            if "target_snr" not in params:
                self._set_target_snr(params, raw_snr, reason="custom_overlap_priority")
            # Normalize raw SNR to [0, 1] priority range
            priority = self._normalize_priority_to_01(raw_snr)
            priorities.append(priority)

        # ========================================================================
        # CONSTRUCT SAMPLE
        # ========================================================================
        sample = {
            "id": sample_id,  #  ADDED: raw integer ID
            "sample_id": f"overlap_{sample_id:06d}",
            "type": "overlap",
            "is_overlap": True,
            "n_signals": n_signals,
            "is_edge_case": is_edge_case,
            "parameters": parameters_list,
            "priorities": priorities,
            "detector_data": detector_data,
            "metadata": {
                "sample_id": sample_id,
                "n_signals": n_signals,
                "mean_snr": float(np.mean(priorities)),
                "max_snr": float(np.max(priorities)),
                "generator": "custom_overlap",
                "noise_only": False,  #  ADDED: explicit flag
            },
        }

        return sample

    def generate_sample_with_simulator(self, sample_id: int, n_signals: int = None) -> Dict:
        """
        Generate sample using OverlappingSignalSimulator.
        This provides COMPLETE waveforms with proper SNR calculation.

        Args:
            sample_id: Unique sample ID
            n_signals: Number of overlapping signals (None = random 2-4)

        Returns:
            Sample dict with detector_data, parameters, and priorities
        """

        if n_signals is None:
            n_signals = np.random.choice([1, 2, 3, 4], p=[0.3, 0.4, 0.2, 0.1])

        # Generate scenario using simulator
        scenario = self.simulator.generate_overlapping_scenario(n_signals)

        # Generate detector noise
        noise_data = self.simulator.generate_detector_noise(
            duration=self.config.waveform.duration,
            sampling_rate=self.config.detectors[0].sampling_rate,
        )

        # Inject signals
        injected_data, signal_contributions = self.simulator.inject_signals_to_data(
            scenario, noise_data
        )

        #  Calculate priorities (SNR) for each signal
        priorities = []
        parameters = []

        for signal in scenario["signals"]:
            # Extract signal parameters
            params = {
                "mass_1": signal["mass_1"],
                "mass_2": signal["mass_2"],
                "total_mass": signal["mass_1"] + signal["mass_2"],
                "chirp_mass": (
                    (signal["mass_1"] * signal["mass_2"]) ** (3 / 5)
                    / (signal["mass_1"] + signal["mass_2"]) ** (1 / 5)
                ),
                "mass_ratio": signal["mass_2"] / signal["mass_1"],
                "luminosity_distance": signal["luminosity_distance"],
                "theta_jn": signal["theta_jn"],
                "psi": signal["psi"],
                "phase": signal["phase"],
                "ra": signal["ra"],
                "dec": signal["dec"],
                "a1": signal.get("a_1", 0.0),
                "a2": signal.get("a_2", 0.0),
                "tilt1": signal.get("tilt_1", 0.0),
                "tilt2": signal.get("tilt_2", 0.0),
                "geocent_time": signal.get("geocent_time", 0.0),
                "type": self._classify_signal_type(signal["mass_1"], signal["mass_2"]),
            }

            #  Calculate SNR for this signal across all detectors
            max_snr = 0.0
            for detector_name, signal_strain in signal_contributions[detector_name].items():
                if signal["signal_id"] == signal_strain:
                    snr = estimate_snr_from_strain(
                        signal_strain, sampling_rate=self.config.detectors[0].sampling_rate
                    )
                    max_snr = max(max_snr, snr)

            # Use luminosity distance to estimate SNR if strain method fails
            if max_snr < 5:
                # Fallback: SNR scales as 1/distance for fixed masses
                distance_ref = 500.0  # Mpc
                snr_ref = 15.0
                max_snr = snr_ref * (distance_ref / signal["luminosity_distance"])
                max_snr *= np.sqrt(signal["mass_1"] * signal["mass_2"]) / 30.0  # Mass scaling

            raw_snr = float(np.clip(max_snr, 7.0, 80.0))
            # Do not overwrite an existing sampled target_snr
            if "target_snr" not in params:
                self._set_target_snr(params, raw_snr, reason="simulator_overlap_priority")
            # Normalize raw SNR to [0, 1] priority range
            priority = self._normalize_priority_to_01(raw_snr)
            priorities.append(priority)
            parameters.append(params)

        # Convert detector data to your format
        detector_data = {}
        for detector_name, strain in injected_data.items():
            detector_data[detector_name] = {
                "strain": strain.astype(np.float32),
                "noise": noise_data[detector_name].astype(np.float32),
            }

        # Attach network_snr to parameters
        attach_network_snr_safe(parameters)

        return {
            "sample_id": f"sim_{sample_id:06d}",
            "type": "overlap" if n_signals > 1 else parameters[0]["type"],
            "is_overlap": n_signals > 1,
            "n_signals": n_signals,
            "is_edge_case": False,
            "parameters": parameters,  # List of parameter dicts
            "priorities": priorities,  #  List of SNRs (one per signal)
            "detector_data": detector_data,
            "edge_type_id": encode_edge_type(parameters),
            "metadata": {
                "sample_id": sample_id,
                "generator": "OverlappingSignalSimulator",
                "n_signals": n_signals,
                "mean_snr": float(np.mean(priorities)),
                "max_snr": float(np.max(priorities)),
            },
        }

    def generate_sample_from_gwtc(self, sample_id: int, event_name: str = None) -> Dict:
        """
        Generate sample from real GWTC event.
        Great for validation and augmentation with real data.

        Args:
            sample_id: Unique sample ID
            event_name: Specific event (e.g., 'GW150914'), or None for random

        Returns:
            Sample dict with real event parameters
        """

        # Lazy initialize GWTC loader
        if not hasattr(self, "gwtc_loader"):
            from ahsd.data.gwtc_loader import GWTCLoader

            self.gwtc_loader = GWTCLoader()
            self.gwtc_events = self.gwtc_loader.get_gwtc_events()
            self.logger.info(f" Loaded {len(self.gwtc_events)} GWTC events")

        # Select event
        if event_name is None:
            # Filter high-quality events
            quality_events = self.gwtc_events[
                (self.gwtc_events["network_snr"] > 10) & (self.gwtc_events["mass_1_source"] > 5)
            ]
            if len(quality_events) == 0:
                quality_events = self.gwtc_events

            event = quality_events.sample(1).iloc[0]
        else:
            event = self.gwtc_events[self.gwtc_events["event_name"] == event_name].iloc[0]

        # Convert GWTC parameters to your format
        params = {
            "mass_1": float(event["mass_1_source"]),
            "mass_2": float(event["mass_2_source"]),
            "total_mass": float(event["total_mass_source"]),
            "chirp_mass": float(event["chirp_mass_source"]),
            "mass_ratio": float(event["mass_2_source"] / event["mass_1_source"]),
            "luminosity_distance": float(event["luminosity_distance"]),
            "redshift": float(event["redshift"]),
            "target_snr": float(event["network_snr"]),  #  Real SNR from GWTC!
            "type": self._classify_signal_type(event["mass_1_source"], event["mass_2_source"]),
            "event_name": event["event_name"],
            "gps_time": float(event["gps_time"]),
        }

        # Try to download real strain data (use robust config access)

        # ============================================================================
        # CONFIG ACCESS HELPERS - UNIFIED PATTERN
        # ============================================================================

        def _get_config_value(self, *keys, default=None):
            """
            Unified config accessor supporting both dict and attribute access.

            This method handles the dual nature of self.config (can be dict or object)
            and provides a consistent interface with proper fallbacks.

            Args:
                *keys: Nested keys to traverse (e.g., 'waveform', 'duration')
                default: Default value if key not found or None

            Returns:
                Config value or default

            Examples:
                >>> # Access nested value
                >>> duration = self._get_config_value('waveform', 'duration', default=4.0)
                >>> # Access top-level value
                >>> n_samples = self._get_config_value('n_samples', default=1000)
            """
            value = self.config

            for key in keys:
                if value is None:
                    return default

                # Try dict-style access first (most common)
                if isinstance(value, dict):
                    value = value.get(key)
                # Fallback to attribute access (for dataclass/object configs)
                elif hasattr(value, key):
                    value = getattr(value, key, None)
                else:
                    return default

            # Return value if found and not None, otherwise default
            return value if value is not None else default

        def _get_config_duration(self) -> float:
            """Get waveform duration with fallback to self.duration."""
            return float(self._get_config_value("waveform", "duration", default=self.duration))

        def _get_config_approximant(self) -> str:
            """Get waveform approximant with fallback to 'IMRPhenomPv2'."""
            return str(self._get_config_value("waveform", "approximant", default="IMRPhenomPv2"))

        def _get_config_f_ref(self) -> float:
            """Get reference frequency with fallback to 20.0 Hz."""
            return float(self._get_config_value("waveform", "f_ref", default=20.0))

        def _get_config_f_lower(self) -> float:
            """Get lower frequency cutoff with fallback to 20.0 Hz."""
            return float(self._get_config_value("waveform", "f_lower", default=20.0))

        def _get_extreme_type_config(self, type_name: str) -> Dict:
            """
            Get configuration for specific extreme case type.

            Args:
                type_name: Extreme case type (e.g., 'near_simultaneous_mergers')

            Returns:
                Config dict for that type, or empty dict if not found

            Examples:
                >>> config = self._get_extreme_type_config('extreme_mass_ratio')
                >>> q_max = config.get('q_max', 0.05)
            """
            extreme_config = self._get_config_value("extreme_cases", "types", default={})

            if isinstance(extreme_config, dict):
                return extreme_config.get(type_name, {})

            return {}

        def _get_detector_sampling(det_name: str) -> int:
            # Try attribute-style config first
            try:
                dets = getattr(self.config, "detectors", None)
                if dets:
                    for d in dets:
                        name = getattr(d, "name", None)
                        sr = getattr(d, "sampling_rate", None)
                        if name == det_name and sr:
                            return int(sr)
                    # fallback to first detector entry
                    first_sr = getattr(dets[0], "sampling_rate", None)
                    if first_sr:
                        return int(first_sr)
            except Exception:
                pass

            # Try dict-style config
            try:
                dets = self.config.get("detectors", None)
                if isinstance(dets, list) and len(dets) > 0:
                    for d in dets:
                        if (
                            isinstance(d, dict)
                            and d.get("name") == det_name
                            and d.get("sampling_rate")
                        ):
                            return int(d.get("sampling_rate"))
                    if isinstance(dets[0], dict) and dets[0].get("sampling_rate"):
                        return int(dets[0].get("sampling_rate"))
            except Exception:
                pass

            # Final fallback
            return int(self.sample_rate)

        duration_cfg = _get_config_duration()

        detector_data = {}
        for detector_name in ["H1", "L1", "V1"]:
            if detector_name in event.get("detectors", []):
                sample_rate_cfg = _get_detector_sampling(detector_name)
                try:
                    strain = self.gwtc_loader.download_strain(
                        event["event_name"],
                        detector=detector_name,
                        duration=duration_cfg,
                        sample_rate=sample_rate_cfg,
                    )
                except Exception as e:
                    self.logger.debug(f"Strain download exception for {detector_name}: {e}")
                    strain = None

                if strain is not None:
                    # For real GWTC data, add placeholder noise_type metadata
                    detector_data[detector_name] = {
                        "strain": strain.astype(np.float32),
                        "is_real_data": True,
                        "noise": None,  # Real data doesn't have separate noise
                        "noise_type": "gwtc_real",  # Metadata indicating real GWTC strain
                        "psd": (
                            self.psds.get(detector_name, {}).get("psd")
                            if hasattr(self, "psds")
                            else None
                        ),
                        "frequencies": (
                            self.psds.get(detector_name, {}).get("frequencies")
                            if hasattr(self, "psds")
                            else None
                        ),
                    }

        # If strain download failed for all detectors, synthesize per-detector strain using GWTC params
        if len(detector_data) == 0:
            self.logger.warning(
                f"Real strain download failed for {event['event_name']}, synthesizing signals using GWTC parameters"
            )
            # Build synthetic strains per detector present in the event
            for detector_name in event.get("detectors", []):
                sample_rate_cfg = _get_detector_sampling(detector_name)
                # Create a local waveform generator matching requested sampling and duration
                try:
                    local_wg = WaveformGenerator(sample_rate=sample_rate_cfg, duration=duration_cfg)
                    synth = local_wg.generate_waveform(params, detector_name)
                except Exception as e:
                    self.logger.debug(f"Waveform generation failed for {detector_name}: {e}")
                    # Fallback to global waveform generator and resample if necessary
                    try:
                        synth = self.waveform_generator.generate_waveform(params, detector_name)
                    except Exception as e2:
                        self.logger.warning(f"Fallback waveform generation failed: {e2}")
                        synth = np.zeros(int(self.sample_rate * self.duration), dtype=np.float32)

                # Ensure dtype and length
                try:
                    synth = np.asarray(synth, dtype=np.float32)
                except Exception:
                    synth = np.array(synth, dtype=np.float32)

                # Generate noise for synthesized GWTC strain
                psd_dict = self.psds[detector_name]
                noise, noise_type = self._get_noise_for_detector(detector_name, psd_dict)

                detector_data[detector_name] = {
                    "strain": synth,
                    "noise": noise.astype(np.float32),
                    "is_real_data": False,
                    "noise_type": noise_type,
                    "psd": (
                        self.psds.get(detector_name, {}).get("psd")
                        if hasattr(self, "psds")
                        else None
                    ),
                    "frequencies": (
                        self.psds.get(detector_name, {}).get("frequencies")
                        if hasattr(self, "psds")
                        else None
                    ),
                }

        return {
            "sample_id": f'gwtc_{sample_id:06d}_{event["event_name"]}',
            "type": params["type"],
            "is_overlap": False,
            "n_signals": 1,
            "is_edge_case": False,
            "is_real_event": True,
            "parameters": [params],
            "priorities": [params["target_snr"]],  #  Real SNR from GWTC!
            "detector_data": detector_data,
            "metadata": {
                "sample_id": sample_id,
                "generator": "GWTC",
                "event_name": event["event_name"],
                "observing_run": event.get("observing_run", "Unknown"),
                "far": float(event.get("far", 0.0)),
            },
        }

    def _compute_snr_from_params(self, params: Dict, detector_data: Optional[Dict] = None) -> float:
        """
        Compute SNR for a signal from parameters.
        Uses either actual detector data or distance-based estimate.
        """

        if detector_data is not None:
            # Calculate from actual strain if available
            max_snr = 0.0
            for detector_name, data in detector_data.items():
                try:
                    strain = data.get("strain")
                    if strain is not None and len(strain) > 0:
                        snr = estimate_snr_from_strain(
                            strain, psd=data.get("psd"), sampling_rate=self.sample_rate
                        )
                        max_snr = max(max_snr, snr)
                except:
                    continue

            if max_snr > 5:
                unclipped = float(max_snr)
                clipped = float(np.clip(unclipped, 7.0, 80.0))
                try:
                    if clipped >= 79.999 and unclipped > 80.0:
                        import traceback

                        short_params = {
                            k: params.get(k) for k in ("mass_1", "mass_2", "luminosity_distance")
                        }
                        self.logger.warning(
                            f"[SNR-CLIP] _compute_snr_from_params clipped {unclipped:.2f} -> {clipped:.2f}; params={short_params}"
                        )
                        stack = "".join(traceback.format_stack(limit=6))
                        self.logger.warning(stack)
                except Exception:
                    pass

                return clipped

        # Fallback: Distance-based SNR estimate
        distance = params.get("luminosity_distance", 500.0)
        m1 = params.get("mass_1", 30.0)
        m2 = params.get("mass_2", 30.0)

        # Reference: SNR ‚àù M_chirp^(5/6) / distance
        chirp_mass = (m1 * m2) ** (3 / 5) / (m1 + m2) ** (1 / 5)

        # CRITICAL FIX (Jan 19, 2026): Use event-type-specific baseline (same as _estimate_snr_from_params)
        event_type = params.get('type', None)
        if event_type is None:
            # Infer event type from mass boundaries
            # BNS: both masses < 2.5 M‚òâ (NS upper mass limit)
            # NSBH: one mass < 2.5, other > 2.5 (extreme mass ratio)
            # BBH: both masses > 2.5 M‚òâ (both likely BH)
            
            if m1 <= 2.5 and m2 <= 2.5:
                event_type = 'BNS'
            elif (m1 <= 2.5 and m2 > 2.5) or (m2 <= 2.5 and m1 > 2.5):
                event_type = 'NSBH'
            else:
                event_type = 'BBH'
        
        # Event-type-specific baseline parameters (consistency with _estimate_snr_from_params)
        baseline_params = {
            'BBH': {'snr': 35.0, 'mass': 30.0, 'distance': 1300.0},
            'BNS': {'snr': 25.0, 'mass': 1.4, 'distance': 130.0},
            'NSBH': {'snr': 20.0, 'mass': 6.0, 'distance': 400.0}
        }
        
        baseline = baseline_params.get(event_type, baseline_params['BBH'])
        snr_baseline = baseline['snr']
        distance_baseline = baseline['distance']
        mass_baseline = baseline['mass']

        snr = snr_baseline * (distance_baseline / distance) * (chirp_mass / mass_baseline) ** (5/6)

        # Add random factor for realism (¬±20%)
        snr *= np.random.uniform(0.8, 1.2)

        return float(np.clip(snr, 7.0, 80.0))

    def _generate_sample_with_priority(
        self, sample_id: int, generator_type: str = "simulator", event_name: str = None, **kwargs
    ) -> Dict:
        """
        Generate sample using specified generator with proper priority calculation.

        Args:
            sample_id: Unique sample ID
            generator_type: 'simulator' or 'gwtc'
            event_name: For GWTC, specific event name
            **kwargs: Additional generation parameters

        Returns:
            Sample dict with priorities properly set
        """

        if generator_type == "simulator":
            # Use OverlappingSignalSimulator
            n_signals = kwargs.get(
                "n_signals", np.random.choice([1, 2, 3, 4], p=[0.3, 0.4, 0.2, 0.1])
            )

            scenario = self.simulator.generate_overlapping_scenario(n_signals)
            noise_data = self.simulator.generate_detector_noise(
                duration=self.duration, sampling_rate=self.sample_rate
            )
            injected_data, signal_contributions = self.simulator.inject_signals_to_data(
                scenario, noise_data
            )

            priorities = []
            parameters = []

            for signal in scenario["signals"]:
                params = {
                    "mass_1": signal["mass_1"],
                    "mass_2": signal["mass_2"],
                    "luminosity_distance": signal["luminosity_distance"],
                    "total_mass": signal["mass_1"] + signal["mass_2"],
                    "chirp_mass": (
                        (signal["mass_1"] * signal["mass_2"]) ** (3 / 5)
                        / (signal["mass_1"] + signal["mass_2"]) ** (1 / 5)
                    ),
                    "type": self._classify_signal_type(signal["mass_1"], signal["mass_2"]),
                    **{k: v for k, v in signal.items() if k not in ["signal_id"]},
                }

                # Compute SNR from detector data
                detector_data_for_snr = {}
                for detector_name in self.detectors:
                    if detector_name in signal_contributions:
                        signal_strain = signal_contributions[detector_name].get(signal["signal_id"])
                        if signal_strain is not None:
                            detector_data_for_snr[detector_name] = {"strain": signal_strain}

                priority = self._compute_snr_from_params(params, detector_data_for_snr)
                # Do not overwrite an existing sampled target_snr
                if "target_snr" not in params:
                    self._set_target_snr(params, priority, reason="sim_scenario_compute")
                priorities.append(priority)
                parameters.append(params)

            detector_data = {
                det: {"strain": injected_data[det].astype(np.float32)}
                for det in injected_data.keys()
            }

            return {
                "sample_id": f"sim_{sample_id:06d}",
                "type": "overlap" if n_signals > 1 else parameters[0]["type"],
                "is_overlap": n_signals > 1,
                "n_signals": n_signals,
                "parameters": parameters,
                "priorities": priorities,  #  Proper SNR-based priorities!
                "detector_data": detector_data,
                "metadata": {
                    "sample_id": sample_id,
                    "generator": "OverlappingSignalSimulator",
                    "mean_snr": float(np.mean(priorities)),
                    "max_snr": float(np.max(priorities)),
                },
            }

        elif generator_type == "gwtc":
            # Use GWTC real events
            if not hasattr(self, "gwtc_loader"):
                from ahsd.data.gwtc_loader import GWTCLoader

                self.gwtc_loader = GWTCLoader()
                self.gwtc_events = self.gwtc_loader.get_gwtc_events()

            if event_name:
                event = self.gwtc_events[self.gwtc_events["event_name"] == event_name].iloc[0]
            else:
                quality_events = self.gwtc_events[
                    (self.gwtc_events["network_snr"] > 10) & (self.gwtc_events["mass_1_source"] > 5)
                ]
                event = (
                    quality_events.sample(1).iloc[0]
                    if len(quality_events) > 0
                    else self.gwtc_events.sample(1).iloc[0]
                )

            params = {
                "mass_1": float(event["mass_1_source"]),
                "mass_2": float(event["mass_2_source"]),
                "luminosity_distance": float(event["luminosity_distance"]),
                "target_snr": float(event["network_snr"]),  # Real SNR from GWTC!
                "type": self._classify_signal_type(event["mass_1_source"], event["mass_2_source"]),
                "event_name": event["event_name"],
            }

            return {
                "sample_id": f'gwtc_{sample_id:06d}_{event["event_name"]}',
                "type": params["type"],
                "is_overlap": False,
                "n_signals": 1,
                "parameters": [params],
                "priorities": [params["target_snr"]],  #  Real GWTC SNR!
                "detector_data": {},  # Implement strain download if needed
                "metadata": {
                    "sample_id": sample_id,
                    "generator": "GWTC",
                    "event_name": event["event_name"],
                },
            }

        else:
            # Fallback to your existing generation with SNR estimation
            sample = self._generate_single_sample(sample_id, **kwargs)

            #  FIX: Add priority calculation to existing samples
            if "priorities" not in sample or all(p == 0 for p in sample.get("priorities", [])):
                priorities = []
                for params in sample.get("parameters", []):
                    detector_data = sample.get("detector_data", {})
                    priority = self._compute_snr_from_params(params, detector_data)
                    priorities.append(priority)
                    # Do not overwrite existing sampled SNR
                    if "target_snr" not in params:
                        self._set_target_snr(params, priority, reason="fallback_generator_assign")

                sample["priorities"] = [self._normalize_priority_to_01(p) for p in priorities]

            return sample

    def _generate_sample_with_simulator(self, sample_id: int, n_signals: int = 1) -> Dict:
        """
        Generate sample using OverlappingSignalSimulator for realistic physics.

        Args:
            sample_id: Unique sample identifier
            n_signals: Number of signals to generate (1 for single, 2+ for overlap)

        Returns:
            Sample dict with proper structure and priorities

        Raises:
            ValueError: If simulator is not initialized
            RuntimeError: If signal generation fails
        """

        # ========================================================================
        # VALIDATION
        # ========================================================================
        if self.simulation is None:
            raise ValueError(
                f"Cannot generate sample {sample_id}: "
                f"OverlappingSignalSimulator not initialized. "
                f"Check __init__ and ensure simulation is set."
            )

        if n_signals < 1:
            raise ValueError(f"n_signals must be >= 1, got {n_signals}")

        try:
            # ====================================================================
            # GENERATE SCENARIO
            # ====================================================================
            scenario = self.simulation.generate_overlapping_scenario(n_signals)

            # Extract parameters from scenario
            parameters_list = scenario["signals"]

            # ====================================================================
            # GENERATE DETECTOR NOISE
            # ====================================================================
            noise_data = self.simulation.generate_detector_noise(
                duration=self.duration, sampling_rate=self.sample_rate
            )

            # ====================================================================
            # INJECT SIGNALS INTO NOISE
            # ====================================================================
            #  FIXED: Pass entire scenario dict (API expects scenario, not signals)
            detector_data, signal_contributions = self.simulation.inject_signals_to_data(
                scenario=scenario,  #  Changed from signals=scenario['signals']
                noise_data=noise_data,
            )

            # ====================================================================
            # CALCULATE PRIORITIES (SNRs)
            # ====================================================================
            priorities = []

            for signal in scenario["signals"]:
                signal_id = signal["signal_id"]
                max_snr = 0.0
                detector_snrs = {}

                # Proper nested dict access with validation
                for detector_name in self.detectors:
                    # Check if detector exists in signal_contributions
                    if detector_name not in signal_contributions:
                        self.logger.warning(
                            f"Detector {detector_name} not found in signal_contributions "
                            f"for sample {sample_id}, signal {signal_id}"
                        )
                        continue

                    detector_signals = signal_contributions[detector_name]

                    # Validate structure
                    if not isinstance(detector_signals, dict):
                        self.logger.warning(
                            f"signal_contributions[{detector_name}] is not a dict "
                            f"(got {type(detector_signals).__name__}) for sample {sample_id}"
                        )
                        continue

                    # Check if this signal exists for this detector
                    if signal_id not in detector_signals:
                        self.logger.debug(
                            f"Signal {signal_id} not found in {detector_name} contributions "
                            f"for sample {sample_id}"
                        )
                        continue

                    signal_strain = detector_signals[signal_id]

                    # Validate strain data
                    if signal_strain is None or len(signal_strain) == 0:
                        self.logger.warning(
                            f"Empty signal strain for signal {signal_id} in {detector_name} "
                            f"for sample {sample_id}"
                        )
                        continue

                    # Compute SNR from strain
                    try:
                        snr = self._compute_snr_from_strain(signal_strain, detector_name)
                        max_snr = max(max_snr, snr)
                        detector_snrs[detector_name] = float(snr)
                    except Exception as e:
                        self.logger.warning(
                            f"Failed to compute SNR for signal {signal_id} in {detector_name}: {e}"
                        )

                # Fallback if SNR computation failed for all detectors
                if max_snr < 5.0:
                    self.logger.warning(
                        f"SNR computation failed for signal {signal_id} in sample {sample_id}, "
                        f"using parameter-based estimate"
                    )
                    max_snr = self._estimate_snr_from_params(signal)

                # Clip and store priority
                # Diagnostic: if the raw computed SNR is above the clipping threshold,
                # log the per-detector contributions so we can trace the origin.
                if float(max_snr) > 80.0:
                    try:
                        self.logger.warning(
                            f"[SNR-ORIGIN] sample={sample_id} signal={signal_id} raw_max_snr={max_snr:.3f} "
                            f"detector_snrs={detector_snrs}"
                        )
                    except Exception:
                        self.logger.warning(
                            f"[SNR-ORIGIN] sample={sample_id} signal={signal_id} raw_max_snr={max_snr}"
                        )

                priority = float(np.clip(max_snr, 7.0, 80.0))
                # Respect any pre-sampled value: do not overwrite
                if "target_snr" not in signal:
                    # Use central helper so we get diagnostic traces for clipped values
                    try:
                        self._set_target_snr(signal, priority, reason="simulator_assign")
                    except Exception:
                        # Best-effort: fall back to direct assignment if helper is unavailable
                        signal["target_snr"] = float(priority)
                # Normalize raw SNR to [0, 1] using log-scale
                normalized_priority = self._normalize_priority_to_01(priority)
                priorities.append(normalized_priority)

            # ====================================================================
            # CONSTRUCT SAMPLE
            # ====================================================================
            is_overlap = n_signals > 1
            sample_type = "overlap" if is_overlap else scenario["signals"][0].get("type", "BBH")

            sample = {
                "id": sample_id,
                "sample_id": f"{'overlap' if is_overlap else 'single'}_{sample_id:06d}",
                "type": sample_type,
                "is_overlap": is_overlap,
                "n_signals": n_signals,
                "parameters": parameters_list,
                "priorities": priorities,
                "detector_data": detector_data,
                "edge_type_id": encode_edge_type(parameters_list),
                "metadata": {
                    "sample_id": sample_id,
                    "n_signals": n_signals,
                    "mean_snr": float(np.mean(priorities)) if priorities else 0.0,
                    "max_snr": float(np.max(priorities)) if priorities else 0.0,
                    "generator": "simulator",
                    "scenario_type": scenario.get("scenario_type", "unknown"),
                },
            }

            # Attach network_snr to parameters
            attach_network_snr_safe(parameters_list)

            return sample

        except Exception as e:
            self.logger.error(
                f"Failed to generate sample {sample_id} with simulator "
                f"(n_signals={n_signals}): {e}",
                exc_info=True,
            )
            raise RuntimeError(f"Simulator generation failed for sample {sample_id}: {e}") from e

    def _classify_signal_type(self, mass_1: float, mass_2: float) -> str:
        """
        Classify signal type based on component masses.
        BBH: Both masses > 3 Msun
        BNS: Both masses < 3 Msun
        NSBH: One mass > 3, other < 3
        """
        NEUTRON_STAR_MAX_MASS = 3.0

        if mass_1 > NEUTRON_STAR_MAX_MASS and mass_2 > NEUTRON_STAR_MAX_MASS:
            return "BBH"
        elif mass_1 < NEUTRON_STAR_MAX_MASS and mass_2 < NEUTRON_STAR_MAX_MASS:
            return "BNS"
        else:
            return "NSBH"

    def _compute_snr_from_strain(self, strain: np.ndarray, detector_name: str) -> float:
        """
        Compute optimal SNR from signal strain and detector PSD.

        Args:
            strain: Signal strain array
            detector_name: Name of detector (H1, L1, V1)

        Returns:
            Optimal matched-filter SNR
        """

        try:
            # Get PSD for this detector
            psd_dict = self.psds[detector_name]
            psd = psd_dict["psd"]
            frequencies = psd_dict["frequencies"]

            # FFT of strain (normalize by N to get proper Fourier amplitudes)
            # Note: failing to normalize by N inflates |~h|^2 and thus the SNR by a factor ~N^2.
            N = len(strain)
            strain_fft = np.fft.rfft(strain) / float(N)
            freq_array = np.fft.rfftfreq(len(strain), d=1.0 / self.sample_rate)

            # Interpolate PSD to match frequency array
            psd_interp = np.interp(freq_array, frequencies, psd)

            # Avoid division by zero
            psd_interp[psd_interp == 0] = np.inf

            # Compute optimal SNR (matched-filter formula)
            integrand = np.abs(strain_fft) ** 2 / psd_interp

            # Integrate (using trapezoidal rule)
            df = freq_array[1] - freq_array[0]
            snr_squared = 4.0 * np.sum(integrand) * df

            snr = np.sqrt(max(snr_squared, 0.0))

            return float(snr)

        except Exception as e:
            self.logger.warning(f"SNR computation failed for {detector_name}: {e}")
            return 0.0
